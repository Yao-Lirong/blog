<!DOCTYPE html>
<html lang="en">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Yao Lirong">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Yao Lirong">
        <meta name="keywords" content="Cornell,AI,CS,Computer Science,Artificial Intelligence,Yao,Lirong,姚立嵘,Programming">
    <meta name="description" content="姚立嵘 (Yao Lirong)'s Personal Website">
    <meta name="description" content="So Google, fxxk you.">
<meta property="og:type" content="article">
<meta property="og:title" content="Running MobileBert on Android with TensorFlow Lite">
<meta property="og:url" content="https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/index.html">
<meta property="og:site_name" content="Yao Lirong&#39;s Blog">
<meta property="og:description" content="So Google, fxxk you.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yao-lirong.github.io/images/tflite-support.png">
<meta property="article:published_time" content="2024-09-22T04:00:00.000Z">
<meta property="article:modified_time" content="2025-09-03T01:42:46.391Z">
<meta property="article:author" content="Yao Lirong">
<meta property="article:tag" content="Yao Lirong, Lirong Yao, Cornell, Computer Science, Artificial Intelligence">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yao-lirong.github.io/images/tflite-support.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/blog/assets/favicon.ico">
    <title>Running MobileBert on Android with TensorFlow Lite · Yao Lirong&#39;s Blog</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/blog/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/blog/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/blog/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/blog/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/blog/scripts/main.js" as="script">
    <link rel="preload" href="/blog/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->

    <!-- 百度统计 -->
    
    <!-- CNZZ 统计 -->
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-225410555-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-225410555-1');
        </script>
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/blog/atom.xml" title="Yao Lirong's Blog" type="application/atom+xml">
</head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/blog/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/blog/">Yao Lirong's Blog</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/blog/">Yao Lirong&#39;s Blog</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">Running MobileBert on Android with TensorFlow Lite</a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- 主页  -->
    <!-- 404页  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
                Running MobileBert on Android with TensorFlow Lite
            <!-- 404 -->
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            <!-- 404 -->
        </p>
        <!-- 文章页 meta -->
            <div class="post-intros">
                <!-- 文章页标签  -->
                    <div class="post-intro-tags">
</div>

                <!-- 文章字数统计 -->
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2024/09/22</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/blog/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <p>So Google, fxxk you.</p>
<span id="more"></span>
<h2 id="prerequsities">Prerequsities</h2>
<p>This picture very well explains how TFLite works and also why
TensorFlow 2 has both a <code>tf</code> and a <code>keras</code>.</p>
<figure>
<img src="https://web.archive.org/web/20220216170621if_/https://www.tensorflow.org/lite/images/convert/workflow.svg" alt="TFLite Workflow">
<figcaption aria-hidden="true">TFLite Workflow</figcaption>
</figure>
<h2 id="detours">Detours</h2>
<p>This section is mostly rant, but it is meaningful in preventing you
from taking any of the wrong path. Skip to the next section for a
tutorial on what to do.</p>
<ol type="1">
<li><p>We first found the Google’s official release <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-research/google-research/tree/master/mobilebert">http://google-research/mobilebert/</a>,
but</p>
<ul>
<li>the tutorial was unclear: Why do I need <code>data_dir</code> and
<code>output_dir</code> to export TFLite? How do I even read in the
pre-trained weights?</li>
<li>the code itself was pretty messy: why did they have export function
and training function all at this same file <code>run_squad.py</code>
and the only way to tell the program whether to train/export is checking
whether <code>export_dir is None</code> rather than passing a flag?</li>
</ul>
<p>In figuring out what each part does in this code, I looked up
TensorFlow 1’s doc and good lord they were broken. Google doesn’t even
host it anywhere: you have to go to <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/docs/tree/master/site/en/r1">a
GitHub repo</a> to read them in <code>.md</code> format. At this moment
I decided I will not touch anything written by TensorFlow 1’s API. (I
actually went through this pain back at my first ML intern in Haier, but
not again)</p></li>
<li><p>Sidenote before this: I didn’t know you can release model’s on
Kaggle (thought everyone releases on Hugging Face) and Google <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/discussions/product-feedback/448425">moved
their own TensorFlow Hub to Kaggle</a></p>
<p>So my supervisor found me <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/models/google/mobilebert/tensorFlow1">a
more readable Google release on Kaggle</a> with some high-level API and
doesn’t require you to read the painful source code. The above link has
<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/models/tensorflow/mobilebert">a redirect
to TensorFlow 2 implementation</a> with an official TFLite release. How
neat.</p>
<p>However, the official TFLite release</p>
<ol type="1">
<li>doesn’t have <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/guide/signatures">signature</a> -
TensorFlow’s specification of input and output (remember when you pass
inputs to a model you need to give name to them
e.g. <code>token_ids = ..., mask = ...</code>) which is required for
Xiaomi Service Framework to run a TFLite. P.S. Yes signature is not
required to specify when exporting, but for god’s sake all your tutorial
teaches people to use it and your own released ditched it? WTF
Google.</li>
<li>is broken (as expected?). <a href="forgot%20where%20the%20guide%20was">When I tried to run it on my
PC</a>, I got the following error
<code>indices_has_only_positive_elements was not true.gather index out of boundsNode number 2 (GATHER) failed to invoke.gather index out of boundsNode number 2 (GATHER) failed to invoke</code>.
Someone encountered <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/tensorflow/issues/59730">a similar
bug</a> while running the example code provided by TensorFlow and the
Google SWE found a bug in their example. At this moment I decided not to
trust this TFLite file anymore and just convert it on my own.</li>
</ol></li>
<li><p>So let’s use this official TensorFlow 2 implementation and <a href="forgot%20where%20the%20guide%20was">convert it to TFLite</a>. It
was all good and running on my PC, but</p>
<ol type="1">
<li>Its output format was really weird
<ul>
<li>It output consists of
<code>'mobile_bert_encoder', 'mobile_bert_encoder_1', 'mobile_bert_encoder_2', ..., 'mobile_bert_encoder_51'</code></li>
<li>Each of these has shape <code>(1, 4, 128, 128)</code> for a
<code>seq_length = 128, hidden_dim = 512</code> model. I figured 4 being
the number of heads and the other 128 is <code>hidden_dim</code> for
each head.</li>
<li>They output attention scores, not the final encoded vector: my input
was 5 tokens and they output is
<code>output[0, 0, 0, :] = array([0.198, 0.138, 0.244, 0.148, 0.270, 0. , 0. , ...</code>.
They sum to 1 and any other positions at <code>output</code> are 0 , so
attention score was my best guess.</li>
</ul></li>
<li>It doesn’t run on Android phone:
<code>tflite engine load failed due to java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 153. Are you using old TFLite binary with newer model?</code>
A <a target="_blank" rel="external nofollow noopener noreferrer" href="https://stackoverflow.com/questions/67883156/tflite-runtime-op-builtin-code-out-of-range-131-are-you-using-old-tflite-bi">Stack
Overflow answer</a> suggests the TensorFlow used to export TFLite
running on my PC doesn’t match the version of TFLite run time on this
Android phone. It can also be caused by me messing up with the whole
environment while installing <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model">Optimum</a>
to export TFLite last night, but I didn’t bother to look because I
finally found the solution</li>
</ol></li>
<li><p>And comes the savior, the king, the go-to solution in MLOps -
Huggingface. Reminded by a discussion I read by chance, I came to
realize <code>TFMobileBertModel.from_pretrained</code> actually returns
the Keras model (and the without <code>TF</code> version returns a
PyTorch model). That means I can just use Hugging Face API to read it
in, then use the native TensorFlow 2 API to export to TFLite. And
everything works like a charm now. The final output signature is just
Hugging Face’s familiar
<code>['last_hidden_state', 'pooler_output']</code></p></li>
</ol>
<h2 id="converting-tensorflow-model-to-tflite">Converting TensorFlow
Model to TFLite</h2>
<p>Conversion is pretty straight forward. You can just follow this
official guide: <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/models/convert/convert_models">For
Mobile &amp; Edge: Convert TensorFlow models</a>. Though I actually
followed my predecessor’s note (which actually comes from <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/guide/signatures">another TF
tutorial</a>). He also told me to caution that calling
<code>tf.disable_eager_execution()</code> can lead to absence of
signature, so do not call <code>tf.disable_eager_execution()</code> to
disable eager mode.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertTokenizerFast, TFMobileBertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert Model</span></span><br><span class="line"><span class="keyword">if</span> be_sane:</span><br><span class="line">    bert_model = TFMobileBertModel.from_pretrained(kerasH5_model_path) <span class="keyword">if</span> keras_file <span class="keyword">else</span> \</span><br><span class="line">                 TFMobileBertModel.from_pretrained(pytorch_model_path, from_pt = <span class="literal">True</span>)</span><br><span class="line">    converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># be crazy or already knows the messy TensorFlow.SavedModel format</span></span><br><span class="line">    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)</span><br><span class="line">tflite_model = converter.convert()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save Model</span></span><br><span class="line">tflite_output_path = <span class="string">&#x27;/model.tflite&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(tflite_output_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  f.write(tflite_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Signature</span></span><br><span class="line"><span class="comment"># Empty signature means error in the export process and the file cannot be used by Xiaomi Service Framework</span></span><br><span class="line">interpreter = tf.lite.Interpreter(model_path=tflite_output_path)</span><br><span class="line">interpreter = tf.lite.Interpreter(model_content=tflite_model)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line">signatures = interpreter.get_signature_list()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tflite model signatures:&quot;</span>, signatures)</span><br></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;serving_default&#x27;</span>: &#123;<span class="string">&#x27;inputs&#x27;</span>: [<span class="string">&#x27;attention_mask&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;input_ids&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;outputs&#x27;</span>: [<span class="string">&#x27;last_hidden_state&#x27;</span>, <span class="string">&#x27;pooler_output&#x27;</span>]&#125;&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>In addition, summarizing from the detours I took,</p>
<ul>
<li>Do not use Hugging Face’s Optimum for (at least vanilla) conversion
because it just calls the above command (see <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/optimum/blob/e0f58121140ce4baa01919ad70a6c13e936f7605/optimum/exporters/tflite/convert.py#L363-L371">code</a>)</li>
<li>Do not even bother to look at <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-research/google-research/tree/master/mobilebert#export-mobilebert-to-tf-lite-format">Google’s
original code</a> converting MobileBert to TFLite because nobody knows
what they’re writing.</li>
</ul>
<h2 id="running-tflite-on-pc">Running TFLite (on PC)</h2>
<p>Running TFLite on Android phone is the other department’s task. I
just want to run the TFLite file on PC to test everything’s good. To do
that, I strictly followed TensorFlow’s official guide: <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"><strong>TensorFlow
Lite inference: Load and run a model in Python</strong></a>.Our
converted models have the signatures, you can just follow the “with a
defined SignatureDef” guide.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = MobileBertTokenizerFast(<span class="string">f&quot;<span class="subst">&#123;model_path&#125;</span>/vocab.txt&quot;</span>)</span><br><span class="line">t_output = tokenizer(<span class="string">&quot;越过长城，走向世界&quot;</span>, return_tensors=<span class="string">&quot;tf&quot;</span>)</span><br><span class="line">ii, tt, am = t_output[<span class="string">&#x27;input_ids&#x27;</span>], t_output[<span class="string">&#x27;token_type_ids&#x27;</span>], t_output[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line"><span class="comment"># `get_signature_runner()` with empty input gives the &quot;serving_default&quot; runner</span></span><br><span class="line"><span class="comment"># `runner` input parameter is specified by `serving_default[&#x27;inputs&#x27;]`</span></span><br><span class="line">runner = interpreter.get_signature_runner() </span><br><span class="line">output = runner(input_ids = ii, token_type_ids = tt, attention_mask = am)</span><br><span class="line"><span class="keyword">assert</span> output.keys == [<span class="string">&#x27;last_hidden_state&#x27;</span>, <span class="string">&#x27;pooler_output&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>On the other hand, for a model without signatures, you need to use
the more primitive API <code>input_details</code> and
<code>output_details</code>. They specify the following properties,
where <code>index</code> is (probably) the index of this tensor in the
compute graph. To pass input values and get output values, you need to
access them by this index.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">interpreter.allocate_tensors()</span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"></span><br><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>], input_data)</span><br><span class="line">interpreter.invoke()</span><br><span class="line">output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(output_data)</span><br></pre></td></tr></table></figure>
<p>The following is the <code>input_details</code> of the non-signature
Google packed MobileBert.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">interpreter.get_input_details()</span><br><span class="line">[&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;model_attention_mask:0&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="string">&#x27;shape&#x27;</span>: array([  <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line">  <span class="string">&#x27;shape_signature&#x27;</span>: array([  <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line">  <span class="string">&#x27;dtype&#x27;</span>: numpy.int64,</span><br><span class="line">  <span class="string">&#x27;quantization&#x27;</span>: (<span class="number">0.0</span>, <span class="number">0</span>),</span><br><span class="line">  <span class="string">&#x27;quantization_parameters&#x27;</span>: &#123;<span class="string">&#x27;scales&#x27;</span>: array([], dtype=float32),</span><br><span class="line">   <span class="string">&#x27;zero_points&#x27;</span>: array([], dtype=int32),</span><br><span class="line">   <span class="string">&#x27;quantized_dimension&#x27;</span>: <span class="number">0</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;sparsity_parameters&#x27;</span>: &#123;&#125;&#125;,</span><br><span class="line">  &#123;...&#125;]</span><br></pre></td></tr></table></figure>
<h2 id="numerical-accuracy">Numerical Accuracy</h2>
<p>Our original torch/TensorFlow encoder and the converted TFLite
encoder, when both running on PC using Python, has a 1.2% difference in
their output (<code>last_hidden_state</code> or
<code>pooled_output</code>). We <strong>do not know</strong> where this
discrepancy comes from.</p>
<h2 id="converting-tokenizer-to-tflite">Converting Tokenizer to
TFLite</h2>
<p>We exported and ran the <em>encoder</em>, but that’s not enough. We
can’t ask the user to type in <code>token_ids</code> every time.
Therefore, we need to integrate the preprocessor (tokenizer) into our
TFLite file. To do that, we first tried integrating <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-preprocess/3">Google’s
official Keras tokenizer implementation</a> into our BERT model and
convert them together into a TFLite (yeah I didn’t learn the lesson).
This failed in the converting step for reasons that would become clear
later. And we switched gears to follow some other guide and first try to
convert a standalone tokenizer to TFLite.</p>
<p>Tokenizer is a part of the TensorFlow Text library. I followed the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/text/guide/text_tf_lite"><strong>official
guide: Converting TensorFlow Text operators to TensorFlow
Lite</strong></a> with <code>text.FastBertTokenizer</code>. Note when
you follow it, do it carefully and closely. I encountered a few problems
along the way:</p>
<ol type="1">
<li><p>When you change the <code>text.WhitespaceTokenizer</code> in
guide to our <code>text.FastBertTokenizer</code>, remember to specify a
<code>text.FastBertTokenizer(vocab=vocab_lst)</code>. We need not the
path to the vocab but the actual list
e.g. <code>[ "[PAD]", "[unused0]", "[unused1]", ...]</code> describes
the vocab where <code>[PAD]</code> maps to token id 0,
<code>[unused0]</code> to token id 1, and so on.</p></li>
<li><p><code>text.FastBertTokenizer</code> (or the standard version)
does not add <code>[CLS]</code> token for you. Google says this is to
make sure “you are able to manipulate the tokens and determine how to
construct your segments separately” (<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/text/issues/146">GitHub issue</a>).
How considerate you are, dear Google. I spent one and a half day
figuring out how to add these tokens when the model’s input length needs
to be fixed, otherwise it triggers TensorFlow’s compute graph to throw
“can’t get variable-length input” error. I finally found a solution in
<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-ai-edge/mediapipe/blob/a91256a42bbe49f8ebdb9e2ec7643c5c69dbec6f/mediapipe/model_maker/python/text/text_classifier/bert_tokenizer.py#L58-L71">Google’s
mediapipe’s implementation</a>.</p></li>
<li><p><code>Could not translate MLIR to FlatBuffer</code> when running
<code>tflite_model = converter.convert()</code>: as mentioned, you must
follow the guide very carefully. The guide specifies a TensorFlow Text
version. If not this version, the conversion would fail</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U <span class="string">&quot;tensorflow-text==2.11.*&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>Encountered unresolved custom op: FastBertNormalize</code>
when running converted interpreter / signature: as stated in the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/text/guide/text_tf_lite#inference">Inference
section of the guide</a>, tokenizers are custom operations and need to
be specified when running inference. (I can’t find doc for
<code>InterpreterWithCustomOps</code> anywhere but it does have an
argument <code>model_path</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">interp = interpreter.InterpreterWithCustomOps(</span><br><span class="line">    model_content=tflite_model,<span class="comment"># or model_path=TFLITE_FILE_PATH</span></span><br><span class="line">    custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)</span><br></pre></td></tr></table></figure></li>
<li><p>TensorFlow Text custom ops are not found on Android: the above
inference guide writes</p>
<blockquote>
<p>while the example below shows inference in Python, the steps are
similar in other languages with some minor API translations</p>
</blockquote>
<p>which is a total lie. Android does not support these operations as
the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/guide/op_select_allowlist#tensorflow_text_and_sentencepiece_operators">custom
text op list</a> only mentions python support.</p></li>
</ol>
<p>At the end, I did manage to 1 merge the above tokenizer and
HuggingFace model, 2 export a TFLite model that reads in a text and
outputs the last hidden state. However, I seem to have lost that piece
of the code. Don’t worry though. Because thanks to Google’s shitty
framework, it only works with very few tokenizer implementations anyway.
The work-for-all solution is to build your own tokenizer in Java.</p>
<blockquote>
<p>P.S. While debugging the FlatBuffer error, I came across the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/guide/authoring">TensorFlow
authoring tool</a> that can explicitly specify a function’s input output
format and detect op unsupported by TFLite. However, the tools is pretty
broken for me. Debugging this tool would probably take longer than
finding the problem yourself online / ask on a forum.</p>
</blockquote>
<h2 id="writing-your-own-tokenizer">Writing Your Own Tokenizer</h2>
<p>What’s weird is TensorFlow does have an official BERT on Android
example. Reading it again, I found their tokenizer is actually
implemented by C++ (<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier#key_features_of_the_bertnlclassifier_api">see
this example</a>). The repo containing the tokenizer code is called <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/text/tokenizers/bert_tokenizer.h">tflite-support</a>.
Finding <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/lite/inference_with_metadata/lite_support#current_use-case_coverage">this
library’s doc</a>, it becomes clear that the text-related operations are
currently not supported.</p>
<figure>
<img src="/images/tflite-support.png" alt="TFLite-Support Current use-case coverage">
<figcaption aria-hidden="true">TFLite-Support Current use-case
coverage</figcaption>
</figure>
<p>Google seems to have used JNI to call the C++ implementation of
tokenizer (<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/tflite-support/blob/8ed4a7b70df385a253aad7ed7df782439f42da6c/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/text/nlclassifier/BertNLClassifier.java#L39-L53">see
code</a>).</p>
<p>Therefore, we’d better write our own tokenizer. Luckily Hugging Face
also has a Bert on Android example - <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/tflite-android-transformers/tree/master/bert">tflite-android-transformers</a>
and writes more accessible code. We directly copied <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/tflite-android-transformers/tree/master/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization">their
tokenizer implementation</a>.</p>
<p>However, when switching to Chinese vocabulary, the tokenizer goes
glitchy. See the following example where we tokenize the
sentence「越过长城 ，走向世界」</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Our Java tokenizer gives the following tokens, which detokenizes to the following string</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">19871</span>, <span class="number">20327</span>, <span class="number">14871</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">14460</span>, <span class="number">13743</span>, <span class="number">17575</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">&#x27;[CLS] 越过长城 ， 走向世界 [SEP]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># On the other hand, official Hugging Face python BertTokenizer gives</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">6814</span>, <span class="number">7270</span>, <span class="number">1814</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">1403</span>, <span class="number">686</span>, <span class="number">4518</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">&#x27;[CLS] 越 过 长 城 ， 走 向 世 界 [SEP]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspecting the first difference, our Java tokenizer seems to have used sentencepiece </span></span><br><span class="line">tokenizer.decode([<span class="number">19871</span>])</span><br><span class="line"><span class="string">&#x27;##过&#x27;</span></span><br></pre></td></tr></table></figure>
<p>It turns out <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-research/bert/blob/master/multilingual.md#tokenization">BERT
in its original implementation</a> (<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/tokenization.py#L207">code</a>)
does not use sentence-piece tokenizer on Chinese characters. Instead, it
uses character level tokenizer. Therefore, we need to first insert a
whitespace to every character to ensure sentence-piece isn’t applied.
Note Hugging Face tokenizer follows BERT original python code very
closely so you can <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/tflite-android-transformers/blob/dcd6da1bfb28e3cd6bc83b58a112cdcd3d6cc2fe/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization/BasicTokenizer.java#L34">easily
find where to insert</a> that piece of code.</p>
<ul>
<li><p>Bert original implementation in Python, with Chinese logic</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">  text = convert_to_unicode(text)</span><br><span class="line">  text = <span class="variable language_">self</span>._clean_text(text)</span><br><span class="line">  <span class="comment"># Chinese Logic</span></span><br><span class="line">  text = <span class="variable language_">self</span>._tokenize_chinese_chars(text)</span><br><span class="line">  orig_tokens = whitespace_tokenize(text)</span><br></pre></td></tr></table></figure></li>
<li><p>Hugging Face tokenizer in Java, without Chinese logic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">BasicTokenizer</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">tokenize</span><span class="params">(String text)</span> &#123;</span><br><span class="line">    <span class="type">String</span> <span class="variable">cleanedText</span> <span class="operator">=</span> cleanText(text);</span><br><span class="line">    <span class="comment">// Insert Here</span></span><br><span class="line">    List&lt;String&gt; origTokens = whitespaceTokenize(cleanedText);</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="building-a-classifier">Building a Classifier</h2>
<p>The final task is actually to build a classifier of 28 online store
commodity classes. As I mentioned in the <a href="#Detours">Detours
section</a>, I do not know and don’t wanna bother to know how to define
or change a signature. Therefore, I again turn to Hugging Face for its
<code>MobileBertForSequenceClassification</code>.</p>
<p>The default classification head only has 1 layer, I changed its
structure to give it more expressive power.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = MobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, num_labels=<span class="built_in">len</span>(labels), problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    id2label=id2label, label2id=label2id)</span><br><span class="line">model.classifier = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line">    (<span class="string">&#x27;relu1&#x27;</span>, nn.LeakyReLU()),</span><br><span class="line">    (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">1024</span>, num_labels))</span><br><span class="line">]))</span><br><span class="line"><span class="comment"># Fine-tune ...</span></span><br><span class="line">torch.save(model.state_dict(), model_path)</span><br></pre></td></tr></table></figure>
<p>However, this throws error when you try to read such a fine-tuned
model back in. <code>MobileBertForSequenceClassification</code> is set
to have one-layer classification head, so it cannot read in your
self-defined classifier’s weights.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    num_labels=len(labels), id2label=id2label, label2id=label2id)</span><br><span class="line"></span><br><span class="line">&gt; Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at ./ckpts/ and are newly initialized: [<span class="string">&#x27;classifier.bias&#x27;</span>, <span class="string">&#x27;classifier.weight&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>To solve this, you can</p>
<ol type="1">
<li>Save encoder weight and classifier weight separately, then load them
separately</li>
<li>Create a custom class corresponding to your weights and initialize
an instance of that class instead</li>
</ol>
<p>2 is clearly <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/transformers/issues/1001#issuecomment-520162877">the
more sensible way</a>. You should read the very clearly written
<code>MobileBertForSequenceClassification</code> to understand what
exactly needs to be changed. It turns out all we have to do is to extend
the original class and change its <code>__init__</code> part, so it has
a 2-layer classification head.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertForSequenceClassification, TFMobileBertForSequenceClassification</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">MobileBertForSequenceClassification</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu1&#x27;</span>, nn.LeakyReLU()),</span><br><span class="line">            (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">1024</span>, <span class="number">28</span>))</span><br><span class="line">        ]))</span><br><span class="line">        <span class="variable language_">self</span>.post_init()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TFCustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">TFMobileBertForSequenceClassification</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, *inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config, *inputs, **kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.classifier = keras.Sequential([</span><br><span class="line">            keras.layers.Dense(<span class="number">1024</span>, input_dim=<span class="number">768</span>, name=<span class="string">&#x27;fc1&#x27;</span>),</span><br><span class="line">            keras.layers.LeakyReLU(alpha=<span class="number">0.01</span>, name = <span class="string">&#x27;relu1&#x27;</span>),  <span class="comment"># Keras defaults alpha to 0.3</span></span><br><span class="line">            keras.layers.Dense(<span class="number">28</span>, name=<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(labels), id2label=id2label, label2id=label2id)</span><br><span class="line">tf_model = TFCustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    ..., from_pt=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>However, you may find these two models output different values on the
same input. A closer look at weights unveil that <strong>Hugging Face
didn’t convert classifier’s weights from our Torch model to TensorFlow
model correctly</strong>. We have to set them manually instead.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf_model.classifier.get_layer(<span class="string">&quot;fc1&quot;</span>).set_weights([torch_model.classifier.fc1.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc1.bias.detach()])</span><br><span class="line">tf_model.classifier.get_layer(<span class="string">&quot;fc2&quot;</span>).set_weights([torch_model.classifier.fc2.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc2.bias.detach()])</span><br></pre></td></tr></table></figure>
<p>And now we are finally ready to go.</p>
<h2 id="quantization">Quantization</h2>
<p>I followed this official doc: <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ai.google.dev/edge/litert/models/post_training_quantization">Post-training
quantization</a>. Because of time limit, I didn’t try Quantization Aware
Training (QAT).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vanilla_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">tflite_model = vanilla_converter.convert()</span><br><span class="line"></span><br><span class="line">quant8_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant8_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">tflite_quant8_model = quant8_converter.convert()</span><br><span class="line"></span><br><span class="line">quant16_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant16_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">quant16_converter.target_spec.supported_types = [tf.float16]</span><br><span class="line">tflite_quant16_model = quant16_converter.convert()</span><br></pre></td></tr></table></figure>
<p>Below I report several key metrics for this Chinese-MobileBERT + a
2-layer classification head of [768*1024, 1024*<code>class_num</code>].
This was tested on a Xiaomi 12X with snapdragon 870. The baseline model
is my colleague’s BERT-Large implementation with accuracy 88.50% and
size 1230MB. My model’s accuracy was bad at first: 75.01% with
hyper-parameter <code>weight_decay = 0.01, learning_rate = 1e-4</code>,
but we searched out a good hyper-parameter of
<code>weight_decay = 2e-4,learning_rate = 2e-5</code> giving 86.01%. We
had 28 classes, 38000 training data in total, and trained for 5 epochs
where the validation accuracy roughly flattens.</p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 5%">
<col style="width: 24%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 4%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Quantization</th>
<th>Logit Difference</th>
<th>Accuracy</th>
<th>Accuracy (after hyper-param search)</th>
<th>Model Size (MB)</th>
<th>Inference Time(ms)</th>
<th>Power Usage(ma)</th>
<th>CPU(%)</th>
<th>Memory(MB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float32 (No quant)</td>
<td>0</td>
<td>75.01%</td>
<td>86.094%</td>
<td>101.4</td>
<td>1003.3</td>
<td>89.98</td>
<td>108.02</td>
<td>267.11</td>
</tr>
<tr class="even">
<td>float16</td>
<td>0.015%</td>
<td>75.01%</td>
<td>86.073%</td>
<td>51</td>
<td>838</td>
<td>64.15</td>
<td>108.77</td>
<td>377.11</td>
</tr>
<tr class="odd">
<td>int8</td>
<td>4.251%</td>
<td>63.49%</td>
<td>85.947%</td>
<td>25.9</td>
<td>573.8</td>
<td>60.09</td>
<td>110.83</td>
<td>233.19</td>
</tr>
</tbody>
</table>
<p>If look at the not fine-tuned, vanilla transformer encoder only, the
<code>last_hidden_state</code> has a difference:</p>
<table>
<thead>
<tr class="header">
<th>Quantization</th>
<th>Logit Difference</th>
<th>Model Size (MB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float32 (No quant)</td>
<td>0</td>
<td>97</td>
</tr>
<tr class="even">
<td>float16</td>
<td>0.1%</td>
<td>48.1</td>
</tr>
<tr class="odd">
<td>int8</td>
<td>19.8%</td>
<td>24.9</td>
</tr>
</tbody>
</table>
<h2 id="small-language-models">Small Language Models</h2>
<p>BERT is the go-to option for classification task. But when it comes
to small BERT, we had several options:</p>
<ul>
<li><p>mobileBERT</p></li>
<li><p>distilledBERT</p></li>
<li><p>tinyBERT</p></li>
</ul>
<p>As the post is about, we used mobileBERT at last because it’s by
Google Brain and Google probably knows their thing best.</p>
<p>On the other hand, if you’re looking for small generative model,
which people mostly call SLM (Small Language Model) as opposed to LLM, I
found these options but didn’t try them myself.</p>
<ul>
<li>openELM: Apple, 1.1B</li>
<li>Phi-2: Microsoft, 2.7B</li>
</ul>
<h2 id="post-script">Post Script</h2>
<p>If you want to build an app utilizing edge transformer, I would
recommend to read the source code of <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/huggingface/tflite-android-transformers">Hugging
Face’s toy app</a>. It doesn’t have a README or tutorial, nor have I
gone through it personally, but everything from TensorFlow sucks
(including MediaPipe unfortunately)</p>
<p>When checking back on this tutorial at date 2024/12/28, I found
Google released <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/google-ai-edge/ai-edge-torch">AI Edge
Torch</a>, the official tool converting PyTorch models into a .tflite
format. So you may probably want to try this first, but again, don’t
trust anything from TensorFlow team.</p>

    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>Author：<a href="https://yao-lirong.github.io/blog">Yao Lirong</a>
            </p><p>Link：<a href="https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/">https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/</a>
            </p><p>Publish date：<a href="https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/">September 22nd 2024, 12:00:00 am</a>
            </p><p>Update date：<a href="https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/">September 2nd 2025, 9:42:46 pm</a>
            </p><p>License：本文采用 <a rel="external nofollow noopener noreferrer" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a> 进行许可</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/blog/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/" title="YouTube Recommendation Algorithms (2016)">
                    <div class="nextTitle">YouTube Recommendation Algorithms (2016)</div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/blog/2024-09-09-Variational-Inference/" title="Variational Inference">
                    <div class="prevTitle">Variational Inference</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->

            
            
            
            <!-- utteranc评论 -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
    <!---->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="//github.com/Yao-Lirong" class="iconfont-archer github" target="_blank" title="github"></a>
                <a href="//twitter.com/yao_lirong" class="iconfont-archer twitter" target="_blank" title="twitter"></a>
                <a href="//www.linkedin.com/in/yao-lirong/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
                <a href="/blog/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank" rel="external nofollow noopener noreferrer">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- 不蒜子  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style="top:50vh;">
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#prerequsities"><span class="toc-number">1.</span> <span class="toc-text">Prerequsities</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#detours"><span class="toc-number">2.</span> <span class="toc-text">Detours</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#converting-tensorflow-model-to-tflite"><span class="toc-number">3.</span> <span class="toc-text">Converting TensorFlow
Model to TFLite</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#running-tflite-on-pc"><span class="toc-number">4.</span> <span class="toc-text">Running TFLite (on PC)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#numerical-accuracy"><span class="toc-number">5.</span> <span class="toc-text">Numerical Accuracy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#converting-tokenizer-to-tflite"><span class="toc-number">6.</span> <span class="toc-text">Converting Tokenizer to
TFLite</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#writing-your-own-tokenizer"><span class="toc-number">7.</span> <span class="toc-text">Writing Your Own Tokenizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#building-a-classifier"><span class="toc-number">8.</span> <span class="toc-text">Building a Classifier</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#quantization"><span class="toc-number">9.</span> <span class="toc-text">Quantization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#small-language-models"><span class="toc-number">10.</span> <span class="toc-text">Small Language Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#post-script"><span class="toc-number">11.</span> <span class="toc-text">Post Script</span></a></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 72
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/25</span>
            <a class="archive-post-title" href="/blog/2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/">Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/15</span>
            <a class="archive-post-title" href="/blog/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/">YouTube Recommendation Algorithms (2016)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span>
            <a class="archive-post-title" href="/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/">Running MobileBert on Android with TensorFlow Lite</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span>
            <a class="archive-post-title" href="/blog/2024-09-09-Variational-Inference/">Variational Inference</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/2024-08-23-Hyper-Parameter-Tuning-with-Optuna/">Hyper-Parameter Tuning with Optuna</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/2024-07-02-KV-Cache/">KV Cache</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/">Conducting Multi-Round Conversation with Transformers</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/14</span>
            <a class="archive-post-title" href="/blog/2024-05-14-GPT-4o-Release/">GPT-4o Release</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/22</span>
            <a class="archive-post-title" href="/blog/2024-04-22-CLIP/">CLIP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span>
            <a class="archive-post-title" href="/blog/2024-04-08-Gradient-Scaling/">Gradient Scaling</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/13</span>
            <a class="archive-post-title" href="/blog/2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&-AdamW)/">Decoupled Weight Decay Regularization (SGDW & AdamW)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span>
            <a class="archive-post-title" href="/blog/2024-03-01-Mixed-Precision-Training/">Mixed-Precision Training</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/22</span>
            <a class="archive-post-title" href="/blog/2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/">Parameter and FLOP Count in Transformer Model</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/">Memory Pinning and Transfer Data between Host (CPU) and Device (GPU)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/27</span>
            <a class="archive-post-title" href="/blog/2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/">Switching Personal Homepage Theme to al-folio</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span>
            <a class="archive-post-title" href="/blog/2024-01-21-Visual-Information-Theory/">Visual Information Theory</a>
        </li>
                </ul>
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span>
            <a class="archive-post-title" href="/blog/2023-12-01-Quantization/">Quantization</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span>
            <a class="archive-post-title" href="/blog/2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/">Fine-Tuning LLMs: Prompt Tuning, Adapter, LoRA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span>
            <a class="archive-post-title" href="/blog/2023-11-16-Graph-Networks-&-GraphCast/">Graph Networks & GraphCast</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/04</span>
            <a class="archive-post-title" href="/blog/2023-04-04-First-Time-Debugging-with-ChatGPT/">First Time Debugging with ChatGPT</a>
        </li>
                </ul>
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2022-12-31-2022-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2022 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/01</span>
            <a class="archive-post-title" href="/blog/2022-11-01-%E8%A7%A6%E4%B9%90-&-RPG-Codex-RPG%E6%96%87%E6%9C%AC%E5%86%99%E4%BD%9C%E8%AE%A8%E8%AE%BA/">触乐 & RPG Codex: RPG文本写作讨论</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/04</span>
            <a class="archive-post-title" href="/blog/2022-09-04-Deploy-a-Reddit-Bot-on-Heroku/">Deploy a Reddit Bot on Heroku</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/2022-08-23-How-to-Succeed-in-CS6784-(also-in-Academic-Life-in-General)/">How to Succeed in CS6784 (also in Academic Life in General)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span>
            <a class="archive-post-title" href="/blog/2022-06-11-JavaScript-Manual/">JavaScript Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span>
            <a class="archive-post-title" href="/blog/2022-04-23-%E5%8D%9A%E5%AE%A2SEO%E4%BC%98%E5%8C%96/">博客SEO优化</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/09</span>
            <a class="archive-post-title" href="/blog/2022-04-09-%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91-(FFmpeg-DaVinci)/">Video Editing (FFmpeg DaVinci)</a>
        </li>
                </ul>
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2021-12-31-2021-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2021 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/15</span>
            <a class="archive-post-title" href="/blog/2021-12-15-Look-Back-on-Cornell-21FA/">Look Back on Cornell 21FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span>
            <a class="archive-post-title" href="/blog/2021-09-16-Intro-to-SQL/">SQL Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span>
            <a class="archive-post-title" href="/blog/2021-09-10-Java-Quick-Guide/">Java Quick Guide</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span>
            <a class="archive-post-title" href="/blog/2021-08-31-Introduction-to-C/">C Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/29</span>
            <a class="archive-post-title" href="/blog/2021-08-29-%E6%9B%B4%E6%96%B0archer%E4%B8%BB%E9%A2%98--%E8%BF%81%E7%A7%BBHexo%E5%8D%9A%E5%AE%A2/">更新archer主题 / 迁移Hexo博客</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span>
            <a class="archive-post-title" href="/blog/2021-06-28-Install-and-Configure-Aria2-on-Linux/">Install and Configure Aria2 on WSL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/23</span>
            <a class="archive-post-title" href="/blog/2021-06-23-On-Intelligence/">On Intelligence</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/28</span>
            <a class="archive-post-title" href="/blog/2021-05-28-Introduction-to-TensorFlow-1.x/">TensorFlow 1.x Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span>
            <a class="archive-post-title" href="/blog/2021-05-24-Look-Back-on-Cornell-21SP/">Look Back on Cornell 21SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/15</span>
            <a class="archive-post-title" href="/blog/2021-05-15-Setting-up-a-Server/">Setting up a Server</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span>
            <a class="archive-post-title" href="/blog/2021-02-11-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(3)/">Tsinghua DSA 作业总结 (3)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/2021-02-10-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(2)/">Tsinghua DSA 作业总结 (2)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/2021-02-09-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(1)/">Tsinghua DSA 作业总结 (1)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span>
            <a class="archive-post-title" href="/blog/2021-01-11-CornellTsinghua-20FA-%E6%80%BB%E7%BB%93/">Look Back on Cornell/Tsinghua 20FA</a>
        </li>
                </ul>
            <div class="archive-year"> 2020 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2020-12-31-2020-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2020 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/2020-11-29-C++-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">C++ Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/2020-11-29-Python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">Python Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span>
            <a class="archive-post-title" href="/blog/2020-11-23-Latex-%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E6%89%8B%E5%86%8C/">LaTeX Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/17</span>
            <a class="archive-post-title" href="/blog/2020-11-17-Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">Python网络爬虫与信息提取</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/13</span>
            <a class="archive-post-title" href="/blog/2020-10-13-Algorithm-Design-%E5%8F%8A-CS4820-%E4%B8%80%E8%88%AC%E6%80%A7%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93/">CS4820 及 Algorithm Design 一般性内容总结</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/02</span>
            <a class="archive-post-title" href="/blog/2020-10-02-INFO1998-Intro-to-Machine-Learning/">INFO1998 Intro to Machine Learning (sklearn, pandas)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/29</span>
            <a class="archive-post-title" href="/blog/2020-09-29-Add-Open-with-Windows-Terminalto-Right-Click-Menu/">Add "Open with Windows Terminal" to Right-Click Menu</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span>
            <a class="archive-post-title" href="/blog/2020-09-07-CS2024-C++-Programming/">CS2024 C++ Programming</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/blog/2020-09-05-Windows%E4%B8%8B%E9%85%8D%E7%BD%AEPostgreSQL/">Windows下配置PostgreSQL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span>
            <a class="archive-post-title" href="/blog/2020-06-24-Kinekt-as-Web-Cam/">Kinect as Web Cam</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/27</span>
            <a class="archive-post-title" href="/blog/2020-05-27-Cornell-20SP-%E6%80%BB%E7%BB%93/">Look Back on Cornell 20SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span>
            <a class="archive-post-title" href="/blog/2020-03-15-Introduction-to-Vim/">Introduction to Vim</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/24</span>
            <a class="archive-post-title" href="/blog/2020-01-24-CS2043-Unix-Tools-and-Scripting/">CS2043 Unix Tools and Scripting</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/20</span>
            <a class="archive-post-title" href="/blog/2020-01-20-Installing-Ocaml-on-Linux/">Installing and Configuring Ocaml on Linux</a>
        </li>
                </ul>
            <div class="archive-year"> 2019 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span>
            <a class="archive-post-title" href="/blog/2019-12-22-Cornell-19FA-%E6%80%BB%E7%BB%93/">Look Back on Cornell 19FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/17</span>
            <a class="archive-post-title" href="/blog/2019-12-17-add-pdf-file-to-hexo/">Add pdf file to hexo</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/blog/2019-11-21-import-Junit-and-JavaFx-into-VSCode/">Import Junit and JavaFx into VSCode</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span>
            <a class="archive-post-title" href="/blog/2019-07-07-P1162-%E5%A1%AB%E6%B6%82%E9%A2%9C%E8%89%B2/">P1162 填涂颜色</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span>
            <a class="archive-post-title" href="/blog/2019-07-04-P1141-01%E8%BF%B7%E5%AE%AB/">P1141 01迷宫</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/2019-07-02-P1118-Backward-Digital-Sums/">P1118 Backward Digital Sums</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/26</span>
            <a class="archive-post-title" href="/blog/P1019%20%E5%8D%95%E8%AF%8D%E6%8E%A5%E9%BE%99/">P1019 单词接龙</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/P1101%20%E5%8D%95%E8%AF%8D%E6%96%B9%E9%98%B5/">P1101 单词方阵</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/16</span>
            <a class="archive-post-title" href="/blog/P1219%20%E5%85%AB%E7%9A%87%E5%90%8E/">P1219 八皇后</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span>
            <a class="archive-post-title" href="/blog/P1031%20%E5%9D%87%E5%88%86%E7%BA%B8%E7%89%8C/">P1031 均分纸牌</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/03</span>
            <a class="archive-post-title" href="/blog/P2678%20%E8%B7%B3%E7%9F%B3%E5%A4%B4/">P2678 跳石头</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/P1090%20%E5%90%88%E5%B9%B6%E6%9E%9C%E5%AD%90/">P1090 合并果子</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/P1309%20%E7%91%9E%E5%A3%AB%E8%BD%AE/">P1309 瑞士轮</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/Intro-to-Git-Command/">Introduction to Git Command</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/hello-world/">Hello World</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="NOI">
                <span class="iconfont-archer">&#xe606;</span>
                NOI
            </span>
            <span class="sidebar-tag-name" data-tags="Logistics">
                <span class="iconfont-archer">&#xe606;</span>
                Logistics
            </span>
            <span class="sidebar-tag-name" data-tags="Cornell">
                <span class="iconfont-archer">&#xe606;</span>
                Cornell
            </span>
            <span class="sidebar-tag-name" data-tags="Review">
                <span class="iconfont-archer">&#xe606;</span>
                Review
            </span>
            <span class="sidebar-tag-name" data-tags="CS3110">
                <span class="iconfont-archer">&#xe606;</span>
                CS3110
            </span>
            <span class="sidebar-tag-name" data-tags="Manual">
                <span class="iconfont-archer">&#xe606;</span>
                Manual
            </span>
            <span class="sidebar-tag-name" data-tags="Vim">
                <span class="iconfont-archer">&#xe606;</span>
                Vim
            </span>
            <span class="sidebar-tag-name" data-tags="Journal">
                <span class="iconfont-archer">&#xe606;</span>
                Journal
            </span>
            <span class="sidebar-tag-name" data-tags="Tsinghua">
                <span class="iconfont-archer">&#xe606;</span>
                Tsinghua
            </span>
            <span class="sidebar-tag-name" data-tags="Book">
                <span class="iconfont-archer">&#xe606;</span>
                Book
            </span>
            <span class="sidebar-tag-name" data-tags="ML">
                <span class="iconfont-archer">&#xe606;</span>
                ML
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/blog/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://yao-lirong.github.io/blog",
        root: siteMetaRoot,
        author: "Yao Lirong"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/blog/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/blog/scripts/share.js" async></script>
        <!-- mermaid -->
    </body>
</html>

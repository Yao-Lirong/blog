<!DOCTYPE html>
<html lang="en">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Yao Lirong">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Yao Lirong">
        <meta name="keywords" content="Cornell,AI,CS,Computer Science,Artificial Intelligence,Yao,Lirong,姚立嵘,Programming">
    <meta name="description" content="姚立嵘 (Yao Lirong)'s Personal Website">
    <meta name="description" content="CLIP investigates whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain (CV).">
<meta property="og:type" content="article">
<meta property="og:title" content="CLIP">
<meta property="og:url" content="https://yao-lirong.github.io/blog/2024-04-22-CLIP/index.html">
<meta property="og:site_name" content="Yao Lirong&#39;s Blog">
<meta property="og:description" content="CLIP investigates whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain (CV).">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yao-lirong.github.io/images/CLIP_2.png">
<meta property="og:image" content="https://yao-lirong.github.io/images/CLIP_1.png">
<meta property="og:image" content="https://yao-lirong.github.io/images/CLIP_9.png">
<meta property="article:published_time" content="2024-04-22T04:00:00.000Z">
<meta property="article:modified_time" content="2025-09-03T01:42:46.384Z">
<meta property="article:author" content="Yao Lirong">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yao-lirong.github.io/images/CLIP_2.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/blog/assets/favicon.ico">
    <title>CLIP · Yao Lirong&#39;s Blog</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/blog/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/blog/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/blog/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/blog/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/blog/scripts/main.js" as="script">
    <link rel="preload" href="/blog/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->

    <!-- 百度统计 -->
    
    <!-- CNZZ 统计 -->
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-225410555-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-225410555-1');
        </script>
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/blog/atom.xml" title="Yao Lirong's Blog" type="application/atom+xml">
</head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/blog/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/blog/">Yao Lirong's Blog</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/blog/">Yao Lirong&#39;s Blog</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">CLIP</a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- 主页  -->
    <!-- 404页  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
                CLIP
            <!-- 404 -->
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            <!-- 404 -->
        </p>
        <!-- 文章页 meta -->
            <div class="post-intros">
                <!-- 文章页标签  -->
                    <div class="post-intro-tags">
        <a class="post-tag" href="javascript:void(0);" data-tags="ML">ML</a>
</div>

                <!-- 文章字数统计 -->
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2024/04/22</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/blog/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <p>CLIP investigates whether it is possible to transfer the success of
task-agnostic web-scale pre-training in NLP to another domain (CV).</p>
<span id="more"></span>
<blockquote>
<p>This line of work represents the current pragmatic middle ground
between learning from a limited amount of supervised “gold-labels” and
learning from practically unlimited amounts of raw text.</p>
</blockquote>
<h2 id="approach">2 Approach</h2>
<h3 id="advantage-of-natural-language-supervision">2.1 Advantage of
Natural Language Supervision</h3>
<ul>
<li>easy to scale: natural language data amount is huge, much easier to
obtain than crowd-sourced labeling</li>
<li>flexible zero-shot transfer: connects image representation to
language; different from unsupervised or self-supervised model that is
limited to image domain.</li>
</ul>
<h3 id="constructing-dataset">2.2 Constructing Dataset</h3>
<p>To explore effects of web-scale pre-training, we first build a
web-scale dataset.</p>
<ol type="1">
<li>Construct a query list of size 500,000 that contains words occurred
&gt;= 100 times in Wikipedia</li>
<li>Search for images of these queries, construct a dataset of 400M
(image, text) pair</li>
<li><strong>Class balance</strong> (yeah that’s the word describing
“make each class have the same number of samples so it’s fair”) by
including 20,000 pairs per query</li>
</ol>
<h3 id="what-to-predict-what-is-the-loss">2.3 What to Predict? What is
the Loss?</h3>
<p>Previous methods with natural language supervision attempt is about
predicting a bag of words (BoW) / phrase n-gram representation of
labels. The authors explore different approaches. This work is all about
large scale pre-training and <strong>scaling</strong>. <strong>Training
efficiency</strong> is the key to scaling natural language supervision.
Authors selected final pre-training method based on efficiency. They
compared three approaches:</p>
<ol type="1">
<li><strong>Transformer language model (captioning model)</strong>:
train a transformer to predict the caption of an image. So this is a
generative task and uses transformer’s loss function. It learns 3 times
slower than the baseline - approach 2.</li>
<li><strong>A model predicts BoW encoding of the caption</strong>: this
was used as a simple baseline and authors found approach 1 couldn’t even
beat this baseline. This approach still tries to <strong>predict the
exact words</strong> of the text label, but the order of how words
appear no longer matters. This is not much easier due to the wide
variety of descriptions, comments, and related text that co-occur with
images.</li>
<li><strong>A contrastive model predicts which text <em>as a whole</em>
is paired with which image</strong>: In this way, we decrease the output
space to only the number of classes we have. We learn 4 times faster
than the baseline - approach 2.</li>
</ol>
<figure>
<img src="/images/CLIP_2.png" alt="Accuracy vs #(images processed)">
<figcaption aria-hidden="true">Accuracy vs #(images
processed)</figcaption>
</figure>
<p>See Figure 2 for a detailed comparison on <em>accuracy vs. #(images
fed)</em> of these three models. This illustrates how fast / slow a
training method learns.</p>
<table>
<colgroup>
<col style="width: 18%">
<col style="width: 42%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Output Space</th>
<th>Answer Space: In ideal scenario, what do we choose from?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Transformer Language Model</td>
<td>All English sentences (permutation of all English words)</td>
<td>500K queries</td>
</tr>
<tr class="even">
<td>BoW prediction model</td>
<td>Word count bucket of all English sentences (combination of all
English words)</td>
<td>500K queries</td>
</tr>
<tr class="odd">
<td>Contrastive pairing model</td>
<td>Sentences describing class and labels</td>
<td><code>batch_size</code> pre-selected queries (32768 in CLIP)</td>
</tr>
</tbody>
</table>
<p>It’s worth noting that CLIP uses a very large minibatch size of <span class="math inline">2<sup>15</sup> = 32768</span></p>
<h3 id="model-architecture-and-scaling">2.4 Model Architecture and
Scaling</h3>
<figure>
<img src="/images/CLIP_1.png" alt="Summary of CLIP">
<figcaption aria-hidden="true">Summary of CLIP</figcaption>
</figure>
<p>Image encoder has two architectures: ResNet-50 and ViT</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>Note:</p>
<ol type="1">
<li><code>d_e</code> represents multi-modal embedding space.</li>
<li>the temperature parameter <span class="math inline"><em>τ</em></span> is directly optimized as a
log-parameterized multiplicative scalar to avoid turning as a
hyper-parameter. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/model.py#L367-L368">implementation
in original release</a></li>
</ol>
<p>The authors train CLIP from scratch without initializing the image
encoder with ImageNet weights or the text encoder with pre-trained
weights.</p>
<p>This section also describes how to scale the text encoder and how to
scale both kinds of image encoder.</p>
<h2 id="experiments">3 Experiments</h2>
<p>Authors conducted experiments on 36 different datasets.</p>
<h3 id="zero-shot-transfer">3.1 Zero-Shot Transfer</h3>
<p>Authors wanted to experiment on zero-shot transfer ability because of
the ability demonstrated in language models. The following is the most
exciting sentence to me in this paper. I think it explains a lot of
large-scale design choices by OpenAI team. Did this paper inspire Ilya
to go all the way down the path of scaling?</p>
<blockquote>
<p>Our focus on studying zero-shot transfer as an evaluation of task
learning is inspired by work demonstrating task learning in the field of
NLP. To our knowledge Liu et al. (2018) first identified task learning
as an “unexpected side-effect” when a language model trained to generate
Wikipedia articles learned to reliably transliterate names between
languages.</p>
</blockquote>
<p>Authors explain in detail how we do zero-shot classification and give
an interpretation to the pipeline. I wrote the previous “output space”
and “answer space” thing based on this interpretation.</p>
<blockquote>
<p>The cosine similarity of these embeddings is then calculated, scaled
by a temperature parameter <span class="math inline"><em>τ</em></span> ,
and normalized into a probability distribution via a softmax. Note that
this prediction layer is a multinomial logistic regression classifier
with L2-normalized inputs, L2-normalized weights, no bias, and
temperature scaling. When interpreted this way, the image encoder is the
computer vision backbone which computes a feature representation for the
image and the text encoder is a hypernetwork which generates the weights
of a linear classifier based on the text specifying the visual concepts
that the classes represent. Continuing with this interpretation, every
step of CLIP pre-training can be viewed as optimizing the performance of
a randomly created proxy to a computer vision dataset which contains 1
example per class and has 32,768 total classes defined via natural
language descriptions.</p>
</blockquote>
<p><strong>prompt engineering and ensembling </strong></p>
<p>Text in our training data is usually a sentence, but text in test
data is just a one word label. To bridge this gap, we use some prompt
template.</p>
<ul>
<li>default: <code>A photo of a &#123;label&#125;</code></li>
<li>on several fine-grained image classification datasets, it’s helpful
to specify the category:
<code>A photo of a &#123;label&#125;, a type of pet</code> or
<code>a satellite photo of a &#123;label&#125;</code></li>
<li>ensembling several different prompts improve performance: use
different context prompts such as <code>A photo of a big &#123;label&#125;</code>
and <code>A photo of a small &#123;label&#125;</code>. Authors construct the
ensemble over the embedding space instead of probability space. In this
way, they cache a single set of averaged text embedding so compute cost
doesn’t increase in amortized time.</li>
</ul>
<p><strong>scaling law</strong></p>
<figure>
<img src="/images/CLIP_9.png" alt="Zero-shot CLIP scales wrt model compute">
<figcaption aria-hidden="true">Zero-shot CLIP scales wrt model
compute</figcaption>
</figure>
<p>Scaling law is the law that empirically shows that performance is
predictable as a function of important quantities such as training
compute and dataset size.</p>
<p>On 36 different datasets, ResNet CLIP’s average zero-shot error is
well modeled by a log-log linear scaling trend. However, performance on
individual evaluations is much more varied despite the smooth overall
trend. Authors did not report ViT CLIP scaling results.</p>
<h3 id="representation-learning">3.2 Representation Learning</h3>
<p>To use CLIP as a representation of the image, there are two common
approaches:</p>
<ul>
<li>Fitting a linear classifier on a representation extracted from the
model</li>
<li>End-to-end fine-tuning of the model.</li>
</ul>
<p>Fine-tuning increases flexibility, and prior work has convincingly
demonstrated that fine-tuning outperforms linear classification on most
image classification datasets. However, OpenAI chooses to use linear
classifier to measure CLIP performance for the following reasons:</p>
<ul>
<li><p>the more official reason: we chose it because it’s weak and
therefore better shows how dataset-agnostic CLIP is</p>
<blockquote>
<p>Our work is focused on developing a high-performing task and
dataset-agnostic pre-training approach. Fine-tuning, because it adapts
representations to each dataset during the fine-tuning phase, can
compensate for and potentially mask failures to learn general and robust
representations during the pre-training phase. Linear classifiers,
because of their limited flexibility, instead highlight these failures
and provide clear feedback during development</p>
</blockquote></li>
<li><p>the more practical reason:</p>
<blockquote>
<p>Fine-tuning opens up a much larger design and hyper-parameter space,
which makes it difficult to fairly evaluate and computationally
expensive. By comparison, linear classifiers require minimal
hyper-parameter tuning and have standardized implementations and
evaluation procedures.</p>
</blockquote></li>
<li><p>bonus reason:</p>
<blockquote>
<p>Linear classifier has the added benefit of being very similar to the
approach used for its zero-shot classifiers which enables extensive
comparisons and analysis</p>
</blockquote></li>
</ul>
<p><strong>approach</strong>: Appendix A.3 provides a full guideline of
training such a linear classifier, including details on hyper-parameter
search, solver method, and train-valid-test split. Notably, the input to
the Logistic Regression is the image embedding (output of the image
encoder <code>I_f</code>), not the multi-modal embedding (image
embedding that went through the multi-modal linear projection)</p>
<p><strong>results</strong>: when comparing to other models of similar
compute requirement, small CLIP have wins and loses. However, CLIP
scales very well and the largest model achieves both SOTA score and
compute efficiency.</p>
<p><strong>ViT vs ResNet</strong>: The authors found CLIP ViT is about
3x more compute efficient than CLIP ResNet. This is aligned with ViT
paper’s finding</p>
<p><strong>Out-of-Domain Performance and Natural Distribution
Shift</strong>: Researchers often find models exceeding human on
ImageNet test set can still make simple mistakes on other test data and
score much lower than human. A common explanation is these models are
adept at finding patterns within dataset, so improve in-distribution
performance. However many of these patterns are spurious and do not hold
for other distributions and result in large drops in performance on
other datasets.</p>
<p>Most of the studies that reach the above explanation limited their
evaluation model to those trained on ImageNet. Therefore, the authors
want to know to what degree are these failures attributable to deep
learning, ImageNet, or some combination of the two? They explore this by
evaluating ImageNet models on natural distribution shifted dataset.</p>
<p>Natural distribution shift means testing trained models on data that
is different in e.g. image style, image blurriness, geographic location,
and camera operation (<em>Hendrycks et al. The many faces of
robustness</em>). “Natural” is used to make a distinction from synthetic
distribution shift made through style-transferred or adversarially
generated.</p>
<ol type="1">
<li>Authors found CLIP perform much better on these natural distribution
shifted dataset.</li>
<li>However, this doesn’t necessarily mean supervised learning on
ImageNet causes a robustness gap. Other details of CLIP, such as its
large and diverse pre-training dataset or use of natural language
supervision could also produce robust models.</li>
<li>Therefore, OpenAI measured how the performance of CLIP models change
after adapting to the ImageNet distribution via an L2 regularized
logistic regression classifier fit to CLIP features on the ImageNet
training set. This improved accuracy on ImageNet by 9.2% to 85.4%, but
average accuracy under distribution shift slightly decreases.</li>
</ol>
<p>To me this doesn’t say much. If you fine-tune (or fit a linear
classifier) to a specific dataset, of course you’d expect its behavior
to be bad on some other dataset. But on the contrary, these
natural-distribution-shifted dataset is not that different from
ImageNet. Yes, there are some animations / sketches, but most are just
some more pictures of that class. And CLIP with an ImageNet linear head
cannot get them right. I guess what the authors want to say is that
ImageNet is not just A arbitrary dataset, but has almost become a
machine learning benchmark dataset. It is supposed to be general because
all models train on it and these models will be deployed to all sorts of
scenario.</p>
<p>The authors didn’t go far to attack the generality of ImageNet or
even draw any conclusion on why fitting an ImageNet classification head
hurts natural distribution shift performance. The authors just prompt to
caution that though prior work has also pre-trained models on
distributions other than ImageNet, it is common to study and release
models only after they have been fine-tuned to ImageNet. And it would be
wise to also study the models pre-trained on distributions other than
ImageNet.</p>
<p><strong>Results:</strong> Taken together, these results suggest that
the recent shift towards large-scale task and dataset agnostic
pre-training combined with a reorientation towards zero-shot and
few-shot benchmarking on broad evaluation suites promotes the
development of more robust systems and provides a more accurate
assessment of performance.</p>
<h2 id="data-overlap-analysis">5 Data Overlap Analysis</h2>
<p>A concern with pre-training on a very large internet dataset is
unintentional overlap with downstream evals. One option to prevent this
is to identify and remove all duplicates before training a model. While
this guarantees reporting true hold-out performance, it requires knowing
all possible data which a model might be evaluated on ahead of time.
This has the downside of limiting the scope of benchmarking and
analysis.</p>
<p>Therefore, OpenAI instead built a duplicate detector, document how
much overlap occurs, and run experiments on dataset with and without
these overlaps to measure how performance changes due to these overlaps.
So instead of simply removing them, they record performance of before
and after removing them.</p>
<p>They found that there is a median overlap of 2.2% and an average
overlap of 3.2%. Due to this small amount of overlap, overall accuracy
is rarely shifted by more than 0.1% with only 7 datasets above this
threshold.</p>
<p>It would be useful if OpenAI also releases their duplicate detector
model. Appendix C discusses it in more details but it doesn’t seem like
OpenAI ever released it.</p>
<h2 id="limitations">6 Limitations</h2>
<p><strong>Performance</strong>:</p>
<ol type="1">
<li>CLIP cannot beat dataset-specific trained &amp; designed models:
CLIP zero-shot performs better than a pre-trained ResNet-50 feature + a
linear classifier, but on most datasets, CLIP is well below the SOTA for
that specific dataset.</li>
<li>zero-shot CLIP still generalizes poorly to data that is truly
out-of-distribution for it: CLIP simply has a super large domain, not
really a general model. For example, MNIST digits are not at all in its
web-scraped huge dataset, so CLIP does surprisingly bad on this super
simple dataset.</li>
<li>CLIP is limited to “choosing”: CLIP cannot just take in a picture
and spit out its class. You need to give CLIP a range to choose from.
CLIP is based on “choosing”, not “generating” (image captioning
model)</li>
</ol>
<p><strong>Training Methodology</strong>:</p>
<ol type="1">
<li>In training time, CLIP repeatedly queried performance on full
validation sets to guide optimization. These validation sets often have
thousands of examples, which is unrealistic for true zero-shot
scenarios. On the contrary, LLM in training time doesn’t do this
(?)</li>
<li>Training dataset comes from Internet. Its image-text pairs are
unfiltered and uncurated and result in CLIP models learning many social
biases.</li>
</ol>
<p><strong>Supervision with Natural Language</strong>:</p>
<ol type="1">
<li>Many complex tasks and visual concepts can be difficult to specify
just through text.</li>
<li>Actual training examples are undeniably useful but CLIP does not
optimize for few-shot performance directly. In our work, we fall back to
fitting linear classifiers on top of CLIP’s features. This results in a
counter-intuitive drop in performance when transitioning from a
zero-shot to a few-shot setting.</li>
</ol>
<h2 id="broader-impacts">7 Broader Impacts</h2>
<p>In this section, the authors mainly introduces the bias exists in
CLIP and what kind of surveillance it can be used for.</p>
<p>Nothing too interesting, but they discussed how tweaking the category
system can improve model’s performance. This reminds me of what I did in
Xiaomi’s oversea app store tagging project, where I added new category
and modified existing category’s definition to improve the
cos-similarity based zero-shot classification model performance.</p>
<blockquote>
<p>Given that we observed that people under 20 were the most likely to
be classified in both the crime-related and non-human animal categories,
we carried out classification for the images with the same classes but
with an additional category ‘child’ added to the categories. We found
that this drastically reduced the number of images of people under 20
classified in either crime-related categories or non-human animal
categories (Table 7). This points to how class design has the potential
to be a key factor determining both the model performance and the
unwanted biases or behavior the model may exhibit</p>
</blockquote>
<p>The authors then go on to conclude that</p>
<blockquote>
<p>Decisions about things like class design are a key determiner not
only of model performance, but also of how and in what contexts model
biases manifest</p>
</blockquote>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li><p>Data is still the king in ML. It is possible to transfer the
success of task-agnostic web-scale pre-training in NLP to CV.</p></li>
<li><p>The key to scaling &amp; training efficiency is how compact your
output space is (word permutation - &gt; word combination -&gt;
<code>batch_size</code>)</p></li>
<li><p>We can use prompt ensembling to improve CLIP’s
performance.</p></li>
<li><p>To use CLIP as the feature extractor and put a linear classifier
on top of it, we use the image embedding (image encoder’s output), not
he multi-modal embedding (image embedding went through the multi-modal
linear projection);</p>
<p>On the other hand, for zero-shot classification, you use multi-modal
embedding, the same as the training process except now you only have one
image and calculate the cos similarity with all class names.</p></li>
<li><p>Decisions about things like class design are a key determiner not
only of model performance, but also of how and in what contexts model
biases manifest</p></li>
</ul>

    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>Author：<a href="https://yao-lirong.github.io/blog">Yao Lirong</a>
            </p><p>Link：<a href="https://yao-lirong.github.io/blog/2024-04-22-CLIP/">https://yao-lirong.github.io/blog/2024-04-22-CLIP/</a>
            </p><p>Publish date：<a href="https://yao-lirong.github.io/blog/2024-04-22-CLIP/">April 22nd 2024, 12:00:00 am</a>
            </p><p>Update date：<a href="https://yao-lirong.github.io/blog/2024-04-22-CLIP/">September 2nd 2025, 9:42:46 pm</a>
            </p><p>License：本文采用 <a rel="external nofollow noopener noreferrer" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a> 进行许可</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/blog/2024-05-14-GPT-4o-Release/" title="GPT-4o Release">
                    <div class="nextTitle">GPT-4o Release</div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/blog/2024-04-08-Gradient-Scaling/" title="Gradient Scaling">
                    <div class="prevTitle">Gradient Scaling</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->

            
            
            
            <!-- utteranc评论 -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
    <!---->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="//github.com/Yao-Lirong" class="iconfont-archer github" target="_blank" title="github"></a>
                <a href="//twitter.com/yao_lirong" class="iconfont-archer twitter" target="_blank" title="twitter"></a>
                <a href="//www.linkedin.com/in/yao-lirong/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
                <a href="/blog/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank" rel="external nofollow noopener noreferrer">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- 不蒜子  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style="top:50vh;">
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#approach"><span class="toc-number">1.</span> <span class="toc-text">2 Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#advantage-of-natural-language-supervision"><span class="toc-number">1.1.</span> <span class="toc-text">2.1 Advantage of
Natural Language Supervision</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#constructing-dataset"><span class="toc-number">1.2.</span> <span class="toc-text">2.2 Constructing Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-to-predict-what-is-the-loss"><span class="toc-number">1.3.</span> <span class="toc-text">2.3 What to Predict? What is
the Loss?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#model-architecture-and-scaling"><span class="toc-number">1.4.</span> <span class="toc-text">2.4 Model Architecture and
Scaling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments"><span class="toc-number">2.</span> <span class="toc-text">3 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#zero-shot-transfer"><span class="toc-number">2.1.</span> <span class="toc-text">3.1 Zero-Shot Transfer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#representation-learning"><span class="toc-number">2.2.</span> <span class="toc-text">3.2 Representation Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-overlap-analysis"><span class="toc-number">3.</span> <span class="toc-text">5 Data Overlap Analysis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#limitations"><span class="toc-number">4.</span> <span class="toc-text">6 Limitations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#broader-impacts"><span class="toc-number">5.</span> <span class="toc-text">7 Broader Impacts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#takeaways"><span class="toc-number">6.</span> <span class="toc-text">Takeaways</span></a></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 72
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/25</span>
            <a class="archive-post-title" href="/blog/2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/">Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/15</span>
            <a class="archive-post-title" href="/blog/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/">YouTube Recommendation Algorithms (2016)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span>
            <a class="archive-post-title" href="/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/">Running MobileBert on Android with TensorFlow Lite</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span>
            <a class="archive-post-title" href="/blog/2024-09-09-Variational-Inference/">Variational Inference</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/2024-08-23-Hyper-Parameter-Tuning-with-Optuna/">Hyper-Parameter Tuning with Optuna</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/2024-07-02-KV-Cache/">KV Cache</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/">Conducting Multi-Round Conversation with Transformers</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/14</span>
            <a class="archive-post-title" href="/blog/2024-05-14-GPT-4o-Release/">GPT-4o Release</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/22</span>
            <a class="archive-post-title" href="/blog/2024-04-22-CLIP/">CLIP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span>
            <a class="archive-post-title" href="/blog/2024-04-08-Gradient-Scaling/">Gradient Scaling</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/13</span>
            <a class="archive-post-title" href="/blog/2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&-AdamW)/">Decoupled Weight Decay Regularization (SGDW & AdamW)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span>
            <a class="archive-post-title" href="/blog/2024-03-01-Mixed-Precision-Training/">Mixed-Precision Training</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/22</span>
            <a class="archive-post-title" href="/blog/2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/">Parameter and FLOP Count in Transformer Model</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/">Memory Pinning and Transfer Data between Host (CPU) and Device (GPU)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/27</span>
            <a class="archive-post-title" href="/blog/2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/">Switching Personal Homepage Theme to al-folio</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span>
            <a class="archive-post-title" href="/blog/2024-01-21-Visual-Information-Theory/">Visual Information Theory</a>
        </li>
                </ul>
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span>
            <a class="archive-post-title" href="/blog/2023-12-01-Quantization/">Quantization</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span>
            <a class="archive-post-title" href="/blog/2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/">Fine-Tuning LLMs: Prompt Tuning, Adapter, LoRA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span>
            <a class="archive-post-title" href="/blog/2023-11-16-Graph-Networks-&-GraphCast/">Graph Networks & GraphCast</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/04</span>
            <a class="archive-post-title" href="/blog/2023-04-04-First-Time-Debugging-with-ChatGPT/">First Time Debugging with ChatGPT</a>
        </li>
                </ul>
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2022-12-31-2022-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2022 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/01</span>
            <a class="archive-post-title" href="/blog/2022-11-01-%E8%A7%A6%E4%B9%90-&-RPG-Codex-RPG%E6%96%87%E6%9C%AC%E5%86%99%E4%BD%9C%E8%AE%A8%E8%AE%BA/">触乐 & RPG Codex: RPG文本写作讨论</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/04</span>
            <a class="archive-post-title" href="/blog/2022-09-04-Deploy-a-Reddit-Bot-on-Heroku/">Deploy a Reddit Bot on Heroku</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/2022-08-23-How-to-Succeed-in-CS6784-(also-in-Academic-Life-in-General)/">How to Succeed in CS6784 (also in Academic Life in General)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span>
            <a class="archive-post-title" href="/blog/2022-06-11-JavaScript-Manual/">JavaScript Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span>
            <a class="archive-post-title" href="/blog/2022-04-23-%E5%8D%9A%E5%AE%A2SEO%E4%BC%98%E5%8C%96/">博客SEO优化</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/09</span>
            <a class="archive-post-title" href="/blog/2022-04-09-%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91-(FFmpeg-DaVinci)/">Video Editing (FFmpeg DaVinci)</a>
        </li>
                </ul>
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2021-12-31-2021-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2021 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/15</span>
            <a class="archive-post-title" href="/blog/2021-12-15-Look-Back-on-Cornell-21FA/">Look Back on Cornell 21FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span>
            <a class="archive-post-title" href="/blog/2021-09-16-Intro-to-SQL/">SQL Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span>
            <a class="archive-post-title" href="/blog/2021-09-10-Java-Quick-Guide/">Java Quick Guide</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span>
            <a class="archive-post-title" href="/blog/2021-08-31-Introduction-to-C/">C Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/29</span>
            <a class="archive-post-title" href="/blog/2021-08-29-%E6%9B%B4%E6%96%B0archer%E4%B8%BB%E9%A2%98--%E8%BF%81%E7%A7%BBHexo%E5%8D%9A%E5%AE%A2/">更新archer主题 / 迁移Hexo博客</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span>
            <a class="archive-post-title" href="/blog/2021-06-28-Install-and-Configure-Aria2-on-Linux/">Install and Configure Aria2 on WSL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/23</span>
            <a class="archive-post-title" href="/blog/2021-06-23-On-Intelligence/">On Intelligence</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/28</span>
            <a class="archive-post-title" href="/blog/2021-05-28-Introduction-to-TensorFlow-1.x/">TensorFlow 1.x Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span>
            <a class="archive-post-title" href="/blog/2021-05-24-Look-Back-on-Cornell-21SP/">Look Back on Cornell 21SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/15</span>
            <a class="archive-post-title" href="/blog/2021-05-15-Setting-up-a-Server/">Setting up a Server</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span>
            <a class="archive-post-title" href="/blog/2021-02-11-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(3)/">Tsinghua DSA 作业总结 (3)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/2021-02-10-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(2)/">Tsinghua DSA 作业总结 (2)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/2021-02-09-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(1)/">Tsinghua DSA 作业总结 (1)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span>
            <a class="archive-post-title" href="/blog/2021-01-11-CornellTsinghua-20FA-%E6%80%BB%E7%BB%93/">Look Back on Cornell/Tsinghua 20FA</a>
        </li>
                </ul>
            <div class="archive-year"> 2020 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2020-12-31-2020-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/">2020 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/2020-11-29-C++-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">C++ Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/2020-11-29-Python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">Python Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span>
            <a class="archive-post-title" href="/blog/2020-11-23-Latex-%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E6%89%8B%E5%86%8C/">LaTeX Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/17</span>
            <a class="archive-post-title" href="/blog/2020-11-17-Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">Python网络爬虫与信息提取</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/13</span>
            <a class="archive-post-title" href="/blog/2020-10-13-Algorithm-Design-%E5%8F%8A-CS4820-%E4%B8%80%E8%88%AC%E6%80%A7%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93/">CS4820 及 Algorithm Design 一般性内容总结</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/02</span>
            <a class="archive-post-title" href="/blog/2020-10-02-INFO1998-Intro-to-Machine-Learning/">INFO1998 Intro to Machine Learning (sklearn, pandas)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/29</span>
            <a class="archive-post-title" href="/blog/2020-09-29-Add-Open-with-Windows-Terminalto-Right-Click-Menu/">Add "Open with Windows Terminal" to Right-Click Menu</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span>
            <a class="archive-post-title" href="/blog/2020-09-07-CS2024-C++-Programming/">CS2024 C++ Programming</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/blog/2020-09-05-Windows%E4%B8%8B%E9%85%8D%E7%BD%AEPostgreSQL/">Windows下配置PostgreSQL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span>
            <a class="archive-post-title" href="/blog/2020-06-24-Kinekt-as-Web-Cam/">Kinect as Web Cam</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/27</span>
            <a class="archive-post-title" href="/blog/2020-05-27-Cornell-20SP-%E6%80%BB%E7%BB%93/">Look Back on Cornell 20SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span>
            <a class="archive-post-title" href="/blog/2020-03-15-Introduction-to-Vim/">Introduction to Vim</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/24</span>
            <a class="archive-post-title" href="/blog/2020-01-24-CS2043-Unix-Tools-and-Scripting/">CS2043 Unix Tools and Scripting</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/20</span>
            <a class="archive-post-title" href="/blog/2020-01-20-Installing-Ocaml-on-Linux/">Installing and Configuring Ocaml on Linux</a>
        </li>
                </ul>
            <div class="archive-year"> 2019 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span>
            <a class="archive-post-title" href="/blog/2019-12-22-Cornell-19FA-%E6%80%BB%E7%BB%93/">Look Back on Cornell 19FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/17</span>
            <a class="archive-post-title" href="/blog/2019-12-17-add-pdf-file-to-hexo/">Add pdf file to hexo</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/blog/2019-11-21-import-Junit-and-JavaFx-into-VSCode/">Import Junit and JavaFx into VSCode</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span>
            <a class="archive-post-title" href="/blog/2019-07-07-P1162-%E5%A1%AB%E6%B6%82%E9%A2%9C%E8%89%B2/">P1162 填涂颜色</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span>
            <a class="archive-post-title" href="/blog/2019-07-04-P1141-01%E8%BF%B7%E5%AE%AB/">P1141 01迷宫</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/2019-07-02-P1118-Backward-Digital-Sums/">P1118 Backward Digital Sums</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/26</span>
            <a class="archive-post-title" href="/blog/P1019%20%E5%8D%95%E8%AF%8D%E6%8E%A5%E9%BE%99/">P1019 单词接龙</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/P1101%20%E5%8D%95%E8%AF%8D%E6%96%B9%E9%98%B5/">P1101 单词方阵</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/16</span>
            <a class="archive-post-title" href="/blog/P1219%20%E5%85%AB%E7%9A%87%E5%90%8E/">P1219 八皇后</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span>
            <a class="archive-post-title" href="/blog/P1031%20%E5%9D%87%E5%88%86%E7%BA%B8%E7%89%8C/">P1031 均分纸牌</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/03</span>
            <a class="archive-post-title" href="/blog/P2678%20%E8%B7%B3%E7%9F%B3%E5%A4%B4/">P2678 跳石头</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/P1090%20%E5%90%88%E5%B9%B6%E6%9E%9C%E5%AD%90/">P1090 合并果子</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/P1309%20%E7%91%9E%E5%A3%AB%E8%BD%AE/">P1309 瑞士轮</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/Intro-to-Git-Command/">Introduction to Git Command</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/hello-world/">Hello World</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="NOI">
                <span class="iconfont-archer">&#xe606;</span>
                NOI
            </span>
            <span class="sidebar-tag-name" data-tags="Logistics">
                <span class="iconfont-archer">&#xe606;</span>
                Logistics
            </span>
            <span class="sidebar-tag-name" data-tags="Cornell">
                <span class="iconfont-archer">&#xe606;</span>
                Cornell
            </span>
            <span class="sidebar-tag-name" data-tags="Review">
                <span class="iconfont-archer">&#xe606;</span>
                Review
            </span>
            <span class="sidebar-tag-name" data-tags="CS3110">
                <span class="iconfont-archer">&#xe606;</span>
                CS3110
            </span>
            <span class="sidebar-tag-name" data-tags="Manual">
                <span class="iconfont-archer">&#xe606;</span>
                Manual
            </span>
            <span class="sidebar-tag-name" data-tags="Vim">
                <span class="iconfont-archer">&#xe606;</span>
                Vim
            </span>
            <span class="sidebar-tag-name" data-tags="Journal">
                <span class="iconfont-archer">&#xe606;</span>
                Journal
            </span>
            <span class="sidebar-tag-name" data-tags="Tsinghua">
                <span class="iconfont-archer">&#xe606;</span>
                Tsinghua
            </span>
            <span class="sidebar-tag-name" data-tags="Book">
                <span class="iconfont-archer">&#xe606;</span>
                Book
            </span>
            <span class="sidebar-tag-name" data-tags="ML">
                <span class="iconfont-archer">&#xe606;</span>
                ML
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/blog/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://yao-lirong.github.io/blog",
        root: siteMetaRoot,
        author: "Yao Lirong"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/blog/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/blog/scripts/share.js" async></script>
        <!-- mermaid -->
    </body>
</html>

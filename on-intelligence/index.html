<!DOCTYPE html>
<html lang="en">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Yao Lirong">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Yao Lirong">
        <meta name="keywords" content="Cornell,AI,CS,Computer Science,Artificial Intelligence,Yao,Lirong,姚立嵘,Programming">
    <meta name="description" content="姚立嵘 (Yao Lirong)'s Personal Website">
    <meta name="description" content="Complexity is a symptom of confusion, not a cause.">
<meta property="og:type" content="article">
<meta property="og:title" content="On Intelligence">
<meta property="og:url" content="https://yao-lirong.github.io/blog/on-intelligence/index.html">
<meta property="og:site_name" content="Yao Lirong&#39;s Blog">
<meta property="og:description" content="Complexity is a symptom of confusion, not a cause.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-06-23T04:00:00.000Z">
<meta property="article:modified_time" content="2022-06-08T19:33:42.000Z">
<meta property="article:author" content="Yao Lirong">
<meta property="article:tag" content="Book">
<meta name="twitter:card" content="summary">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/blog/assets/favicon.ico">
    <title>On Intelligence · Yao Lirong&#39;s Blog</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/blog/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/blog/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/blog/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/blog/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/blog/scripts/main.js" as="script">
    <link rel="preload" href="/blog/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->

    <!-- 百度统计 -->
    
    <!-- CNZZ 统计 -->
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-225410555-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-225410555-1');
        </script>
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/blog/atom.xml" title="Yao Lirong's Blog" type="application/atom+xml">
</head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/blog/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/blog/">Yao Lirong's Blog</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/blog/">Yao Lirong&#39;s Blog</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">On Intelligence</a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- 主页  -->
    <!-- 404页  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
                On Intelligence
            <!-- 404 -->
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            <!-- 404 -->
        </p>
        <!-- 文章页 meta -->
            <div class="post-intros">
                <!-- 文章页标签  -->
                    <div class="post-intro-tags">
        <a class="post-tag" href="javascript:void(0);" data-tags="Book">Book</a>
</div>

                <!-- 文章字数统计 -->
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2021/06/23</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/blog/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <blockquote>
<p>Complexity is a symptom of confusion, not a cause.</p>
</blockquote>
<span id="more"></span>
<h2 id="artificial-intelligence">1 Artificial Intelligence</h2>
<ul>
<li>计算机学界的主流观点：不需要学习大脑</li>
<li>此观点的起始：Turing
Test，即让人们<strong>认为</strong>它是智能，产生 intelligent behavior
更重要</li>
<li>the Chinese Room: 在中文屋中智能没有产生，作者认为 Understanding
cannot be measured by external behavior; it is instead an internal
metric of how the brain remembers things and uses its memories to make
predictions.
但绝大多数的所谓”AI”和这里的中文屋和这一定义无任何相似之处</li>
</ul>
<h2 id="neural-networks">2 Neural Networks</h2>
<p>一些可能已经过时的观点：</p>
<ul>
<li>Neural Network 没有考虑 feedback 和 time changing inputs</li>
<li>Cognitive Scientist 虽然想记录大脑中的
feedback，但是迫于现有技术(fMRI)只能记录脑内活动的位置，无法记录连续的变化</li>
</ul>
<h2 id="the-human-brain">3 The Human Brain</h2>
<blockquote>
<p>Mind is the creation of the cells in the brain.</p>
</blockquote>
<blockquote>
<p>The cortex is extremely flexible and that the inputs to the brain are
just patterns. It doesn’t matter where the patterns come from; as long
as they correlate over time in consistent ways, the brain can make sense
of them.</p>
</blockquote>
<ul>
<li><p>Function Hierarchy: 脑的每个功能部分都被划为
hierarchy，以输入的视觉为例</p>
<ol type="1">
<li>V1 (primary sensory areas): rawest, most basic level</li>
<li>V2, V4, IT: concerned with more specialized or more abstract
aspects</li>
<li>association area: receive inputs from more than one sense</li>
</ol>
<p>虽然是一个
hierarchy，但是实际上当我们从低层走向高层的过程中，information always
flows in the opposite direction as well, and with more projections
feeding back down the hierarchy than up.</p></li>
<li><p><strong>Uniformity of Cortex Parts</strong>: Mountcastle found
that parts of cortex performing different function is very similar in
appearance and structure. From there, he argues that all regions of the
cortex are performing the same operation. The thing that makes the
vision area visual and the motor area motoric is how the regions of
cortex are connected to each other and to other parts of the central
nervous system.</p></li>
<li><p>Plasticity of Cortex:
我们发现如果大脑某个部分损坏，另一个部分可以接管它原先的人物，这佐证了
Mountcastle 的观点。另有一个 Thought
Experiment：假设我们的大脑并不具有如此的可塑性，那么这就意味着我们的某个大脑部位是专门用来学习中文汉字的，但是对于生物进化来说，汉字进化地太快了，大脑根本不可能适应地这么快（或者外国人也可以迅速学中文亦能佐证这一观点）</p></li>
<li><p>Similarity of Inputs into Brain:
不管视觉听觉还是什么输入，真正进了人体都是 Action Potentials. They are
all the same - just patterns. 也用来佐证 Mountcastle 的观点。There are
spatial and and temporal patterns:</p>
<ul>
<li>Spatial Patterns: coincident patterns in time; they are created when
multiple receptors in the same sense organ are stimulated
simultaneously</li>
<li>Temporal Patterns: patterns entering your sensory organs are
constantly changing over time</li>
</ul></li>
<li><p>进一步给出了关于以上两点的例子：认为同时做出反应的假手是自己的手
/ 镜头连舌头上的压感接收器，用舌头看东西</p></li>
</ul>
<h2 id="memory">4 Memory</h2>
<ul>
<li><p>驳斥人脑比计算机更快，计算力更高 -&gt;
人脑能做到比计算机快是因为运行原理根本不同 -&gt; 引出本章主旨: the brain
doesn’t “compute” the answers to problems; it retrieves the answers from
memory.</p></li>
<li><p><strong>Four attributes of neocortical memory that are
fundamentally different from computer memory</strong>:</p>
<ul>
<li>The neocortex stores sequences of patterns -&gt; predictions of
future events</li>
<li>The neocortex recalls patterns auto-associatively -&gt; recall
memories appropriate for prediction</li>
<li>The neocortex stores patterns in an invariant form -&gt; apply
knowledge of past to new situations that are similar but not
identical</li>
<li>The neocortex stores patterns in a hierarchy.</li>
</ul>
<p>接下来我们将详细介绍前三个特征并在第6章介绍最后一个特征
“阶层”</p></li>
<li><p>Sequential Pattern: story is stored in your head in a sequential
fashion and can only be recalled in the same sequence. You can’t
remember the entire story at once.</p>
<p>一个有趣的观点: Truly random thoughts don’t exist. Memory recall
almost always follows a pathway of association.</p></li>
<li><p>Self-Associativity: The memory system can recall complete
patterns when given only partial or distorted inputs. This is a result
of Hebbian Learning: Firing together Wires together, so when only a part
of the cell is activated, the whole group of cells will be
activated.</p></li>
<li><p>Invariant Representation: 人脑不是CD或硬盘，we don’t remember or
recall things with complete fidelity. Instead, the brain remembers the
important relationships in the world, independent of the details.</p>
<p>我们常用视觉来举例子：some set of the cells in the face recognition
area remain active as long as your friend’s face is anywhere in your
field of vision, regardless of its size, position, orientation, scale,
and expression. This <strong>stability of cell firing</strong> is an
invariant representation.</p></li>
<li><p>小引子导入下一章：下一章的主旨是人脑的主要功能就是 make
predictions using memories，but given that the cortex stores invariant
information, how can it make specific predictions? It combines knowledge
of the invariant structure with the most recent details.</p></li>
</ul>
<h2 id="a-new-framework-of-intelligence">5 A New Framework of
Intelligence</h2>
<blockquote>
<p>Prediction is not just one of the things your brain does. It is the
primary function of the neocortex, and the foundation of intelligence.
The cortex is an organ of prediction.</p>
</blockquote>
<p>这是作者本书中最基本的观点，也就是他所说的新的智能框架
(Memory-Prediction Framework of Intelligence) 。具体地来解释 Prediction
这个概念：Your brain makes low-level sensory predictions about what it
expects to see, hear, and feel at every given moment, and it does so in
parallel. All regions of your neocortex are simultaneously trying to
predict what their next experience will be. “Prediction” means that the
neurons involved in sensing your door <strong>become active in advance
of them actually receiving sensory input</strong>. When the sensory
input does arrive, it is <strong>compared with what was
expected</strong>. <strong>Correct predictions result in
understanding.</strong> <strong>Incorrect predictions result in
confusion</strong> and prompt you to pay attention. 不局限于 sensory
input，motor output 在我们的大脑中也是和 sensory input一样的 pattern, so
neocortex can also remembers what behavior (pattern) leads to what
sensory input (patter) and we can <strong>direct behavior to satisfy its
predictions</strong>.</p>
<p>作者举了很多关于 prediction
的例子（预知乐曲的旋律，朋友的样子，你妈下一句话会说什么…）其中最有意思的例子应该是
“filling in”，即我们原来了解过的人脑的 “自动补全”
功能：人眼虽然有盲点但我们视觉没有盲点，自动将三个角补全成三角形，描绘出被树遮挡的大楼的样子，等等。Your
visual cortex is drawing on memories of similar patterns and is making a
continuous stream of predictions that fill in for any missing input.</p>
<p>Behavior Cortex Intelligence 之间到底是个什么关系？
从进化历程来看，cortex 起到什么作用？我们为什么要进化出 Cortex: in the
beginning, the cortex served to make more efficient use of existing
behaviors, not to create entirely new behaviors.
但是后来在进化过程中有了 new behavior？</p>
<ol type="1">
<li><p>Reptile: Keen senses and well-developed brains endowed them with
complex behavior, but relatively rigid</p></li>
<li><p>Mammals: Neocortex covering the old brain (reptile brain)</p>
<p>Now sensory patterns are simultaneously fed into the neocortex and
the old brain. The recalled memory is compared with the sensory input
stream. It both “fills in” the current input and predicts what will be
seen next.</p></li>
<li><p>Humans:</p>
<ul>
<li>large front part of cortex for high-level planning and thought, so
it could store more sophisticated types of memories and make predictions
based on complex relationships</li>
<li>motor cortex makes more connections with our muscles so cortex
usurps motor control from other parts of the brain (old brain) and now
the cortex can direct behavior to satisfy its predictions.</li>
</ul></li>
</ol>
<p>本部分也反驳了第一章中所谓的人工智能学者的 behavior determines
intelligence 观点：早在 reptile 时期，动物就有了生存本能的
behavior，但是直到 cortex 出现，它们才有了 intelligence。而 cortex
的核心功能就是 prediction.</p>
<p>To make predictions of future events, your neocortex has to store
sequences of patterns. To recall the appropriate memories, it has to
retrieve patterns by their similarity to past patterns (auto-associative
recall). And, finally, memories have to be stored in an invariant form
so that the knowledge of past events can be applied to new situations
that are similar but not identical to the past. How the physical cortex
accomplishes these tasks, plus a fuller exploration of its hierarchy, is
the subject of the next chapter.</p>
<h2 id="how-the-cortex-works">6 How the Cortex Works</h2>
<ul>
<li><p>invariant representation:</p>
<p>Light receptors in retina concentrate in fovea and sparse out in
periphery, so retinal image relayed onto V1 is highly distorted.
However, we don’t perceive any retinal pattern change at all. This is a
result of invariant representation.</p>
<p>In the course of spanning four cortical stages from retina to IT:
cells in retina and V2 are rapidly changing, spatially specific,
tiny-feature recognition cells. When we go to IT region, something
magical happens and the cells become constantly firing, spatially
nonspecific, object recognition cells. (They now fire when seeing a
face, no matter it’s on the left or on the right)</p></li>
<li><p>Integrating the Senses:
我们到现在为止都是讨论同一类型输入预测同一类型结果，实际上 association
area
使得我们也可以预测其他类型的结果，比如视觉输入用来预测听觉，嗅觉等等的结果，亦可以用来指导动作</p></li>
<li><p>A New View of V1: 前文的模型有两个问题：仅当到了 IT
这一层时，我们奇迹般地获得了 invariant
representation；大脑中大部分区域都是像 association area
一样得到多个输入，但我们的模型中好像 V2 只有 V1 一个输入，V4 只有 V2
一个。</p>
<p>To answer these questions, we propose a new model: V1, V2, V4 are not
single cortical regions. Rather, each is a collection of many smaller
subregions. V1 has largest number of little cortical areas. V2 has
fewer, but larger subregions, each connecting to a number of V1’s
subregions. Same for V4 and we have a single IT which has a bird’s eye
view of the entire visual world. Now the job of any cortical region is
to find out how its inputs are related, to memorize the sequence of
correlations between them, and to use this memory to predict how the
inputs will behave in the future. We can say <strong>each region of
cortex forms invariant representation</strong> drawn from the input
areas hierarchically below it.</p></li>
<li><p>A Model of the World: 作者认为世界中 Every object is composed of
a collection of smaller objects, and most objects are part of larger
objects. In an analogous way, memories are stored in the
<strong>hierarchical structure</strong> of the cortex. Time really
matters and information flowing into the brain arrives as a
<strong>sequence of patterns</strong>. 对于每个 cortical
region，它识别出来这个 sequence，将其抽象成一个 name - a constant
pattern of cell
firing，并将这个名字发给他的上级。所以我们也可以说大脑存储的是 Sequence
of Sequences. By collapsing predictable sequences into “named objects”
at each region in our hierarchy, we achieve more and more stability the
higher we go. This creates invariant representations.</p></li>
<li><p>Sequences of Sequences: Two processes are at the essence of
learning. Assume we are sorting out colored papers.</p>
<ul>
<li>bottom-up classification: deciding what color this paper is</li>
<li>top-down sequence recognition: deciding which sequence are we
reading in</li>
</ul>
<p>Notice these two processes help each other. 1. If you know the most
likely sequence for this series of inputs, you will use this knowledge
to decide how to classify the ambiguous input. 2. recognizing any
sequence would be impossible if you hadn’t first classified each piece
of paper.</p>
<p>When we have finally recognized a color sequence, say “red red blue
green”, we just pass this name to the next higher region; just like the
colors to this region, the name is just a pattern to be combined with
other inputs, classified, and then put into yet a higher-order sequence.
The next higher up region doesn’t have to know what it means.</p></li>
<li><p>What a Region of Cortex Looks Like: 我们说过每个 cortical region
有六层 (six layers 从上到下分别为 L1, L2, …, L6 不要跟视觉的 V1 V4 搞混)
但我们一般不把每一层看做人脑的基本单位，而是把 columns running
perpendicular to the layer 看做 basic unit of computation in the cortex.
作者认为它是 basic unit of prediction.</p>
<p>我们接下来讨论 How cortical regions communicate with each other
共有三种方法：</p>
<ul>
<li>Upward Flow: Converge inputs from lower regions goes to the input
layer of the next region through axons</li>
<li>Downward Flow: Axons in layer 1 spread over long distances, so
information flowing down the hierarchy from one column has the potential
to activate many columns in the regions below it.</li>
<li>Lateral Flow: L1 给 L4,5 发指令运动，L4,5
收到指令的同时，不仅向下给肌肉发放运动信号，也把这个消息告诉
thalamus，thalamus 过一会后会把这个消息重新传回给 L1。其中 thalamus
收到来自许多不同 L4, L5 的信息，然后再把这些信息一起返回给所有 L1
，这样本 column 就知道知道周围其他人现在收到的信息。Column not only
knows the sequence name (downward flow from above), but also where we
are within the sequence (activity from other columns)</li>
</ul></li>
<li><p>How a Region of Cortex Works - The Details:</p>
<ul>
<li><p>How does a cortical region classifies inputs?</p>
<p>It’s too complicated, we assume it does</p></li>
<li><p>How does it learn sequences of patterns?</p>
<p>Input from lower region -&gt; layer 4 fires -&gt; layers 2,3,5 fire
-&gt; layer 1 fires to tell the region up some input has come. Fire
together Wire together, so 2,3,5,1 wire together. 2,3,5 now can fire
without a layer 4 input, so they learn to “anticipate” when they should
fire based on firing of 1. Half of input to layer 1 comes from layer 5
in neighboring columns. This information represents what was happening
moments before. It represents columns that were active prior to your
column becoming active. The other half of the input to layer 1 comes
from layer 6 cells in hierarchically higher regions. This information is
more stationary. It represents the name of the sequence you are
currently experiencing. Combining these two information, a
prediction/sequence is formed.</p></li>
<li><p>How does it form a constant “name” for a sequence?</p>
<p>constant names = constant input to the next region during learned
sequences = need to turn off the output of the layer 2 and layer 3 cells
when a column predicts its activity, or, alternately, to make these
cells active when the column can’t predict its activity. Layer 2 cell
represent the name of the sequence and they stay on when we are within
the sequence. Layer 3b cell represents don’t fire when our column
successfully predicts its input but do fire when it doesn’t predict its
activity.</p></li>
<li><p>How does it make specific predictions?</p>
<p>If you expect a fifth (prediction / invariant representation) and
hear a D (specific input). In layer 2 we fire all intervals of fifth. In
layer 4 we fire all intervals starting with D. The intersection between
the two is our specific prediction.</p></li>
</ul></li>
<li><p>Flowing Up and Flowing Down:</p>
<ol type="1">
<li>上层给下层 prediction</li>
<li>当下层得到的输入与 prediction 不符
(unexpected)，我们将此特征传导给更上一层，直到 some higher region can
interpret it as part of its normal sequence of events.</li>
<li>That higher region generates a new prediction and propagates it
down</li>
</ol></li>
<li><p>Can Feedback Really Do that? Feedback synapses are all far away
from cell’s body, so it’s doubted whether the feedback currents can
really make a difference. 但是新研究发现离得远的 synapse
可能有其他特殊的效果（并不确切证实）</p></li>
<li><p>How the Cortex Learns:
比如我们有1,2,3层，一开始单个文字在第3层，随着我们持续学习和不断练习单个文字移到了第2层，相对的，我们在第3层习得短语这个
pattern。This ensures that we free up the top for learning more subtle,
more complex relationships. 这也是我们变得更熟练的原因。</p></li>
<li><p>The Hippocampus:
我们常认为海马体是生成新记忆的中心，在作者的模型中，Hippocampus is the
top region of neocortex. 我们刚刚说 unexpected input 被传输给上层，so if
something gets to the top of the cortical pyramid, it is the information
that can’t be understood by previous experience, the input that is truly
new and unexpected. That’s what stored in Hippocampus, but it won’t be
stored forever. It’s either transferred down to the cortex (长期记忆) or
eventually lost (遗忘)
所谓人在壮中年时对”新事物”的记忆没有那么好实际上是因为这些”新”的东西实际上早已在以前的生活中出现过，所以人对第一次记忆特别深刻，对之后的类似事物就没那么好记性。（它竟然和
How the Cortex Learns 这很扯的一节联起来了）</p></li>
<li><p>An Alternative Path up the Hierarchy: 这里要介绍的是从 Layer5
-&gt; thalamus
的路径。这条路径可开可关，它要么被上层激活打开，要么被下层的 unexpected
input 激活。我们认为这条路径代表注意力，两种开启方式分别对应主动关注(pay
attention)，以及因为奇怪的现象而被动关注 (attention is caught)</p></li>
<li><p>Closing Thoughts:
分享了作者从零想结构写代码最后竟然能跑的例子，但是相对的如果别人只给你看一堆代码结构规划，你可能会怀疑这东西到底能不能跑，类比到脑结构中，怀疑的原因是
it is because our intuitive sense of the capacity of the cortex and the
power of its hierarchical structure is inadequate.</p></li>
</ul>
<h2 id="consciousness-and-creativity">7 Consciousness and
Creativity</h2>
<ul>
<li>Animals and Human Intelligence: Memory and Prediction are the core
of “Intelligence” and they are used by all livings. There is just a
continuum of methods and sophistication in how they do it.
<ol type="1">
<li>One-cell animal: They used DNA as the medium for memory. Individuals
could not learn and adapt within their lifetimes. They could only pass
on the DNA-based memory of the world to their offspring through their
genes.</li>
<li>Modifiable Nervous System: An individual could now learn about the
structure of its world and adapt its behavior accordingly within its
lifetime. But an individual still could not communicate this knowledge
to its offspring other than by direct observation. Neocortex was also
created at this time.</li>
<li>Human Intelligence: It begins with the invention of language and the
expansion of our large neocortex. The more important is language. We
humans can learn a lot of the structure of the world within our
lifetimes, and we can effectively communicate this to many other humans
via language.</li>
</ol></li>
<li>What is Creativity? Recall that we make predictions by combining the
invariant memory recall of what should happen next with the details
pertaining to this moment in time. All cortical predictions are
predictions by analogy. We are being creative when our memory-prediction
system operates at a higher level of abstraction, when it makes uncommon
predictions, using uncommon analogies. 注意 GEB 中也提到说 analogy
是智慧的核心</li>
<li>What is Consciousness? 有人认为 consciousness/mind
在身体之外，但是实际上它就在脑中。Your thoughts, which are located in
the brain, are physically separate from the body and the rest of the
world. Mind is independent of body, but not of brain.</li>
</ul>
<h2 id="the-future-of-intelligence">8 The Future of Intelligence</h2>
<blockquote>
<p>Because I have been immersed in the neuroscience and computer fields
for over two decades, perhaps my brain has built a high-level model of
how technological and scientific change occurs, and that model predicts
rapid progress. Now is the turning point.</p>
</blockquote>
<ul>
<li>General Direction of Intelligent Machine: Our intelligent machine
may have a set of senses that differ from a human’s. attach to these
senses a hierarchical memory system that works on the same principles as
the cortex. We will then have to train the memory system much as we
teach children. Over repetitive training sessions, our intelligent
machine will build a model of its world as seen through its senses. The
intelligent machine must learn via observation of its world. Once our
intelligent machine has created a model of its world, it can then see
analogies to past experiences, make predictions of future events.
这个智能机器的整体运作方法和大脑相同，但是它并不需要与大脑长得相似或得到和大脑相同的输入，它只需要复合结构的，能够用来作“预测”的输入即可。What
makes it intelligent is that it can understand and interact with its
world via a hierarchical memory model and can think about its world in a
way analogous to how you and I think about our world.</li>
<li>Ethical Problems? No. The strongest applications of intelligent
machines will be where the human intellect has difficulty, areas in
which our senses are inadequate, or in activities we find boring. In
general, these activities have little emotional content.</li>
<li>In the following areas, Intelligent Machines will exceed we humans:
<ul>
<li>Speed: Transistor switch is much faster than human brain’s
electrical signals.</li>
<li>Capacity: we can add capacity to machine’s mind by doing the
followings (these are also what we do in DL/ML)
<ul>
<li>Adding depth to the hierarchy will lead to deeper understanding: the
ability to see higher-order patterns.</li>
<li>Enlarging the capacity within regions will allow the machine to
remember more details, or perceive with greater acuity.</li>
<li>Adding new senses and sensory hierarchies permits the device to
construct better models of the world</li>
</ul></li>
<li>Replicability: we humans learn knowledge and form our own model of
the world rather slowly. However, an intelligent machine need not
undergo this long learning curve, since chips and other storage can be
replicated endlessly and the contents transferred easily.</li>
<li>Sensory Systems: Input patterns to the machine don’t have to be
analogous to animal senses, or even to derive from the real world at
all. In fact, the author suspects that out inability to tackle issue may
be related to a mismatch between the human senses and the physical
phenomena we want to understand. Intelligent machines can have custom
senses more sensitive than our own, or senses that are distributed, or
senses for very small phenomena. They might think in three, four, or
more dimensions.</li>
</ul></li>
</ul>
<h2 id="appendix-the-thousand-brain-theory">Appendix: The Thousand Brain
Theory</h2>
<p>Notes from <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=5LFo36g4Lug">Microsoft Research -
The Thousand Brains Theory by Jeff Hawkins</a></p>
<h3 id="local-cortical-circuit">Local Cortical Circuit</h3>
<p>Inside a local cortical circuit, neurons are organized in layers.
Most connections go vertically across the layers; limited connections go
horizontally within layer. Recent find: all layers have a motor output.
So it’s always sensorimotor input, no pure sensory input.</p>
<p>Vernon Mountcastle: neocortex is remarkably uniform in appearance and
structure because they are actually performing the same basic intrinsic
function. A cortical column is the unit of replication. If you
understand one of it, you understand the whole brain.</p>
<ul>
<li><p>Layer 2,3 - object</p></li>
<li><p>Layer 4 - main input layer</p></li>
<li><p>Layer 6 - location relative to the object</p></li>
</ul>
<p>L6 sends information to L4, L4 processes these information with its
own other input. Over time it forms a representation of what the object
itself is in layer L2,3. On top of that, if we have multiple cortical
involved (imagine multiple fingers touching the cup instead of only
one), we can instantly build a mental image of the cup by the
connections across cortical units happened in L2,3. This is like a
voting mechanism where each finger has a guess of its feeling and they
settle what the object really is by talking to each other.</p>
<h3 id="building-a-reference-map">Building a Reference Map</h3>
<p>A reference map is the sense of relative location as we are touching
the cup</p>
<p>Contrast to the classical view, the vast majority of connections
between cortical regions are not hieratical at all.</p>
<p>Hypothesis: the grid cells in entorhinal cortex also exist in every
cortical column of every neocortex region. They don’t create reference
frames for location but reference frames for the objects we interact
(the cup).</p>
<p>In the classical view, we have a hierarchy in our neocortex. The real
structure is similar, 但我们并不是 杯柄 -&gt; 杯身 -&gt; 整个杯子
这种真正的阶梯式建模，而是每个“层级”都形成一个自己的杯子模型，这些模型并不相同.
This model allows all models to “vote”. Everyone tries to guess what’s
going on.</p>

    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>Author：<a href="https://yao-lirong.github.io/blog">Yao Lirong</a>
            </p><p>Link：<a href="https://yao-lirong.github.io/blog/on-intelligence/">https://yao-lirong.github.io/blog/on-intelligence/</a>
            </p><p>Publish date：<a href="https://yao-lirong.github.io/blog/on-intelligence/">June 23rd 2021, 12:00:00 am</a>
            </p><p>Update date：<a href="https://yao-lirong.github.io/blog/on-intelligence/">June 8th 2022, 3:33:42 pm</a>
            </p><p>License：本文采用 <a rel="external nofollow noopener noreferrer" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a> 进行许可</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/blog/install-and-configure-aria2-on-wsl/" title="Install and Configure Aria2 on WSL">
                    <div class="nextTitle">Install and Configure Aria2 on WSL</div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/blog/tensorflow-1-x-manual/" title="TensorFlow 1.x Manual">
                    <div class="prevTitle">TensorFlow 1.x Manual</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->

            
            
            
            <!-- utteranc评论 -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
    <!---->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="//github.com/Yao-Lirong" class="iconfont-archer github" target="_blank" title="github"></a>
                <a href="//twitter.com/yao_lirong" class="iconfont-archer twitter" target="_blank" title="twitter"></a>
                <a href="//www.linkedin.com/in/yao-lirong/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
                <a href="/blog/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank" rel="external nofollow noopener noreferrer">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- 不蒜子  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style="top:50vh;">
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#artificial-intelligence"><span class="toc-number">1.</span> <span class="toc-text">1 Artificial Intelligence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#neural-networks"><span class="toc-number">2.</span> <span class="toc-text">2 Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-human-brain"><span class="toc-number">3.</span> <span class="toc-text">3 The Human Brain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#memory"><span class="toc-number">4.</span> <span class="toc-text">4 Memory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a-new-framework-of-intelligence"><span class="toc-number">5.</span> <span class="toc-text">5 A New Framework of
Intelligence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-the-cortex-works"><span class="toc-number">6.</span> <span class="toc-text">6 How the Cortex Works</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#consciousness-and-creativity"><span class="toc-number">7.</span> <span class="toc-text">7 Consciousness and
Creativity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-future-of-intelligence"><span class="toc-number">8.</span> <span class="toc-text">8 The Future of Intelligence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#appendix-the-thousand-brain-theory"><span class="toc-number">9.</span> <span class="toc-text">Appendix: The Thousand Brain
Theory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#local-cortical-circuit"><span class="toc-number">9.1.</span> <span class="toc-text">Local Cortical Circuit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#building-a-reference-map"><span class="toc-number">9.2.</span> <span class="toc-text">Building a Reference Map</span></a></li></ol></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 72
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/25</span>
            <a class="archive-post-title" href="/blog/matryoshka-representation-learning-adaptive-retrieval-and-binary-vector-search/">Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/15</span>
            <a class="archive-post-title" href="/blog/youtube-recommendation-algorithms-2016/">YouTube Recommendation Algorithms (2016)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span>
            <a class="archive-post-title" href="/blog/running-mobilebert-on-android-with-tensorflow-lite/">Running MobileBert on Android with TensorFlow Lite</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span>
            <a class="archive-post-title" href="/blog/variational-inference/">Variational Inference</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/hyper-parameter-tuning-with-optuna/">Hyper-Parameter Tuning with Optuna</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/kv-cache/">KV Cache</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/conducting-multi-round-conversation-with-transformers/">Conducting Multi-Round Conversation with Transformers</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/14</span>
            <a class="archive-post-title" href="/blog/gpt-4o-release/">GPT-4o Release</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/22</span>
            <a class="archive-post-title" href="/blog/clip/">CLIP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span>
            <a class="archive-post-title" href="/blog/gradient-scaling/">Gradient Scaling</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/13</span>
            <a class="archive-post-title" href="/blog/decoupled-weight-decay-regularization-sgdw-adamw/">Decoupled Weight Decay Regularization (SGDW & AdamW)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span>
            <a class="archive-post-title" href="/blog/mixed-precision-training/">Mixed-Precision Training</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/22</span>
            <a class="archive-post-title" href="/blog/parameter-and-flop-count-in-transformer-model/">Parameter and FLOP Count in Transformer Model</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/memory-pinning-and-transfer-data-between-host-cpu-and-device-gpu/">Memory Pinning and Transfer Data between Host (CPU) and Device (GPU)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/27</span>
            <a class="archive-post-title" href="/blog/switching-personal-homepage-theme-to-al-folio/">Switching Personal Homepage Theme to al-folio</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span>
            <a class="archive-post-title" href="/blog/visual-information-theory/">Visual Information Theory</a>
        </li>
                </ul>
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span>
            <a class="archive-post-title" href="/blog/quantization/">Quantization</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span>
            <a class="archive-post-title" href="/blog/fine-tuning-llms-prompt-tuning-adapter-lora/">Fine-Tuning LLMs: Prompt Tuning, Adapter, LoRA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span>
            <a class="archive-post-title" href="/blog/graph-networks-graphcast/">Graph Networks & GraphCast</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/04</span>
            <a class="archive-post-title" href="/blog/first-time-debugging-with-chatgpt/">First Time Debugging with ChatGPT</a>
        </li>
                </ul>
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2022-web-journal/">2022 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/01</span>
            <a class="archive-post-title" href="/blog/%E8%A7%A6%E4%B9%90-rpg-codex-rpg%E6%96%87%E6%9C%AC%E5%86%99%E4%BD%9C%E8%AE%A8%E8%AE%BA/">触乐 & RPG Codex: RPG文本写作讨论</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/04</span>
            <a class="archive-post-title" href="/blog/deploy-a-reddit-bot-on-heroku/">Deploy a Reddit Bot on Heroku</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span>
            <a class="archive-post-title" href="/blog/how-to-succeed-in-cs6784-also-in-academic-life-in-general/">How to Succeed in CS6784 (also in Academic Life in General)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span>
            <a class="archive-post-title" href="/blog/javascript-manual/">JavaScript Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span>
            <a class="archive-post-title" href="/blog/%E5%8D%9A%E5%AE%A2seo%E4%BC%98%E5%8C%96/">博客SEO优化</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/09</span>
            <a class="archive-post-title" href="/blog/video-editing-ffmpeg-davinci/">Video Editing (FFmpeg DaVinci)</a>
        </li>
                </ul>
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2021-web-journal/">2021 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/15</span>
            <a class="archive-post-title" href="/blog/look-back-on-cornell-21fa/">Look Back on Cornell 21FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span>
            <a class="archive-post-title" href="/blog/sql-manual/">SQL Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span>
            <a class="archive-post-title" href="/blog/java-quick-guide/">Java Quick Guide</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span>
            <a class="archive-post-title" href="/blog/c-manual/">C Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/29</span>
            <a class="archive-post-title" href="/blog/%E6%9B%B4%E6%96%B0archer%E4%B8%BB%E9%A2%98-%E8%BF%81%E7%A7%BBhexo%E5%8D%9A%E5%AE%A2/">更新archer主题 / 迁移Hexo博客</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span>
            <a class="archive-post-title" href="/blog/install-and-configure-aria2-on-wsl/">Install and Configure Aria2 on WSL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/23</span>
            <a class="archive-post-title" href="/blog/on-intelligence/">On Intelligence</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/28</span>
            <a class="archive-post-title" href="/blog/tensorflow-1-x-manual/">TensorFlow 1.x Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span>
            <a class="archive-post-title" href="/blog/look-back-on-cornell-21sp/">Look Back on Cornell 21SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/15</span>
            <a class="archive-post-title" href="/blog/setting-up-a-server/">Setting up a Server</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span>
            <a class="archive-post-title" href="/blog/tsinghua-dsa-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-3/">Tsinghua DSA 作业总结 (3)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/tsinghua-dsa-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-2/">Tsinghua DSA 作业总结 (2)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/tsinghua-dsa-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-1/">Tsinghua DSA 作业总结 (1)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span>
            <a class="archive-post-title" href="/blog/look-back-on-cornell-tsinghua-20fa/">Look Back on Cornell/Tsinghua 20FA</a>
        </li>
                </ul>
            <div class="archive-year"> 2020 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/blog/2020-web-journal/">2020 Web Journal</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/c-manual/">C++ Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span>
            <a class="archive-post-title" href="/blog/python-manual/">Python Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span>
            <a class="archive-post-title" href="/blog/latex-manual/">LaTeX Manual</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/17</span>
            <a class="archive-post-title" href="/blog/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">Python网络爬虫与信息提取</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/13</span>
            <a class="archive-post-title" href="/blog/cs4820-%E5%8F%8A-algorithm-design-%E4%B8%80%E8%88%AC%E6%80%A7%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93/">CS4820 及 Algorithm Design 一般性内容总结</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/02</span>
            <a class="archive-post-title" href="/blog/info1998-intro-to-machine-learning-sklearn-pandas/">INFO1998 Intro to Machine Learning (sklearn, pandas)</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/29</span>
            <a class="archive-post-title" href="/blog/add-open-with-windows-terminal-to-right-click-menu/">Add "Open with Windows Terminal" to Right-Click Menu</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span>
            <a class="archive-post-title" href="/blog/cs2024-c-programming/">CS2024 C++ Programming</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/blog/windows%E4%B8%8B%E9%85%8D%E7%BD%AEpostgresql/">Windows下配置PostgreSQL</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span>
            <a class="archive-post-title" href="/blog/kinect-as-web-cam/">Kinect as Web Cam</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/27</span>
            <a class="archive-post-title" href="/blog/look-back-on-cornell-20sp/">Look Back on Cornell 20SP</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span>
            <a class="archive-post-title" href="/blog/introduction-to-vim/">Introduction to Vim</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/24</span>
            <a class="archive-post-title" href="/blog/cs2043-unix-tools-and-scripting/">CS2043 Unix Tools and Scripting</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/20</span>
            <a class="archive-post-title" href="/blog/installing-and-configuring-ocaml-on-linux/">Installing and Configuring Ocaml on Linux</a>
        </li>
                </ul>
            <div class="archive-year"> 2019 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span>
            <a class="archive-post-title" href="/blog/look-back-on-cornell-19fa/">Look Back on Cornell 19FA</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">12/17</span>
            <a class="archive-post-title" href="/blog/add-pdf-file-to-hexo/">Add pdf file to hexo</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/blog/import-junit-and-javafx-into-vscode/">Import Junit and JavaFx into VSCode</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span>
            <a class="archive-post-title" href="/blog/p1162-%E5%A1%AB%E6%B6%82%E9%A2%9C%E8%89%B2/">P1162 填涂颜色</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span>
            <a class="archive-post-title" href="/blog/p1141-01%E8%BF%B7%E5%AE%AB/">P1141 01迷宫</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/blog/p1118-backward-digital-sums/">P1118 Backward Digital Sums</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/26</span>
            <a class="archive-post-title" href="/blog/p1019-%E5%8D%95%E8%AF%8D%E6%8E%A5%E9%BE%99/">P1019 单词接龙</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span>
            <a class="archive-post-title" href="/blog/p1101-%E5%8D%95%E8%AF%8D%E6%96%B9%E9%98%B5/">P1101 单词方阵</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/16</span>
            <a class="archive-post-title" href="/blog/p1219-%E5%85%AB%E7%9A%87%E5%90%8E/">P1219 八皇后</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span>
            <a class="archive-post-title" href="/blog/p1031-%E5%9D%87%E5%88%86%E7%BA%B8%E7%89%8C/">P1031 均分纸牌</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/03</span>
            <a class="archive-post-title" href="/blog/p2678-%E8%B7%B3%E7%9F%B3%E5%A4%B4/">P2678 跳石头</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/p1090-%E5%90%88%E5%B9%B6%E6%9E%9C%E5%AD%90/">P1090 合并果子</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span>
            <a class="archive-post-title" href="/blog/p1309-%E7%91%9E%E5%A3%AB%E8%BD%AE/">P1309 瑞士轮</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span>
            <a class="archive-post-title" href="/blog/introduction-to-git-command/">Introduction to Git Command</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/09</span>
            <a class="archive-post-title" href="/blog/hello-world/">Hello World</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="NOI">
                <span class="iconfont-archer">&#xe606;</span>
                NOI
            </span>
            <span class="sidebar-tag-name" data-tags="Logistics">
                <span class="iconfont-archer">&#xe606;</span>
                Logistics
            </span>
            <span class="sidebar-tag-name" data-tags="Cornell">
                <span class="iconfont-archer">&#xe606;</span>
                Cornell
            </span>
            <span class="sidebar-tag-name" data-tags="Review">
                <span class="iconfont-archer">&#xe606;</span>
                Review
            </span>
            <span class="sidebar-tag-name" data-tags="CS3110">
                <span class="iconfont-archer">&#xe606;</span>
                CS3110
            </span>
            <span class="sidebar-tag-name" data-tags="Vim">
                <span class="iconfont-archer">&#xe606;</span>
                Vim
            </span>
            <span class="sidebar-tag-name" data-tags="Manual">
                <span class="iconfont-archer">&#xe606;</span>
                Manual
            </span>
            <span class="sidebar-tag-name" data-tags="Journal">
                <span class="iconfont-archer">&#xe606;</span>
                Journal
            </span>
            <span class="sidebar-tag-name" data-tags="Tsinghua">
                <span class="iconfont-archer">&#xe606;</span>
                Tsinghua
            </span>
            <span class="sidebar-tag-name" data-tags="Book">
                <span class="iconfont-archer">&#xe606;</span>
                Book
            </span>
            <span class="sidebar-tag-name" data-tags="ML">
                <span class="iconfont-archer">&#xe606;</span>
                ML
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/blog/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://yao-lirong.github.io/blog",
        root: siteMetaRoot,
        author: "Yao Lirong"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/blog/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/blog/scripts/share.js" async></script>
        <!-- mermaid -->
    </body>
</html>

{"meta":{"title":"Yao Lirong's Blog","subtitle":null,"description":"姚立嵘 (Yao Lirong)'s Personal Website","author":"Yao Lirong","url":"https://yao-lirong.github.io/blog","root":"/blog/"},"pages":[{"title":"肥肠抱歉，出错了","date":"2025-04-23T16:14:56.051Z","updated":"2021-09-24T03:43:52.000Z","comments":true,"path":"404.html","permalink":"https://yao-lirong.github.io/blog/404.html","excerpt":"","text":""},{"title":"Hello, World","date":"2019-06-27T04:00:00.000Z","updated":"2021-09-24T03:35:56.000Z","comments":true,"path":"about/index.html","permalink":"https://yao-lirong.github.io/blog/about/index.html","excerpt":"","text":"Cornell CS ’2023 Credits Favicon - Concentric Rinds by M.C. Escher Homepage - Encounter by M.C. Escher About Me page - Day and Night by M.C. Escher"}],"posts":[{"title":"Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search","slug":"2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search","date":"2024-12-25T05:00:00.000Z","updated":"2025-09-15T17:52:19.640Z","comments":true,"path":"2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/","permalink":"https://yao-lirong.github.io/blog/2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/","excerpt":"Introduces ways to make retrieval quicker","text":"Intro to Matryoshka Representation Learning In Matryoshka Representation Learning (MRL), we want to construct an encoding ed with dimension d such that its truncations of different lengths (ed/16, ed/8, ed/4, ed/2​) are each (somewhat) valid representations. Suppose you’re training on a classification problem with the classic encoder + classifier head architecture. At train time: classic setting: you just use the vector ed as input to the classifier head MRL: construct multiple classifier heads (in our case 5) and put one on top of encoding of each length (ed/16, …, ed) and average the loss of each classifier head. So we build heads of size [d, num_class], [d/2, num_class], ... [d/16, num_class] Note these classifier heads share weights. Application: Adaptive Retrieval Online retrieval is one of the tasks where latency matters the most. Given a user query q, it is slow to compute KNN from a dataset of size 1M (106) indexes if each index has dimension 3072. With MRL, we can decompose the process into two stages: Shortlist: First retrieve 2K indexes where the distance is computed using only 1024-d vector (the first 1024 elements of the 3072 vector) Rerank: Find KNN among these 2K indexes where the distance is computed using the full length 3072 vector The FLOP is therefore reduced from 3072 × 106 to 1024 × 106 + 3072 × 2K. Ce Gao tested full length 3072-dim vector vs adaptive retrieval using Matryoshka 1024-dim. The accuracy dropped from 99% to 89% with Requests Per Second (RPS) raises from 300 to 1000. Find more details of Matryoshka Representation Learning and its applications in this wonderful blog post. Read from section What is MRL? (Really this Time) Binary Vector Search Ce Gao suggested another way to reduce memory and FLOP use. He proposes to turn the length d FP32 vector into a length d binary vector, where original positive value is set to 1 and original negative value is set to 0. Without using adaptive retrieval, the accuracy dropped from 99% to 83%, but the latency (RPS = 3000) and memory has a significant improvement because previously one single vector / encoding consists of d 32-bit number, whereas now it only consists of d 1-bit number. If you adapt the Adaptive Retrieval setup mentioned earlier: Shortlist: retrieve 2K indexes using full-length but binary vector Rerank: find KNN among 2K indexes using full-length, FP32 vector you get a precision drop from 99% only to 96% with an RPS of 1700. P.S. I discovered this method on Simon Willison’s blog.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"YouTube Recommendation Algorithms (2016)","slug":"2024-10-15-YouTube-Recommendation-Algorithms-(2016)","date":"2024-10-15T04:00:00.000Z","updated":"2025-09-03T01:42:46.382Z","comments":true,"path":"2024-10-15-YouTube-Recommendation-Algorithms-(2016)/","permalink":"https://yao-lirong.github.io/blog/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/","excerpt":"This is a detailed reading of Google’s paper Deep Neural Networks for YouTube Recommendations","text":"This is a detailed reading of Google’s paper Deep Neural Networks for YouTube Recommendations Candidate Generation Problem Setup We pose recommendation as an extreme multi-class classification problem where we predict which video will be watched next. Specifically, we classify a specific video watch wt at time t among millions of videos i (classes) from a video corpus V based on a user U and context C. And $u \\in \\R^d$ represents a high-dimensional “embedding” of the user-context pair and the $v_j \\in \\R^d$ represent embeddings of each candidate video. P(wt = i ∣ U, C) = Softmax(V, u)i Input what we didn’t use: explicit feedback (thumbs up/down, in-product surveys) because there’s too few of them in the tail of videos. embedded video watches: representation of each video: in my understanding, YouTube didn’t extract information from their videos, and fed the extracted info into the embedder. Instead, they directly fed the video ids into the embedder. To my understanding, “Inspired by continuous bag of words language models” means they fed the video ids into the embedder, just like NLP feeds BoW representation into embedder. It doesn’t mean YouTube decomposes a video into word count like BoW. I reached this conclusion from the last sentence of 4.1 Feature Representation - Embedding Categorical Features, where 1000000*32 / (2048*2048) = 7 The overwhelming majority of model parameters are in these high-cardinality embedding spaces - for example, one million IDs embedded in a 32 dimensional space have 7 times more parameters than fully connected layers 2048 units wide. representation of watch history: watch history is a variable-length list of videos. YouTube used average-pooling to transform them into a fixed-length vector. why order-independent pooling? embedded search tokens: each query is tokenized into unigrams and bigrams. each token is embedded. All these embedded tokens from all queries are then pooled and fed into the model as a summarized dense search history. geographic embeddings: The user’s geographic region and device are embedded and concatenated. Simple binary and continuous features: such as the user’s gender, logged-in state and age are input directly into the network as real values normalized to [0, 1]. example (sample) age: ML model often has bias towards past samples because there’s more data about them. It’s common to promote video freshness during re-rank phase, but YouTube also try to reduce this bias as early as in candidate generation phase. We define the example age to be the time between training and obtaining this sample. e.g. t days earlier, after having watched video v, user searched word w and clicked on another video y. We use this sample in training, so its sample age is t. By introducing this feature, our model no longer reflects the average watch likelihood in the training window of several weeks, but the likelihood at a specific time step. At serving time, this feature is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window. 总结某知乎讨论下的内容: example age 和消除第一种 ad position bias 做法类似 纯feed应用中前置内容点击率被高估：新闻客户端，position靠前的是虚高的，这部分叫 position bias，输入给模型的时候也要输入它们的位置，在线上预估时置0 搜索偏向应用中前置内容点击率被低估：手百，手淘，美团等，都会在首页默认展示feed，但很多目的明确的用户压根不会用这些推荐功能，导致这部分展示的内容点击率是被低估了。实际操作中大家可能只针对有feed互动的用户进行采样，抛弃了完全过路型用户的行为，也算是修正bias了 Data Gathering Details Class Balance: generate a fixed number of training examples per user, effectively weighting our users equally in the loss function Permutation-Invariant Pooling: in pooling, YouTube chose average pooling among sum, max, etc, because average performed the best. The important thing is, they decided to abandon sequence information whatsoever. Their explanation is below and I don’t quite buy it because they’re definitely better way to solve this problem than discarding the info altogether. In addition, they did publish another paper on sequence-based recommendation system later. I think at this paper’s publishing time, they either didn’t want to publicize it or have not tested that in detail. Consider the user has just issued a search query for “taylor swift”. Since our problem is posed as predicting the next watched video, a classifier given this information will predict that the most likely videos to be watched are those which appear on the corresponding search results page for “taylor swift”. Unsurprisingly, reproducing the user’s last search page as homepage recommendations performs very poorly. By discarding sequence information and representing search queries with an unordered bag of tokens, the classifier is no longer directly aware of the origin of the label. Next-Watch Heldout: at the time, many collaborative filtering systems implicitly choose the labels and context by holding out a random item and predicting it from other items in the user’s history. They decided to always hold out and predict user’s next watch and achieved much better results. This is now already the standard. In fact it appeared in my college class CS4780 by Killian. Training: In these experiments, a vocabulary of 1M videos and 1M search tokens were embedded with 256 floats each in a maximum bag size of 50 recent watches and 50 recent searches. The softmax layer outputs a multinomial distribution over the same 1M video classes with a dimension of 256 (which can be thought of as a separate output video embedding). negative sampling: for the same user, in addition to his watched videos, we also sample some unwatched videos to generalize the model. Importance Weighting: we do not take softmax over all the 1M videos. Instead, we sample ~5000 of them, compute their probability, re-weight them based on importance (watch time, click rate?) and only compute loss over these samples. loss: this is a multi-class classification problem, so we use cross-entropy loss naturally Inference / Serving: kNN: In serving, we need an algorithm sublinear to number of classes (videos to recommend). Say the last layer of the network has hidden dimension d and we have N videos to predict. Decoding hidden dimension back into per-video logits and take a softmax takes O(dN). Sorting takes O(Nlog N). The total time is O(dN + Nlog N). On the other hand, naive kNN takes O(dN) time in total and some heuristic version like Ball tree can take O(dlog N)​​. distance is based on dot product how do we get video embedding? (where is decoder?) youtube-candidate-generation-architecture All classification models have to include a decoder (FC layer) at the end of the network but before the softmax layer to decode the hidden vector back to per-video logits in video ID space to make prediction. If we have 1M videos and hidden vector is of dimension 256, the decoder is a matrix of size [256, 1M]. However, the graph presented in the paper is very confusing because the authors omit drawing the decoder and made it an implicit part of the softmax layer. Anyway, we know we do have that decoder, so it’s natural to use the vectors in the decoder as our video embedding. The i-th video’s embedding is simply decoder[:, i]. weight sharing / weight tying: this is a concept I encountered in nanoGPT and has become clear here. At the beginning of the network, we have a video encoder from video ID space to hidden space; at the end of the network, we have a video decoder from hidden space back to video ID space. It is possible to share weights (use the same weights) in encoder and decoder to save space (recall this part costs the most parameter). This is just mentioned by people in comment section and is not implemented by Google. Ranking Problem Setup We pose ranking as predicting the expected watch time of a video. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement Input (Feature Engineering) user’s previous interaction with the item itself and other similar items: e.g. user’s past history with the channel that uploaded the video being scored (how many videos has the user watched from this channel? When was the last time the user watched a video on this topic?) propagate information from candidate generation into ranking in the form of features: e.g. which sources nominated this video candidate? What scores did they assign? frequency of past video impressions: If a user was recently recommended a video but did not watch it then the model will naturally demote this impression on the next page load. Input Details embedding space should increase approximately proportional to the logarithm of the number of unique values of data space very large space (video id &amp; query token) is truncated by click-through-rate. So we only recommend videos above a certain CTR. Note these filtered out videos can still be searched out. Out-of-vocabulary values (new / truncated videos) are mapped to the zero embedding. Continuous features are always normalized to [0, 1) In addition to the raw normalized feature x, we also input $\\sqrt x$ and x2​, giving the network more expressive power by allowing it to easily form super- and sub-linear functions of the feature. Feeding powers of continuous features was found to improve offline accuracy. Training We use Logistic Regression to predict expected watch time (EWT) of a video, but LR only predicts 0 or 1. How can we use it to predict EWT? We use weighted logistic regression, where the positive (clicked) impressions are weighted by the observed watch time on the video. Negative (unclicked) impressions all receive unit weight. In training, we use the weighted cross entropy loss: WeightedCrossEntropy = −∑i[Tiyilog pi + (1 − yi)log (1 − pi)] ### Serving In serving, we directly output eθx as the predicted watch time. Why is this the watch time? Recall in Logistic Regression, we have a binary classification problem, so we can define P(Yi = 1|Xi) = p, P(Yi = 0|Xi) = 1 − p and $$ p = \\frac{1}{1 + e^{-\\theta^T x}} $$ In statistics, we define odds as: $$ \\texttt{odds} = \\frac{p}{1-p} = \\frac{1}{e^{-\\theta^Tx}} = e^{\\theta^Tx} $$ If we take a log at both sides, we have the log odds, or logits $$ \\ln(\\texttt{odds}) = \\ln(\\frac{p}{1-p}) = \\theta^Tx $$ Now let’s look at the our weighted logistic regression problem: Positive impressions are weighted by watch time Ti. Negative impressions receive unit weight. We have a total N videos, and k of them are positive (clicked). We have k = pN and the expected watch time of all videos is $\\mathbb E[T] = \\frac{\\sum_i^k T_i}{N}$. Now look at our weighted odds: $$ \\begin{align} \\texttt{wghted odds} &amp;= \\frac{\\text{weighted pos prob}}{\\text{weighted neg prob}}\\\\ &amp;= \\frac{\\sum_i^k T_i}{N-k} = \\frac{\\sum_i^k T_i}{N-pN} = \\frac{\\sum_i^k T_i}{N(1-p)} = \\frac{\\sum_i^k T_i}{N}\\frac{1}{1-p}\\\\ &amp; = \\mathbb E[T](1+p) \\approx \\mathbb E[T] \\hspace{20px}\\text{($p$ is small)} \\end{align} $$ Therefore, at serving time, we can directly output eθx because it is the expected watch time. Evaluation Metric Since we’re essentially predicting a video’s watch time, we “wrongly-predicted video’s watch time” as our evaluation metric. In the paper, author called it “weighted, per-user loss”. Specifically, for each user, we feed the model both positive (clicked) and negative (unclicked) impressions shown to him on a single page. We first predict their respective watch time with our model. “mispredicted watch time” is the positive video’s watch time when the negative impression receives a longer predicted watch time than the positive impression. This user’s loss is total mispredicted watch time / total watch time of ground-truth (positive) impressions. Takeaway example age expected watch time KNN - quick serving feature engineering what is surrogate problem? https://www.youtube.com/watch?v=WK_Nr4tUtl8 Comment RNN这个方法在17年已经由Alex Beutel做上线了，其实在16年初就想做，只是效果还没有完全出来，后来Alex发现了原先做法的一些弱点，很快改进之后就上线了，作为重要的candidates generation来源；排序目标只写了EWT，一是Google的技术保密要求，毕竟还是要做到HR-safe的，论文只能点到即止，二是相对有效并且能够比分开预测ctr和staytime能节省serving latenc – 严林 under 重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Running MobileBert on Android with TensorFlow Lite","slug":"2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite","date":"2024-09-22T04:00:00.000Z","updated":"2025-09-03T01:42:46.391Z","comments":true,"path":"2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/","permalink":"https://yao-lirong.github.io/blog/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/","excerpt":"So Google, fxxk you.","text":"So Google, fxxk you. Prerequsities This picture very well explains how TFLite works and also why TensorFlow 2 has both a tf and a keras. TFLite Workflow Detours This section is mostly rant, but it is meaningful in preventing you from taking any of the wrong path. Skip to the next section for a tutorial on what to do. We first found the Google’s official release http://google-research/mobilebert/, but the tutorial was unclear: Why do I need data_dir and output_dir to export TFLite? How do I even read in the pre-trained weights? the code itself was pretty messy: why did they have export function and training function all at this same file run_squad.py and the only way to tell the program whether to train/export is checking whether export_dir is None rather than passing a flag? In figuring out what each part does in this code, I looked up TensorFlow 1’s doc and good lord they were broken. Google doesn’t even host it anywhere: you have to go to a GitHub repo to read them in .md format. At this moment I decided I will not touch anything written by TensorFlow 1’s API. (I actually went through this pain back at my first ML intern in Haier, but not again) Sidenote before this: I didn’t know you can release model’s on Kaggle (thought everyone releases on Hugging Face) and Google moved their own TensorFlow Hub to Kaggle So my supervisor found me a more readable Google release on Kaggle with some high-level API and doesn’t require you to read the painful source code. The above link has a redirect to TensorFlow 2 implementation with an official TFLite release. How neat. However, the official TFLite release doesn’t have signature - TensorFlow’s specification of input and output (remember when you pass inputs to a model you need to give name to them e.g. token_ids = ..., mask = ...) which is required for Xiaomi Service Framework to run a TFLite. P.S. Yes signature is not required to specify when exporting, but for god’s sake all your tutorial teaches people to use it and your own released ditched it? WTF Google. is broken (as expected?). When I tried to run it on my PC, I got the following error indices_has_only_positive_elements was not true.gather index out of boundsNode number 2 (GATHER) failed to invoke.gather index out of boundsNode number 2 (GATHER) failed to invoke. Someone encountered a similar bug while running the example code provided by TensorFlow and the Google SWE found a bug in their example. At this moment I decided not to trust this TFLite file anymore and just convert it on my own. So let’s use this official TensorFlow 2 implementation and convert it to TFLite. It was all good and running on my PC, but Its output format was really weird It output consists of 'mobile_bert_encoder', 'mobile_bert_encoder_1', 'mobile_bert_encoder_2', ..., 'mobile_bert_encoder_51' Each of these has shape (1, 4, 128, 128) for a seq_length = 128, hidden_dim = 512 model. I figured 4 being the number of heads and the other 128 is hidden_dim for each head. They output attention scores, not the final encoded vector: my input was 5 tokens and they output is output[0, 0, 0, :] = array([0.198, 0.138, 0.244, 0.148, 0.270, 0. , 0. , .... They sum to 1 and any other positions at output are 0 , so attention score was my best guess. It doesn’t run on Android phone: tflite engine load failed due to java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 153. Are you using old TFLite binary with newer model? A Stack Overflow answer suggests the TensorFlow used to export TFLite running on my PC doesn’t match the version of TFLite run time on this Android phone. It can also be caused by me messing up with the whole environment while installing Optimum to export TFLite last night, but I didn’t bother to look because I finally found the solution And comes the savior, the king, the go-to solution in MLOps - Huggingface. Reminded by a discussion I read by chance, I came to realize TFMobileBertModel.from_pretrained actually returns the Keras model (and the without TF version returns a PyTorch model). That means I can just use Hugging Face API to read it in, then use the native TensorFlow 2 API to export to TFLite. And everything works like a charm now. The final output signature is just Hugging Face’s familiar ['last_hidden_state', 'pooler_output'] Converting TensorFlow Model to TFLite Conversion is pretty straight forward. You can just follow this official guide: For Mobile &amp; Edge: Convert TensorFlow models. Though I actually followed my predecessor’s note (which actually comes from another TF tutorial). He also told me to caution that calling tf.disable_eager_execution() can lead to absence of signature, so do not call tf.disable_eager_execution() to disable eager mode. 1234567891011121314151617181920212223from transformers import MobileBertTokenizerFast, TFMobileBertModel# Convert Modelif be_sane: bert_model = TFMobileBertModel.from_pretrained(kerasH5_model_path) if keras_file else \\ TFMobileBertModel.from_pretrained(pytorch_model_path, from_pt = True) converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)else: # be crazy or already knows the messy TensorFlow.SavedModel format converter = tf.lite.TFLiteConverter.from_saved_model(model_path)tflite_model = converter.convert()# Save Modeltflite_output_path = &#x27;/model.tflite&#x27;with open(tflite_output_path, &#x27;wb&#x27;) as f: f.write(tflite_model)# Check Signature# Empty signature means error in the export process and the file cannot be used by Xiaomi Service Frameworkinterpreter = tf.lite.Interpreter(model_path=tflite_output_path)interpreter = tf.lite.Interpreter(model_content=tflite_model)interpreter.allocate_tensors()signatures = interpreter.get_signature_list()print(&quot;tflite model signatures:&quot;, signatures) 1234&#123;&#x27;serving_default&#x27;: &#123;&#x27;inputs&#x27;: [&#x27;attention_mask&#x27;,&#x27;input_ids&#x27;,&#x27;token_type_ids&#x27;],&#x27;outputs&#x27;: [&#x27;last_hidden_state&#x27;, &#x27;pooler_output&#x27;]&#125;&#125; In addition, summarizing from the detours I took, Do not use Hugging Face’s Optimum for (at least vanilla) conversion because it just calls the above command (see code) Do not even bother to look at Google’s original code converting MobileBert to TFLite because nobody knows what they’re writing. Running TFLite (on PC) Running TFLite on Android phone is the other department’s task. I just want to run the TFLite file on PC to test everything’s good. To do that, I strictly followed TensorFlow’s official guide: TensorFlow Lite inference: Load and run a model in Python.Our converted models have the signatures, you can just follow the “with a defined SignatureDef” guide. 12345678tokenizer = MobileBertTokenizerFast(f&quot;&#123;model_path&#125;/vocab.txt&quot;)t_output = tokenizer(&quot;越过长城，走向世界&quot;, return_tensors=&quot;tf&quot;)ii, tt, am = t_output[&#x27;input_ids&#x27;], t_output[&#x27;token_type_ids&#x27;], t_output[&#x27;attention_mask&#x27;]# `get_signature_runner()` with empty input gives the &quot;serving_default&quot; runner# `runner` input parameter is specified by `serving_default[&#x27;inputs&#x27;]`runner = interpreter.get_signature_runner() output = runner(input_ids = ii, token_type_ids = tt, attention_mask = am)assert output.keys == [&#x27;last_hidden_state&#x27;, &#x27;pooler_output&#x27;] On the other hand, for a model without signatures, you need to use the more primitive API input_details and output_details. They specify the following properties, where index is (probably) the index of this tensor in the compute graph. To pass input values and get output values, you need to access them by this index. 12345678interpreter.allocate_tensors()input_details = interpreter.get_input_details()output_details = interpreter.get_output_details()interpreter.set_tensor(input_details[0][&#x27;index&#x27;], input_data)interpreter.invoke()output_data = interpreter.get_tensor(output_details[0][&#x27;index&#x27;])print(output_data) The following is the input_details of the non-signature Google packed MobileBert. 123456789101112interpreter.get_input_details()[&#123;&#x27;name&#x27;: &#x27;model_attention_mask:0&#x27;, &#x27;index&#x27;: 0, &#x27;shape&#x27;: array([ 1, 512], dtype=int32), &#x27;shape_signature&#x27;: array([ 1, 512], dtype=int32), &#x27;dtype&#x27;: numpy.int64, &#x27;quantization&#x27;: (0.0, 0), &#x27;quantization_parameters&#x27;: &#123;&#x27;scales&#x27;: array([], dtype=float32), &#x27;zero_points&#x27;: array([], dtype=int32), &#x27;quantized_dimension&#x27;: 0&#125;, &#x27;sparsity_parameters&#x27;: &#123;&#125;&#125;, &#123;...&#125;] Numerical Accuracy Our original torch/TensorFlow encoder and the converted TFLite encoder, when both running on PC using Python, has a 1.2% difference in their output (last_hidden_state or pooled_output). We do not know where this discrepancy comes from. Converting Tokenizer to TFLite We exported and ran the encoder, but that’s not enough. We can’t ask the user to type in token_ids every time. Therefore, we need to integrate the preprocessor (tokenizer) into our TFLite file. To do that, we first tried integrating Google’s official Keras tokenizer implementation into our BERT model and convert them together into a TFLite (yeah I didn’t learn the lesson). This failed in the converting step for reasons that would become clear later. And we switched gears to follow some other guide and first try to convert a standalone tokenizer to TFLite. Tokenizer is a part of the TensorFlow Text library. I followed the official guide: Converting TensorFlow Text operators to TensorFlow Lite with text.FastBertTokenizer. Note when you follow it, do it carefully and closely. I encountered a few problems along the way: When you change the text.WhitespaceTokenizer in guide to our text.FastBertTokenizer, remember to specify a text.FastBertTokenizer(vocab=vocab_lst). We need not the path to the vocab but the actual list e.g. [ \"[PAD]\", \"[unused0]\", \"[unused1]\", ...] describes the vocab where [PAD] maps to token id 0, [unused0] to token id 1, and so on. text.FastBertTokenizer (or the standard version) does not add [CLS] token for you. Google says this is to make sure “you are able to manipulate the tokens and determine how to construct your segments separately” (GitHub issue). How considerate you are, dear Google. I spent one and a half day figuring out how to add these tokens when the model’s input length needs to be fixed, otherwise it triggers TensorFlow’s compute graph to throw “can’t get variable-length input” error. I finally found a solution in Google’s mediapipe’s implementation. Could not translate MLIR to FlatBuffer when running tflite_model = converter.convert(): as mentioned, you must follow the guide very carefully. The guide specifies a TensorFlow Text version. If not this version, the conversion would fail 1pip install -U &quot;tensorflow-text==2.11.*&quot; Encountered unresolved custom op: FastBertNormalize when running converted interpreter / signature: as stated in the Inference section of the guide, tokenizers are custom operations and need to be specified when running inference. (I can’t find doc for InterpreterWithCustomOps anywhere but it does have an argument model_path) 123interp = interpreter.InterpreterWithCustomOps( model_content=tflite_model,# or model_path=TFLITE_FILE_PATH custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS) TensorFlow Text custom ops are not found on Android: the above inference guide writes while the example below shows inference in Python, the steps are similar in other languages with some minor API translations which is a total lie. Android does not support these operations as the custom text op list only mentions python support. At the end, I did manage to 1 merge the above tokenizer and HuggingFace model, 2 export a TFLite model that reads in a text and outputs the last hidden state. However, I seem to have lost that piece of the code. Don’t worry though. Because thanks to Google’s shitty framework, it only works with very few tokenizer implementations anyway. The work-for-all solution is to build your own tokenizer in Java. P.S. While debugging the FlatBuffer error, I came across the TensorFlow authoring tool that can explicitly specify a function’s input output format and detect op unsupported by TFLite. However, the tools is pretty broken for me. Debugging this tool would probably take longer than finding the problem yourself online / ask on a forum. Writing Your Own Tokenizer What’s weird is TensorFlow does have an official BERT on Android example. Reading it again, I found their tokenizer is actually implemented by C++ (see this example). The repo containing the tokenizer code is called tflite-support. Finding this library’s doc, it becomes clear that the text-related operations are currently not supported. TFLite-Support Current use-case coverage Google seems to have used JNI to call the C++ implementation of tokenizer (see code). Therefore, we’d better write our own tokenizer. Luckily Hugging Face also has a Bert on Android example - tflite-android-transformers and writes more accessible code. We directly copied their tokenizer implementation. However, when switching to Chinese vocabulary, the tokenizer goes glitchy. See the following example where we tokenize the sentence「越过长城 ，走向世界」 1234567891011# Our Java tokenizer gives the following tokens, which detokenizes to the following stringtokenizer.decode([101, 6632, 19871, 20327, 14871, 8024, 6624, 14460, 13743, 17575, 102])&#x27;[CLS] 越过长城 ， 走向世界 [SEP]&#x27;# On the other hand, official Hugging Face python BertTokenizer givestokenizer.decode([101, 6632, 6814, 7270, 1814, 8024, 6624, 1403, 686, 4518, 102])&#x27;[CLS] 越 过 长 城 ， 走 向 世 界 [SEP]&#x27;# Inspecting the first difference, our Java tokenizer seems to have used sentencepiece tokenizer.decode([19871])&#x27;##过&#x27; It turns out BERT in its original implementation (code) does not use sentence-piece tokenizer on Chinese characters. Instead, it uses character level tokenizer. Therefore, we need to first insert a whitespace to every character to ensure sentence-piece isn’t applied. Note Hugging Face tokenizer follows BERT original python code very closely so you can easily find where to insert that piece of code. Bert original implementation in Python, with Chinese logic 1234567def tokenize(self, text): &quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot; text = convert_to_unicode(text) text = self._clean_text(text) # Chinese Logic text = self._tokenize_chinese_chars(text) orig_tokens = whitespace_tokenize(text) Hugging Face tokenizer in Java, without Chinese logic 12345public final class BasicTokenizer &#123; public List&lt;String&gt; tokenize(String text) &#123; String cleanedText = cleanText(text); // Insert Here List&lt;String&gt; origTokens = whitespaceTokenize(cleanedText); Building a Classifier The final task is actually to build a classifier of 28 online store commodity classes. As I mentioned in the Detours section, I do not know and don’t wanna bother to know how to define or change a signature. Therefore, I again turn to Hugging Face for its MobileBertForSequenceClassification. The default classification head only has 1 layer, I changed its structure to give it more expressive power. 12345678910model = MobileBertForSequenceClassification.from_pretrained( model_path, num_labels=len(labels), problem_type=&quot;multi_label_classification&quot;, id2label=id2label, label2id=label2id)model.classifier = nn.Sequential(OrderedDict([ (&#x27;fc1&#x27;, nn.Linear(768, 1024)), (&#x27;relu1&#x27;, nn.LeakyReLU()), (&#x27;fc2&#x27;, nn.Linear(1024, num_labels))]))# Fine-tune ...torch.save(model.state_dict(), model_path) However, this throws error when you try to read such a fine-tuned model back in. MobileBertForSequenceClassification is set to have one-layer classification head, so it cannot read in your self-defined classifier’s weights. 12345torch_model = CustomMobileBertForSequenceClassification.from_pretrained( model_path, problem_type=&quot;multi_label_classification&quot;, num_labels=len(labels), id2label=id2label, label2id=label2id)&gt; Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at ./ckpts/ and are newly initialized: [&#x27;classifier.bias&#x27;, &#x27;classifier.weight&#x27;] To solve this, you can Save encoder weight and classifier weight separately, then load them separately Create a custom class corresponding to your weights and initialize an instance of that class instead 2 is clearly the more sensible way. You should read the very clearly written MobileBertForSequenceClassification to understand what exactly needs to be changed. It turns out all we have to do is to extend the original class and change its __init__ part, so it has a 2-layer classification head. 1234567891011121314151617181920212223242526from transformers import MobileBertForSequenceClassification, TFMobileBertForSequenceClassificationclass CustomMobileBertForSequenceClassification(MobileBertForSequenceClassification): def __init__(self, config): super().__init__(config) self.classifier = nn.Sequential(OrderedDict([ (&#x27;fc1&#x27;, nn.Linear(768, 1024)), (&#x27;relu1&#x27;, nn.LeakyReLU()), (&#x27;fc2&#x27;, nn.Linear(1024, 28)) ])) self.post_init()class TFCustomMobileBertForSequenceClassification(TFMobileBertForSequenceClassification): def __init__(self, config, *inputs, **kwargs): super().__init__(config, *inputs, **kwargs) self.classifier = keras.Sequential([ keras.layers.Dense(1024, input_dim=768, name=&#x27;fc1&#x27;), keras.layers.LeakyReLU(alpha=0.01, name = &#x27;relu1&#x27;), # Keras defaults alpha to 0.3 keras.layers.Dense(28, name=&#x27;fc2&#x27;) ])torch_model = CustomMobileBertForSequenceClassification.from_pretrained( model_path, problem_type=&quot;multi_label_classification&quot;, num_labels=len(labels), id2label=id2label, label2id=label2id)tf_model = TFCustomMobileBertForSequenceClassification.from_pretrained( ..., from_pt=True) However, you may find these two models output different values on the same input. A closer look at weights unveil that Hugging Face didn’t convert classifier’s weights from our Torch model to TensorFlow model correctly. We have to set them manually instead. 12tf_model.classifier.get_layer(&quot;fc1&quot;).set_weights([torch_model.classifier.fc1.weight.transpose(1, 0).detach(), torch_model.classifier.fc1.bias.detach()])tf_model.classifier.get_layer(&quot;fc2&quot;).set_weights([torch_model.classifier.fc2.weight.transpose(1, 0).detach(), torch_model.classifier.fc2.bias.detach()]) And now we are finally ready to go. Quantization I followed this official doc: Post-training quantization. Because of time limit, I didn’t try Quantization Aware Training (QAT). 1234567891011vanilla_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)tflite_model = vanilla_converter.convert()quant8_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)quant8_converter.optimizations = [tf.lite.Optimize.DEFAULT]tflite_quant8_model = quant8_converter.convert()quant16_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)quant16_converter.optimizations = [tf.lite.Optimize.DEFAULT]quant16_converter.target_spec.supported_types = [tf.float16]tflite_quant16_model = quant16_converter.convert() Below I report several key metrics for this Chinese-MobileBERT + a 2-layer classification head of [768*1024, 1024*class_num]. This was tested on a Xiaomi 12X with snapdragon 870. The baseline model is my colleague’s BERT-Large implementation with accuracy 88.50% and size 1230MB. My model’s accuracy was bad at first: 75.01% with hyper-parameter weight_decay = 0.01, learning_rate = 1e-4, but we searched out a good hyper-parameter of weight_decay = 2e-4,learning_rate = 2e-5 giving 86.01%. We had 28 classes, 38000 training data in total, and trained for 5 epochs where the validation accuracy roughly flattens. Quantization Logit Difference Accuracy Accuracy (after hyper-param search) Model Size (MB) Inference Time(ms) Power Usage(ma) CPU(%) Memory(MB) float32 (No quant) 0 75.01% 86.094% 101.4 1003.3 89.98 108.02 267.11 float16 0.015% 75.01% 86.073% 51 838 64.15 108.77 377.11 int8 4.251% 63.49% 85.947% 25.9 573.8 60.09 110.83 233.19 If look at the not fine-tuned, vanilla transformer encoder only, the last_hidden_state has a difference: Quantization Logit Difference Model Size (MB) float32 (No quant) 0 97 float16 0.1% 48.1 int8 19.8% 24.9 Small Language Models BERT is the go-to option for classification task. But when it comes to small BERT, we had several options: mobileBERT distilledBERT tinyBERT As the post is about, we used mobileBERT at last because it’s by Google Brain and Google probably knows their thing best. On the other hand, if you’re looking for small generative model, which people mostly call SLM (Small Language Model) as opposed to LLM, I found these options but didn’t try them myself. openELM: Apple, 1.1B Phi-2: Microsoft, 2.7B Post Script If you want to build an app utilizing edge transformer, I would recommend to read the source code of Hugging Face’s toy app. It doesn’t have a README or tutorial, nor have I gone through it personally, but everything from TensorFlow sucks (including MediaPipe unfortunately) When checking back on this tutorial at date 2024/12/28, I found Google released AI Edge Torch, the official tool converting PyTorch models into a .tflite format. So you may probably want to try this first, but again, don’t trust anything from TensorFlow team.","categories":[],"tags":[]},{"title":"Variational Inference","slug":"2024-09-09-Variational-Inference","date":"2024-09-09T04:00:00.000Z","updated":"2025-09-03T01:42:46.391Z","comments":true,"path":"2024-09-09-Variational-Inference/","permalink":"https://yao-lirong.github.io/blog/2024-09-09-Variational-Inference/","excerpt":"","text":"Probabilistic Latent Variable Models The two general forms of probabilistic models are: p(x): a typical probabilistic distribution. In this model, we call x the query. p(y ∣ x): a conditional probabilistic distribution. In this model, we cal x the evidence and y the query. Latent variable models are models that have variables other than the query and the evidence. p(x) = ∑zp(x ∣ z) p(z)​ A classic latent variable model of p(x) is the mixture model, where p(x) is actually a mixture of several different probabilistic model. For example, in the following graph, z is a discrete variable representing which class a datapoint belongs to and is represented by different colors here. p(x ∣ z) is each class’s probability distribution, where in this case can each be modeled by a Gaussian. And p(x)​ when we observe it, is just a bunch of uncolored datapoints and is hard to fit a distribution on it. However, we can see it’s roughly spread in 3 clusters so we introduce the latent variable representing class and we can now well fit a Gaussian mixture model on it (a mixture of 3 Gaussians) Gaussian Mixture Model p(y ∣ x) = ∑zp(y ∣ x, z) p(z) or p(y ∣ x) = ∑zp(y ∣ z) p(z ∣ x): the conditional probability is a bit more free. You can decompose and model it using z​ as you like. An example of latent conditional model is the mixture density network, which we use in RL’s imitation learning to deal with multi-modal situations each requiring a different distribution. Latent Variable Models in General When we use latent variable models, it means we want to decompose a complicated distribution into several simple / easy distributions. By complicated, we mean it’s not possible to write it in a well-defined distribution. By simple / easy, we mean we can write it as a well-defined parametrized distribution, where the parameters can be complex, but the distribution itself is easy to write (a Gaussian of just mean and sigma, or as a Bernoulli with just one variable, etc.) p(x) = ∫p(x ∣ z)p(z)dz p(z) is an “easy” prior we choose. For example a Gaussian, a categorical distribution, etc. p(x ∣ z) should also be an easy distribution, like a Gaussian: $ p(x z) = ({nn}(z), {nn}(z))$ even though the mapping from z to the actual parameters of Gaussian can be complex, where in this case we have to model the mapping through a neural network and this mapping is the learnable part. p(x) is complicated, not possible to write out as any well-defined distribution. Therefore, we decompose it into the two parts above that are easy to parametrize as a probability distribution and learn the parameters inside the distribution. Generative models are not equal to latent variable models. We usually model generative models as latent variable ones because generative models are usually complex probability distributions and we can make it easier by introducing one or more latent variable. How to Train a Latent Variable Model Given dataset 𝒟 = {x1, x2, …, xN}, to fit a typical probabilistic model pθ(x), we use the maximum likelihood estimation: $$ \\theta \\leftarrow \\underset{\\theta}{arg\\!\\max} \\frac 1 N \\sum_i \\log p_\\theta(x_i) $$ In the latent variable model set up, we can substitute the definition in and an MLE would look like $$ \\theta \\leftarrow \\underset{\\theta}{arg\\!\\max} \\frac 1 N \\sum_i \\log \\left( \\int p_\\theta(x_i \\mid z) p(z) dz \\right) $$ pθ(x ∣ z) and p(z) are distributions of our choices, but this integral is still intractable when z is continuous. So now it’s time to do some math tricks. Variational Inference Variational Approximation First, we construct an easy / simple probability distribution qi(z) to approximate p(z|xi) - the posterior distribution specific to datapoint xi. By easy we again mean it is easy to parametrize (a Gaussian, a Bernoulli, etc.) We will show that by introducing this qi(z), we can actually construct a lower bound of log p(xi). What’s good with this lower bound? Later on, we will also prove this bound is sufficiently tight, so as we push up the value of this lower bound, we push up the value of p(xi) which is exactly what we want. $$ \\begin{align} \\log p(x_{i}) &amp;= \\log\\int_{z}p(x_{i}|z)p(z)\\\\ &amp;= \\log\\int_{z}p(x_{i}|z)p(z) \\frac{q_i(z)}{q_i(z)} \\\\ &amp;= \\log \\mathbb E_{z\\sim q_{i}(z)} \\left[\\frac{p(x_{i}|z)p(z)} {q_{i}(z)}\\right] \\\\ &amp;\\geq \\mathbb E_{z\\sim q_{i}(z)} \\left[\\log\\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\\right] &amp;\\text{\\# Jensen's Inequality} \\\\ &amp;= \\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i}|z)+\\log p(z) \\right] - \\mathbb E_{z\\sim q_{i}(z)} \\left[ \\log {q_{i}(z)}\\right]\\\\ &amp;= \\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i}|z)+\\log p(z) \\right] + \\mathcal H_{z\\sim q_{i}(z)} (q_i) = \\mathcal L_i(p, q_i) \\end{align} $$ Recall p(x) is a difficult probability distribution, so we decompose it into two easy distributions p(x|z) and p(z), and use an easy distribution qi(z) to approximate the posterior p(z|xi). Now the good thing is: everything here is tractable: for the first term, we can fix a qi(z) of our choice (recall qi is a distribution we constructed), sample some z, and evaluate the expression. For the second term, we notice it is just the entropy of a distribution and for simple distributions (we constructed qi to be simple), it has a closed form (even if it doesn’t, you can simply sample and evaluate) We call the final lower bound we derived here the variance lower bound or evidence lower bound (ELBO). $$ \\begin{align} \\log p(x_{i}) &amp;\\geq \\mathcal L_i(p, q_i) \\\\ &amp;= \\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i}|z)+\\log p(z) \\right] + \\mathcal H_{z\\sim q_{i}(z)} (q_i) \\end{align} $$ ### Effect of Pushing Up ELBO (Intuitively) Assume our p(⋅)​ is a fixed value, what does pushing up ELBO mean? Here, we give out an intuitive explanation. First, we look at the first term with the two log combined. $$ \\begin{align} &amp;\\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i}|z)+\\log p(z) \\right] \\\\ = &amp;\\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i},z) \\right] \\end{align} $$ To maximize this value, we just have to find a distribution of z, inside which we have the largest value of p(xi, z). Therefore, we want z to distribute mostly under the peak of p(xi, z), Since qi(z) is the distribution we currently have for z, we want qi(z) to sit mostly under the peak of p(xi, z). In the following graph, the y-axis is p(xi, z), the distribution we try to maximize, and the x-axis is our latent variable z. There is also a hidden axis - the probability mass (distribution) of z. We project this hidden axis to the y-axis in this graph. To maximize this first term, we spread z’s mass as much under the peak of p(xi, z) as possible, which makes the green part of this graph. maximize ELBO Now we take the second term entropy into consideration. ℒi(p, qi) = 𝔼z ∼ qi(z)[log p(xi, z)] + ℋz ∼ qi(z)(qi) From our entropy post, we know entropy measures the expected code length of communicating the event described by a random variable. So the more random this variable is, the more code words it’s required to communicate it. Therefore, the more spread out / uniform the distribution is, the higher the entropy. If we’re maxing the entropy, we don’t want the distribution to be skinny. See the following graph. entropy-example When we consider both entropy and the first term, we should achieve this probability distribution depicted in brown. If we don’t have the entropy, z will want to sit under the most likely point, but since we added entropy, z now tries to cover it. In conclusion, (equal sign “=” reads “in effect”) maximize evidence lower bound = cover most of the p(xi|z) distribution = maximize approximation between qi and p(xi|z). maximize ELBO Effect of Pushing Up ELBO (Analytically) Can we measure how good our approximation is? That is, can we measure the distance between p(xi|z) and qi? In fact, we have a nice, analytical way to look at it using KL divergence. For two arbitrary distribution p, q of x, the KL divergence of q from p (the distance from q to p, note KL divergence is not symmetric) is $$ \\begin{align} D_{\\mathrm{KL}}(q|p) &amp;=E_{x\\sim q(x)}\\left[\\log{\\frac{q(x)}{p(x)}}\\right]\\\\ &amp;=E_{x \\sim q(x)}[\\log q(x)]-E_{x \\sim q(x)}[\\log p(x)]\\\\ &amp;=-E_{x \\sim q(x)}[\\log p(x)]-H(q) \\end{align} $$ Doesn’t this look similar to our evidence lower bound? Borrowing that explanation, minimizing KL divergence = cover most of the p(z) distribution = maximize approximation between q and p. KL-divergence Having understood the definition of KL divergence, let’s use it to measure the distance between qi(z) and p(z|xi) - the distribution we want qi to approximate: $$ \\begin{align} D_{KL}(q_{i}(z)\\|p(z \\mid x_{i})) &amp;= E_{z\\sim q_{i}(z)}\\left[\\log{\\frac{q_{i}(z)}{p(z|x_{i})}}\\right]\\\\ &amp;= E_{z\\sim q_{i}(z)}\\left[\\log{\\frac{q_{i}(z)p(x_{i})}{p(x_{i},z)}}\\right]\\\\ &amp;= -E_{z\\sim q_{i}(z)}\\left[\\log p(x_{i}|z)+\\log p(z)\\right] + E_{z\\sim q_{i}(z)}\\log q_i(z) + E_{z\\sim q_{i}(z)}\\log p(x_{i})\\\\ &amp;= -E_{z\\sim q_{i}(z)}\\left[\\log p(x_{i}|z)+\\log p(z)\\right] + \\mathcal H(q_i) + E_{z\\sim q_{i}(z)}\\log p(x_{i})\\\\ &amp;= -\\mathcal L(p, q_i) + \\log p(x_i)\\\\ \\log p(x_i) &amp;= \\mathcal L(p, q_i) + D_{KL}(q_{i}(x_{i})\\|p(z \\mid x_{i})) \\end{align} $$ Therefore, having a good approximation of qi to p(xi|z) = driving KL divergence, which is always a non-negative number, to 0 = the evidence lower bound is a tight bound or even equal to log p(xi)​ - the ultimate thing we want to optimize. Looking at our optimization objective ℒ here: ℒ(p, qi) = log p(xi) − DKL(qi(xi)∥p(z ∣ xi)) When we optimize w.r.t. q: note the first term log p(xi) is independent of q, so its value stays the same. We are in effect optimizing against the KL divergence only, making the distance between our approximation qi and p(z|xi) smaller. The best / extreme case is we have DKL = 0, so ℒ = log p(xi). When we optimize w.r.t. p: Recall our ultimate goal is to make log p(xi) bigger, so we make a better model in theory. Only in theory because we don’t know whether the bound is tight or not. The Learning Algorithm? Therefore, when we optimize ℒ(p, qi)​ w.r.t. q​, we make the bound tighter (make ℒ​ a better approximation of log p(xi)​ ); when we optimize ℒ(p, qi)​ w.r.t. p​, we make a better model in general. By alternating these two steps, we have the actual learning algorithm. Let’s review: which parts are learnable in these two distributions? In our latent variable model setup, we decompose the complicated distribution p(x) into two easy distributions p(x|z) and p(z), where the mapping from z to actual parameters of this p(x|z) distribution needs to be modeled by a complex network. Therefore, the only distribution in the p part with learnable parameters is p(x|z). We denote it with pθ(x|z). In our ELBO setup, we also introduced a simple qi(z) for each datapoint xi to approximate the posterior p(z|xi). To optimize w.r.t. q, we optimize the parameters of each distribution. If qi(z) = 𝒩(μi, σi), we optimize each μi, σi. (we can optimize the entropy value for sure, but I’m not entirely sure how you would take gradient of the expectation term Ez ∼ qi(z)[log p(z)]) Therefore, we have our learning algorithm: $$ \\begin{align} &amp;\\text{for each $x_i$ in $\\{x_1, \\dots, x_N\\}$: }\\\\ &amp;\\hspace{4mm} \\text{sample $z \\sim q_i(z)$}\\\\ &amp;\\hspace{4mm} \\text{optimize against $p$:}\\\\ &amp;\\hspace{4mm} \\hspace{4mm} \\nabla_\\theta \\mathcal L (p_\\theta, q_i) = \\nabla_\\theta \\log p_\\theta(x_i|z) \\\\ &amp;\\hspace{4mm} \\hspace{4mm} \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\mathcal L (p, q_i) \\\\ &amp;\\hspace{4mm} \\text{optimize against $q$:}\\\\ &amp;\\hspace{4mm} \\hspace{4mm} \\nabla_{\\mu_i, \\sigma_i} \\mathcal L (p_\\theta, q_i) = \\nabla_{\\mu_i, \\sigma_i} \\left[\\mathbb E_{z\\sim q_{i}(z)} \\left[\\log p(x_{i}|z)+\\log p(z) \\right] + \\mathcal H_{z\\sim q_{i}(z)} (q_i) \\right] \\\\ &amp;\\hspace{4mm} \\hspace{4mm} (\\mu_i, \\sigma_i) \\leftarrow (\\mu_i, \\sigma_i) + \\alpha \\nabla_{\\mu_i, \\sigma_i} \\mathcal L (p, q_i) \\\\ \\end{align} $$ There’s a problem with optimizing qi though. Note we have a separate q for each data point i, which means if we have N data points, we will have to store N × (|μi| + |σi|) parameters assuming we chose qi to be Gaussian. In machine learning, the number of data points N is usually in millions, making this model unwieldily big. It’s true in inference time we do not use q at all (we’ll see why this is true in the last chapter about VAE), but in training time, we still need them so it’s necessary to keep all these parameters. Therefore, instead of having a separate qi(⋅) to approximate each data point’s P(⋅|xi) specifically, we use a learnable model qϕ(⋅|xi) to approximate p(⋅|xi) This learnable network will take in a datapoint xi, predicts the corresponding μi, σi. We can then sample z​ from this predicted network. Amortized By adapting q to be a learnable network qϕ​ instead, model size does not depend on the number of datapoints anymore. Therefore, it is amortized. The variational lower bound becomes: ℒ(pθ(xi|z), qϕ(z|xi)) = 𝔼z ∼ qϕ(z|xi)[log pθ(xi|z) + log p(z)] + ℋ(qϕ(z|xi)) The learning algorithm naturally becomes: $$ $$ Gradient Over Expectation (Policy Gradient) The question now boils down to how do we calculate this gradient ∇ϕℒ(pθ, qϕ). The second term entropy is easy. We purposefully chose q to be a simple distribution, so there is usually a close form of its entropy and we just have to look it up. The meat is in the first part. How do we take gradient w.r.t. parameter ϕ in the expectation term’s distribution qϕ ? Note the term inside expectation is independent of ϕ, so we can rewrite it as R(xi, z) = log pθ(xi|z) + log p(z) and call the whole thing J. J(ϕ) = ∇ϕ𝔼z ∼ qϕ(z|xi)[R(xi, z)] We chose these namings purposefully because we encountered something similar back in the policy gradient part of reinforcement learning LINK???. Say we have a trajectory τ, sampled from the state transition function with learnable policy πθ, the final expected value we can get from starting state s0 can be written as the following, where R(τ) is a reward function returning the reward of this trajectory. J(θ) = Vπθ(s0) = 𝔼τ ∼ Ps0πθ[R(τ)] We can take the gradient of this value function V w.r.t our policy πθ, so this is called policy gradient. If you’re unfamiliar with RL setup, you just have to know we can derive the following gradient and we can approximate it by sampling M trajectories. $$ $$ Pugging in our $q$ and $\\phi$, $$ $$ Reparametrization Trick We have our full learning algorithm and it’s ready to go now. However, there is a tiny improvement we can do. We defined our qϕ to be a normal distribution 𝒩(μϕ, σϕ) Observe that all normal distributions can be written as a function of the unit normal distribution. Therefore, a sample z is in effect: z ∼ 𝒩(μϕ, σϕ) ⇔ z = μϕ + ϵσϕ, ϵ ∼ 𝒩(0, 1) Let’s rewrite our expectation term to now sample an ϵ from the unit normal distribution instead. By decomposing z into these two parts, we separate the stochastic part and changed z from a sample of some stochastic distribution into a deterministic function z(ϕ, ϵ) parametrized by ϕ and random variable ϵ that is independent of ϕ. ϵ takes the stochastic part alone. Our learnable parameter ϕ now only parametrizes deterministic quantity. ∇ϕJ(ϕ) = ∇ϕ𝔼ϵ ∼ 𝒩(0, 1)[R(xi, μϕ + ϵσϕ)] Aside from these theoretical benefits, mathematically, we do not have to take gradient w.r.t an expectation of parametrized distribution anymore. Instead, the gradient can go straight into the expectation term now like how we usually interchange gradient and expectation (think about discrete case, expectation is just a big sum so we can do it). ∇ϕJ(ϕ) = 𝔼ϵ ∼ 𝒩(0, 1)[∇ϕR(xi, μϕ + ϵσϕ)] Further, to approximate this expectation, we just sample some ϵ from this normal distribution. $$ \\nabla_\\phi J(\\phi) \\approx \\frac 1 M \\sum_j^M \\nabla_\\phi R(x_i, \\mu_\\phi + \\epsilon_j \\sigma_\\phi) $$ With reparametrization, we achieve a lower variance than policy gradient because we are using the derivative of R. (Unfortunately the lecturer didn’t provide a quantitative analysis on this and I don’t know how to prove it) On the other hand, previously, we only took derivative w.r.t. the probability distribution. Why didn’t we use derivative of R back in RL with policy gradient? It’s not we don’t want to but we can’t: we can’t use reparametrization in RL because in RL we usually cannot take derivative w.r.t. reward R. Method Formula Approximation Benefit Deficit Policy Gradient ∇ϕ𝔼z ∼ qϕ(z ∣ xi)[R(xi, z)] $\\frac 1 M \\sum_j^M \\nabla_\\phi[\\log q_\\phi(z_j \\mid x_i)] R(x_i,z_j)$ works with both discrete and continuous latent variable z High variance, requires multiple samples &amp; small learning rates Reparametrization 𝔼ϵ ∼ 𝒩(0, 1)[∇ϕR(xi, μϕ + ϵσϕ)] $\\frac 1 M \\sum_j^M \\nabla_\\phi R(x_i, \\mu_\\phi + \\epsilon_j \\sigma_\\phi)$ low variance, simple to implement (we’ll see soon) only works with continuous variable z and have to model it with a Gaussian In fact, you can forget about the policy gradient method and simply take it for granted that you cannot back propagate a sampled value ∇ϕ𝔼z ∼ qϕ(z|xi), so you have to find some way to make our z​ deterministic, which is what we’re doing here with our reparametrization trick. reparametrization-trick Left is without the “reparameterization trick”, and right is with it. Red shows sampling operations that are non-differentiable. Blue shows loss layers. We forward the network by going up and back propagate it by going down. The forward behavior of these networks is identical, but back propagation can be applied only to the right network. Figure copied from Carl Doersch: Tutorial on Variational Autoencoders Looking at ℒ Directly $$ \\begin{align} \\mathcal L_i = \\mathcal L \\left( p_\\theta(x_i | z), q_\\phi(z | x_i) \\right) &amp;= \\mathbb E_{z\\sim q_\\phi(z | x_i)} \\left[\\log p_\\theta(x_{i}|z)+\\log p(z) \\right] + \\mathcal H (q_\\phi(z|x_i))\\\\ &amp;= \\mathbb E_{z\\sim q_\\phi(z | x_i)} \\left[\\log p_\\theta(x_{i}|z) \\right] + \\mathbb E_{z\\sim q_\\phi(z | x_i)} \\left[\\log p(z) \\right] + \\mathcal H (q_\\phi(z|x_i))\\\\ &amp;= \\mathbb E_{z\\sim q_\\phi(z | x_i)} \\left[\\log p_\\theta(x_{i}|z)\\right] - D_{KL}(q_\\phi(z | x_i)\\|p(z)) \\\\ &amp;= \\mathbb E_{\\epsilon \\sim \\mathcal N(0,1)} \\left[\\log p_\\theta(x_{i}| \\mu_\\phi + \\epsilon \\sigma_\\phi)\\right] - D_{KL}(q_\\phi(z | x_i)\\|p(z)) \\\\ &amp;\\approx \\frac 1 M \\sum_j^M \\log p_\\theta(x_{i}| \\mu_\\phi + \\epsilon_j \\sigma_\\phi) - D_{KL}(q_\\phi(z | x_i)\\|p(z)) \\\\ \\end{align} $$ For the first term, we can just evaluate it. For the second KL term, since we chose both distributions to be easy (in this case Gaussian), there often is a nice analytical form for it. Therefore, we can go ahead to maximize the variational lower bound ℒ​. We can also draw out the following computational graph for the log term and conclude we can back propagate this graph without any problem. On the other hand, if we didn’t do the reparametrization trick, we will get stuck at z: you cannot back propagate z - a sampled value instead of a variable. And we will have to seek help from policy gradient. With reparametrization, we decompose z into two variables μϕ, σϕ we can back propagate through and one stochastic value ϵ we do not care about. computational-graph Variational Autoencoder Setup and Interpretation What we have gone though constitutes the full pipeline of a variational autoencoder. In a variation autoencoder, we have observed variable x and latent variable z encoder qϕ(z|x) = 𝒩(μϕ(x), σϕ(x)) decoder pθ(x|z) = 𝒩(μθ(z), σθ(z)) In training, given an observed sample xi, we encode it to latent variable zi using qϕ, then tries to decode it back with decoder pθ. We maximize the variational lower bound during the process. For all N samples, the training objective looks like: (where the ϵ is a sampled value) $$ \\max_{\\phi,\\theta} \\frac 1 N \\sum_i^N \\log p_\\theta\\left(x_{i}| \\mu_\\phi(x_i) + \\epsilon \\sigma_\\phi(x_i)\\right) - D_{KL}(q_\\phi(z | x_i)\\|p(z)) \\\\ $$ In inference (generation), we sample a z from our prior p(z), then decode it using pθ: z ∼ p(z), x ∼ pθ(x|z) Why does the variational autoencoder work? We talked about many benefits of maximizing this variational lower bound in previous chapter. Let’s look at it again in this decoder-encoder setup,. ℒi = 𝔼z ∼ qϕ(z|xi)[log pθ(xi|z)] − DKL(qϕ(z|xi)∥p(z)) The first log pθ term maximizes the probability of our observed image x given a sample z, so the model makes decoder pθ to reconstruct image x​ as accurate as possible. The second KL term restricts the encoding of an image to be close to the actual prior, which makes sure at inference / generate time, we can directly sample from the prior. Comparison with Auto-Encoder vae-and-ae The VAE’s decoder is trained to convert random points in the embedding space (generated by perturbing the input encodings) to sensible outputs. By contrast, the decoder for the deterministic autoencoder only ever gets as inputs the exact encodings of the training set, so it does not know what to do with random inputs that are outside what it was trained on. So a standard autoencoder cannot create new samples. The reason the VAE is better at sample is that it embeds images into Gaussians in latent space, whereas the AE embeds images into points, which are like delta functions. The advantage of using a latent distribution is that it encourages local smoothness, since a given image may map to multiple nearby places, depending on the stochastic sampling. By contrast, in an AE, the latent space is typically not smooth, so images from different classes often end up next to each other. Figure copied from Probabilistic Machine Learning: An Introduction - Figure 20.26 We can leverage the smoothness of the latent space to perform image interpolation in latent space. Reference Most content of this blog post comes from Berkeley CS 285 (Sergey Levine): Lecture 18, Variational Inference, which I think organized his lecture based on An Introduction to Variational Autoencoders (2.1-2.7, and 2.9.1), or more in-depth on the author’s PhD thesis Variational Inference and Deep Learning: A New Synthesis I found this wonderful tutorial in Probabilistic Machine Learning: Advanced Topics Some graph come from Probabilistic Machine Learning: An Introduction itself and Carl Doersch: Tutorial on Variational Autoencoders, which is referenced in the previous book. Note though the Probabilistic Machine Learning book itself is a horrible book with extremely confusing explanations.","categories":[],"tags":[]},{"title":"Hyper-Parameter Tuning with Optuna","slug":"2024-08-23-Hyper-Parameter-Tuning-with-Optuna","date":"2024-08-23T04:00:00.000Z","updated":"2025-09-03T01:42:46.391Z","comments":true,"path":"2024-08-23-Hyper-Parameter-Tuning-with-Optuna/","permalink":"https://yao-lirong.github.io/blog/2024-08-23-Hyper-Parameter-Tuning-with-Optuna/","excerpt":"After self-implementing a grid-search but having a horrible time writing pyplot visualizing the result, I finally decided to find an existing tool to do the HP tuning for me.","text":"After self-implementing a grid-search but having a horrible time writing pyplot visualizing the result, I finally decided to find an existing tool to do the HP tuning for me. There are two popular HP tuning framework RayTune: almost industry standard Optuna: user friendly, requires minimal modification to original code There’s also skorch integrating scikit-learn and pytorch, so you can use sklearn GridSearchCV. For our simple task, we will go with Optuna. Getting Started To get Optuna running, you just need to add 4 lines in your training logic and a few more lines to start its search. In training logic: 12345678910111213141516171819202122232425def train_model(image_datasets, lr, weight_decay, num_epochs, trial : optuna.trial.Trial=None): optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) for epoch in range(num_epochs): model.train() for inputs, labels in dataloaders[&quot;train&quot;]: # Training Logic model.eval() for inputs, labels in dataloaders[&quot;val&quot;]: running_loss += loss.item() * inputs.size(0) # Eval Logic epoch_loss = running_loss / dataset_sizes[&quot;val&quot;] if epoch_acc &gt; best_acc or (epoch_acc == best_acc and epoch_loss &lt; best_loss): best_acc, best_loss = epoch_acc, epoch_loss &quot;&quot;&quot; OPTUNA CODE GOES HERE: For each epoch, you should report value of a user-defined factor. Optuna uses this factor alone to determine whether to prune out this trial at current epoch step. Your objective value returned has nothing to do with pruning. Read for more at: https://optuna.readthedocs.io/en/v3.6.1/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report &quot;&quot;&quot; if trial is not None: trial.report(epoch_loss, epoch) if trial.should_prune(): raise optuna.exceptions.TrialPruned() return best_loss The following code shows how to set the search space and start the search. 12345678910111213141516171819202122232425def optuna_objective(trial : optuna.trial.Trial): &quot;&quot;&quot; Define a custom objective function we want to optimize on. This function returns value of the criteria you want to finally evaluate your model on. i.e. how you compare different models. The best model should have the best value of this objective. If you say the best model should have highest training accuracy at the last epoch, then return training accuracy at the last epoch here. In our example, we think the best model should have the best `best_loss`, where a model&#x27;s `best_loss` is this model&#x27;s lowest validation loss across all epochs. &quot;&quot;&quot; image_datasets = prepare_data() lr = trial.suggest_float(&quot;lr&quot;, 1e-6, 1e-1, log=True) weight_decay = trial.suggest_float(&quot;weight_decay&quot;, 1e-6, 1e-1, log=True) loss = train_model(image_datasets, lr, weight_decay, 15, trial) return lossif __name__ == &quot;__main__&quot;: &quot;&quot;&quot; Create a study called `plant_144` where we minimize the objective passed in. Start the search. The search ends when we finish 10 trials or spend 3 hours. &quot;&quot;&quot; study = optuna.create_study( direction=&quot;minimize&quot;, study_name=&quot;plant_144&quot;) study.optimize(optuna_objective, n_trials=10, timeout=3*60*60) print(&quot; Objective Value: &quot;, study.best_trial.value) print(&quot; Params: &quot;) for key, value in study.best_trial.params.items(): print(f&quot; &#123;key&#125;: &#123;value&#125;&quot;) The above example was adapted from Optuna’s PyTorch starting example. For more reporting printout statements, check the original example. Saving Study and Board Visualization In addition to printing out all the info to the console and losing them from memory after this python script finishes, we can save them in the form of an RDB (Relational Database, or just database as most databases are RDB). To do this, we pass a database URL to the storage argument 1234study = optuna.create_study( direction=&quot;minimize&quot;, study_name=&quot;plant_144&quot;, storage=&quot;sqlite:///db.sqlite3&quot;) You can now Ctrl+C stop this search at anytime and resume it by running the same code again. Database exposes itself as a server in machines. Therefore, to access it (even in local machine), we use Database URL. Just like to access a webpage online, we use an HTTPS url. In our example here, the history will be stored in a file called db.sqlite3 under current directory. This file is a general database and can store study other than the one called plant_144. You can store another study inside it. 1234study = optuna.create_study( direction=&quot;maximize&quot;, study_name=&quot;plant_8&quot;, storage=&quot;sqlite:///db.sqlite3&quot;) For me this code just worked without having to install SQLite DB. This is probably because it comes with my Ubuntu but I have no idea. Check official tutorial Saving/Resuming Study for more on saving and loading. You can now visualize the search history, each parameter’s importance, etc. with optuna-dashboard 1optuna-dashboard sqlite:///db.sqlite3 optuna-dashboard Multi-GPU Parallelism Support roman’s Stack Overflow answer provides a very simple way to do multi-GPU tuning by utilizing Optuna’s resume feature. To do so, create a study by following the previous code. Then modify your code now to resume instead of starting a new study. 123if __name__ == &#x27;__main__&#x27;: study = optuna.load_study(study_name=&#x27;plant_144&#x27;, storage=&#x27;sqlite:///db.sqlite3&#x27;) study.optimize(objective, n_trials=100) and simply start “resume” this study on different available GPUs 12CUDA_VISIBLE_DEVICES=3 nohup python optuna.py &gt; log3.txt 2&gt;&amp;1 &amp;CUDA_VISIBLE_DEVICES=5 nohup python optuna.py &gt; log5.txt 2&gt;&amp;1 &amp; The history from both processes will be stored under the study called plant_144 in file db.sqlite3. For more information on parallelizing on multiple gpu, check official guide: Easy Parallelization Some Complaints In its visualization, Optuna doesn’t provide an option to filter out the “bad” trial runs, making the scale of all graph ridiculous and usually of no information.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"KV Cache","slug":"2024-07-02-KV-Cache","date":"2024-07-02T04:00:00.000Z","updated":"2025-09-02T23:51:12.123Z","comments":true,"path":"2024-07-02-KV-Cache/","permalink":"https://yao-lirong.github.io/blog/2024-07-02-KV-Cache/","excerpt":"Before this, see 2024/06/17 Conducting Multi-Round Conversation with Transformers for why we need cache. But we have query, key, value three matrices. Why do you only cache past keys and values? How about past queries?","text":"Before this, see 2024/06/17 Conducting Multi-Round Conversation with Transformers for why we need cache. But we have query, key, value three matrices. Why do you only cache past keys and values? How about past queries? Attention Mechanism in Detail Recall the attention process in transformer can be written in the following matrix form: $$ Z = \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$ If we look a particular output at position i, it can be written as: $$ z_i =( {} ) \\begin&#123;bmatrix&#125; v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end&#123;bmatrix&#125; $$ A simple example can be found in the famous Illustrated Transformer self attention output From the formula and the example, we can see that key and values are always a pair in calculation. In fact, this is aligned with the very concept of soft dictionary behind attention: we get a query from somewhere and look at all the keys in the dictionaries to find, for each key, how much it relates to this query and output the weighted average of each key’s value based on the relatedness. Generative Transformer (Decoder Based) Autoregressive Decoder Let’s consider a causal language model, aka a transformer’s autoregressive generative decoder. At inference time, we only care about the output at the last position because the model is autoregressive and the outputs at all the previous positions are exactly the same as our input. (See the above graph from blogpost Transformers-based Encoder-Decoder Models) Therefore, if the current sequence has length s, we only care about zs. All the other outputs z1…s − 1 are useless. Inference code in Karpathy’s nanoGPT corroborated this in its inference time implementation: 1234if targets is None: # inference-time mini-optimization: only forward the lm_head on the very last position logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim loss = None Now revisit the formula to calculate the output zs: $$ z_s =( {} ) \\begin&#123;bmatrix&#125; v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_s \\end&#123;bmatrix&#125; $$ It should be clear that to save computation, we only need to cache the kv values in the previous positions and it’s useless to cache the q value. To give a more detailed example, let’s consider the whole process to generate a sequence of tokens with length n: t1, …, tn. We can see the previous queries are never used in the computation. $$ $$ Time Complexity Boost People complain about the slow inference time of generative transformer model, where it has a quadratic sequence length term O(s2). This quadratic term is caused by QKT matrix multiplication in attention where both matrices have shape s × d. Recall running time of matmul AB where $A \\in \\R^{m \\times p}, B \\in \\R^{p \\times n}$ is O(mpn), so this matmul of query and key matrix has time complexity O(s2d). However, by observing that we only need the output at the very last position in generative model and utilizing KV-cache, we reduce our matrix $Q \\in \\R^{s \\times d}$ to a single vector of $q \\in \\R^{1 \\times d}$ and effectively reduce the time complexity of this operation to O(sd). Therefore, we can eliminate the quadratic term from our inference time and only need linear time s instead. What about Encoder Based Transformer Model? Encoder Based transformer models do not have the issue of repeatedly computing the same past tokens’ attention scores so do not need a KV-cache. Code Implementation Facebook’s cross-lingual language model (XLM) gives a fantastic example of how to implement KV-Cache (or transformers in general, it provides abundant comments of tensor shape at each step). At inference time, do not recompute elements (where slen or a more descriptive naming can be cached_sequence_length is how many previous positions have been cached): link 12345678if cache is not None: _slen = slen - cache[&#x27;slen&#x27;] x = x[:, -_slen:] positions = positions[:, -_slen:] if langs is not None: langs = langs[:, -_slen:] mask = mask[:, -_slen:] attn_mask = attn_mask[:, -_slen:] Retrieve, use and update cache: link1 link2 1234567if self.layer_id in cache: k_, v_ = cache[self.layer_id] k = torch.cat([k_, k], dim=2) # (bs, n_heads, klen, dim_per_head) v = torch.cat([v_, v], dim=2) # (bs, n_heads, klen, dim_per_head)cache[self.layer_id] = (k, v)...cache[&#x27;slen&#x27;] += tensor.size(1) XLM can serve multiple purposes including as a generative causal language model, masked language model, or a translation language model. We use KV-Cache only with causal language model in generate() function, see code. XLM has a Memory module that implements Product-Key Memory Layers whose mechanism rings very familiar to me but I can’t recall where I’ve encountered something similar before. Anyway, you can ignore those Memory implementations and focus on the attention part if use it as a source to learn cache or the basics of attention. More Code Examples This Medium post KV caching explained leads way to where to find Hugging Face’s implementation in general, which can be too modular and abstract nowadays. It’s hidden in the forward function in XXXForCausalLM. Take LlamaForCausalLM as an example, in its forward function, we still need to go down the abstraction to LlamaModel -&gt; LlamaDecoderLayer -&gt; LlamaAttention and we can see the past_key_value there implementing the Cache class. I didn’t read into how Hugging Face did it. This Zhihu post explaining KV-Cache leads way to Hugging Face’s GPT-2. The original GPT-2 code is in fact more straightforward, but you’d better just read XLM. It simply has more comments and the naming is more self-explanatory. PS I initially didn’t find where Hugging Face implemented KV-Cache in current version (transformer 4.40) but only this Cache class and failed to find where it’s used. So I followed the recommendation under this Zhihu post to go to transformer 2.5.0 instead. A quick search like “kv” or “cache” led me to modeling_xlm.py. I was surprised to find early Hugging Face model code was more of a rename of original implementation instead of a refactor they do now. I then read this KV caching explained post. Its graph isn’t super straightforward but it introduces how KV-cache reduces time complexity and where to find Hugging Face’s implementation.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Conducting Multi-Round Conversation with Transformers","slug":"2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers","date":"2024-06-17T04:00:00.000Z","updated":"2025-09-02T23:51:12.124Z","comments":true,"path":"2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/","permalink":"https://yao-lirong.github.io/blog/2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/","excerpt":"I was using LLaVA to query in an image how many characters there are. For higher accuracy, I decided to employ Chain of Thought, but struggled to implement it. CoT is conducted through a multiple round conversation. It is easily done in a graphical chat interface but how is it done internally with code?","text":"I was using LLaVA to query in an image how many characters there are. For higher accuracy, I decided to employ Chain of Thought, but struggled to implement it. CoT is conducted through a multiple round conversation. It is easily done in a graphical chat interface but how is it done internally with code? Token Level Before diving into instruct / chat model, let’s go to the lowest level and think how transformers do generation. Transformer is an autoregressive model: it uses its own output as input for the next round. Looking at nanoGPT’s generate function: 12345678910111213def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None): &quot;&quot;&quot; Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete the sequence max_new_tokens times, feeding the predictions back into the model each time. &quot;&quot;&quot; for _ in range(max_new_tokens): idx_cond = idx if idx.size(1) &lt;= self.config.block_size else idx[:, -self.config.block_size:] logits, _ = self(idx_cond) logits = logits[:, -1, :] / temperature probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx, idx_next), dim=1) return idx If we ignore the details, this for loop is effectively doing: 123456789token0 = tokenizer(text)output1 = model(token0)token1 = get_resposne(output1) output2 = model(token0 + token1)token2 = get_resposne(output2)output3 = model(token0 + token1 + token2)... By writing it out like this, it’s clear that each turn of generation, we feed the previous step input into the model as something new, though exactly the same. Therefore, when we call model(token0 + token1), we forgot about all the attention we calculated in model(token0) even though the attention for token0 part is actually completely the same. This is why people complain transformer inference is slow and this is where the inference speed-up techniques like KV-cache comes in. This also reveals that the very popular graph demonstrating the theory behind transformer’s inference lied (at least to me). When calculate yi + 1, we do not re-use y0…yi or the attention or the activations in the middle. We just re-feed them back into the model as something completely new. Autoregressive Decoder Conversation Level Chat model is also just a text continuation model except it follows a chat template distinguishing which texts are inputted by the user and which are generated by the assistant. In the lowest abstraction level - the token level, for each turn, the model outputs one token and uses that as part of the input in next turn’s generation. One abstraction level higher to this conversation level, to do multiple-round conversation, a chat model similarly outputs one response to one user’s input and uses that response as a part of the input for next turn’s generation. Therefore, to conduct conversation with a chat model, we just append the model’s response at each turn to its corresponding input. 123456input1 = tokenizer(text1)output1 = model(input1)# output1 contains input1 and model&#x27;s response 1response1 = get_resposne(output1) input2 = tokenizer(text2)output2 = model(input1 + response1 + input2) And yes, this means to get output2, we feed input1 + response1 both as new to the model, but this shouldn’t be a concern anymore since we feed each token as new anyway. get_response The question now comes to how we should implement get_response to extract the assistant’s response from the text-continuation model’s output. Find the indicator (prefix) of the start of assistant’s message: Note when the model doesn’t follow the instruction and failed to generate such a prefix, this method fails. 123456prefix = &quot;\\[/INST\\]&quot; # escape special characters for regexwith torch.no_grad(): output = model.generate(**inputs, max_new_tokens = 300)detoked_output = processor.decode(output[0], skip_special_tokens=True)answer_idx = [m.end() for m in re.finditer(prefix, detoked_output)][-1]answer = detoked_output[answer_idx:] recommended - Get the substring that is after the input (prompt): Hugging Face uses this approach in their TextGenerationPipeline. There’s a clean_up_tokenization_spaces variable in decode function which defaults to False. (For what it does, see this discussion) Hugging Face set it to True in both call, but I tried set both to False or one to True the other to False, either can give correct results. That said, it’s still best to follow what Hugging Face wrote. After all they know their codes best. 12345678910with torch.no_grad(): output = model.generate(**inputs, max_new_tokens = 300)detoked_output = processor.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)cutoff = len(text_processor.decode( inputs[&quot;input_ids&quot;][0], skip_special_tokens=True, clean_up_tokenization_spaces=True, ))answer = detoked_output[cutoff:] Detours when Taking the Recommended Approach I had some trouble with this recommended approach at first: 12345678chat = [ &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;&lt;image&gt;\\nHow many animated characters are there in this image?&quot;&#125;]prompt = text_processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)inputs = processor(prompt, image, return_tensors=&quot;pt&quot;).to(device)...detoked_output = processor.decode(output[0], skip_special_tokens=True)cutoff = len(prompt) And cutoff is actually many indexes after the real starting point of assistant’s response. That is because when we apply_chat_template, we added some special tokens &lt;s&gt; &lt;\\s&gt; to indicate the start and end of one turn of conversation with the assistant, but when we detokenize the output, we skip_special_tokens to get the response only and caused this discrepancy. I thought at first that this discrepancy comes from LLaVA replaced &lt;image&gt; token with the image embeddings (or pixel_values as Hugging Face calls it) because &lt;image&gt; also disappeared in the detoked_output. However, after reading LLaVA’s paper: Visual Instruction Tuning Figure 1: LLaVA network architecture, I realized LLaVA actually puts the image in front of the text input instead of inserting it in the middle. LLaVA architecture And &lt;image&gt; disappeared because it’s also a special token. However it was not inside the tokenizer.all_special_tokens. Reading the source code of tokenizer, I’m actually not sure how it was added as a special token so was not able to debug why it’s not in all_special_tokens. For this specific behavior, I submitted an issue on Hugging Face forum. You can find chat template definition in tokenizer_config.json -&gt; \"chat_template\". Also in this file, \"added_tokens_decoder\" attribute defines &lt;image&gt; as a special token. The Complete Code I referenced Hugging Face conversation pipeline for the general structure and the response extractor 12345678910111213141516171819202122232425queries = [ &quot;&lt;image&gt;\\nHow many animated characters are there in this image?&quot;, &quot;Answer with a single number in decimal format. Give no explanations.&quot;]def generate_response(image): chat = [] for query in queries: chat.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query&#125;) prompt = text_processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) inputs = processor(prompt, image, return_tensors=&quot;pt&quot;).to(device) with torch.no_grad(): output = model.generate(**inputs, max_new_tokens = 300) output = processor.decode(output[0], skip_special_tokens=True) input_ids = inputs[&quot;input_ids&quot;] cutoff = len(text_processor.decode( input_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True, )) answer = output[cutoff:] chat.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: answer&#125;) return answer PS As written at the start of this blogpost, it all began from me trying to do multi-round conversation with a transformer. A web search took me to these discussions (link 1, link 2). It’s obvious this accepted approach of appending output to previous message causes great waste of computing resources, which made me realize how transform works internally at the lowest level is itself a waste of resources.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"GPT-4o Release","slug":"2024-05-14-GPT-4o-Release","date":"2024-05-14T04:00:00.000Z","updated":"2025-09-02T23:51:12.133Z","comments":true,"path":"2024-05-14-GPT-4o-Release/","permalink":"https://yao-lirong.github.io/blog/2024-05-14-GPT-4o-Release/","excerpt":"One day before Google I/O, OpenAI made a Spring Update Release, introducing multi-modal end-to-end model GPT4-o","text":"One day before Google I/O, OpenAI made a Spring Update Release, introducing multi-modal end-to-end model GPT4-o Capabilities and Features In their release live, we see Real-time responsiveness in audio mode, ability to be interruptted Detect tone and mood in your speech, including how hard you breath Real-time responsiveness in vision mode: no need to take a picture, just hold your phone’s camera there and it can screenshot(?) for you Right after the live, OpenAI updated their blog, showing more demos: Two GPT-4os harmonizing: on the same device same session. They can sing and harmonize. They can follow user’s instruction to sing faster, sing slower, or sing in a higher voice. Lullaby: user can give instruction by speech to tell GPT-4o to go lighter, louder, … Taking faster: user can give instruction by speech to tell GPT-4o to speak faster, slower Failure cases: it sometimes go wild and speak in another language fail in translation tasks fail in teaching intonation in Chinese Technicality GPT-4o is a single new model end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. Previously, ChatGPT Voice Mode is a pipeline of three separate models: audio-text transcription, GPT-3.5/GPT-4 text model, text-to-audio conversion model Designed a new tokenizer with greater compression: Hindi has 2.9x fewer tokens, Russian 1.7x, Korean 1.7x, Chinese 1.4x, Japanese 1.4x, and European languages, including English, has 1.1x fewer tokens. New tokenizer means fully new pre-trained model (brought up in this reddit thread) It is super fast, responding to audio inputs with an average of 320 milliseconds, while original ChatGPT Voice Mode has latencies of 2.8 seconds (GPT-3.5) and 5.4 seconds (GPT-4) on average. At the same time, GPT-4o “achieves GPT-4 Turbo-level performance on text, reasoning, and coding intelligence.” What did they do to speed up inference? Is it Quantization, MoE or something else? (brought up in this reddit thread) What’s the model size? Nothing is reported. Inspecting the New Tokenizer When I used reddit on the day GPT-4o released, this post came to me suggesting Chinese tokens in OpenAI’s new tokenizer are greatly contaminated. The new tokenizer o200k_base is actually twice as large as the last cl100k_base and has already been loaded to GitHub in this commit.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"CLIP","slug":"2024-04-22-CLIP","date":"2024-04-22T04:00:00.000Z","updated":"2025-09-03T01:42:46.384Z","comments":true,"path":"2024-04-22-CLIP/","permalink":"https://yao-lirong.github.io/blog/2024-04-22-CLIP/","excerpt":"CLIP investigates whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain (CV).","text":"CLIP investigates whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain (CV). This line of work represents the current pragmatic middle ground between learning from a limited amount of supervised “gold-labels” and learning from practically unlimited amounts of raw text. 2 Approach 2.1 Advantage of Natural Language Supervision easy to scale: natural language data amount is huge, much easier to obtain than crowd-sourced labeling flexible zero-shot transfer: connects image representation to language; different from unsupervised or self-supervised model that is limited to image domain. 2.2 Constructing Dataset To explore effects of web-scale pre-training, we first build a web-scale dataset. Construct a query list of size 500,000 that contains words occurred &gt;= 100 times in Wikipedia Search for images of these queries, construct a dataset of 400M (image, text) pair Class balance (yeah that’s the word describing “make each class have the same number of samples so it’s fair”) by including 20,000 pairs per query 2.3 What to Predict? What is the Loss? Previous methods with natural language supervision attempt is about predicting a bag of words (BoW) / phrase n-gram representation of labels. The authors explore different approaches. This work is all about large scale pre-training and scaling. Training efficiency is the key to scaling natural language supervision. Authors selected final pre-training method based on efficiency. They compared three approaches: Transformer language model (captioning model): train a transformer to predict the caption of an image. So this is a generative task and uses transformer’s loss function. It learns 3 times slower than the baseline - approach 2. A model predicts BoW encoding of the caption: this was used as a simple baseline and authors found approach 1 couldn’t even beat this baseline. This approach still tries to predict the exact words of the text label, but the order of how words appear no longer matters. This is not much easier due to the wide variety of descriptions, comments, and related text that co-occur with images. A contrastive model predicts which text as a whole is paired with which image: In this way, we decrease the output space to only the number of classes we have. We learn 4 times faster than the baseline - approach 2. Accuracy vs #(images processed) See Figure 2 for a detailed comparison on accuracy vs. #(images fed) of these three models. This illustrates how fast / slow a training method learns. Approach Output Space Answer Space: In ideal scenario, what do we choose from? Transformer Language Model All English sentences (permutation of all English words) 500K queries BoW prediction model Word count bucket of all English sentences (combination of all English words) 500K queries Contrastive pairing model Sentences describing class and labels batch_size pre-selected queries (32768 in CLIP) It’s worth noting that CLIP uses a very large minibatch size of 215 = 32768 2.4 Model Architecture and Scaling Summary of CLIP Image encoder has two architectures: ResNet-50 and ViT 1234567891011121314151617181920# image_encoder - ResNet or Vision Transformer# text_encoder - CBOW or Text Transformer# I[n, h, w, c] - minibatch of aligned images# T[n, l] - minibatch of aligned texts# W_i[d_i, d_e] - learned proj of image to embed# W_t[d_t, d_e] - learned proj of text to embed# t - learned temperature parameter# extract feature representations of each modalityI_f = image_encoder(I) #[n, d_i]T_f = text_encoder(T) #[n, d_t]# joint multimodal embedding [n, d_e]I_e = l2_normalize(np.dot(I_f, W_i), axis=1)T_e = l2_normalize(np.dot(T_f, W_t), axis=1)# scaled pairwise cosine similarities [n, n]logits = np.dot(I_e, T_e.T) * np.exp(t)# symmetric loss functionlabels = np.arange(n)loss_i = cross_entropy_loss(logits, labels, axis=0)loss_t = cross_entropy_loss(logits, labels, axis=1)loss = (loss_i + loss_t)/2 Note: d_e represents multi-modal embedding space. the temperature parameter τ is directly optimized as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter. implementation in original release The authors train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights. This section also describes how to scale the text encoder and how to scale both kinds of image encoder. 3 Experiments Authors conducted experiments on 36 different datasets. 3.1 Zero-Shot Transfer Authors wanted to experiment on zero-shot transfer ability because of the ability demonstrated in language models. The following is the most exciting sentence to me in this paper. I think it explains a lot of large-scale design choices by OpenAI team. Did this paper inspire Ilya to go all the way down the path of scaling? Our focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learning in the field of NLP. To our knowledge Liu et al. (2018) first identified task learning as an “unexpected side-effect” when a language model trained to generate Wikipedia articles learned to reliably transliterate names between languages. Authors explain in detail how we do zero-shot classification and give an interpretation to the pipeline. I wrote the previous “output space” and “answer space” thing based on this interpretation. The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter τ , and normalized into a probability distribution via a softmax. Note that this prediction layer is a multinomial logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent. Continuing with this interpretation, every step of CLIP pre-training can be viewed as optimizing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32,768 total classes defined via natural language descriptions. prompt engineering and ensembling Text in our training data is usually a sentence, but text in test data is just a one word label. To bridge this gap, we use some prompt template. default: A photo of a &#123;label&#125; on several fine-grained image classification datasets, it’s helpful to specify the category: A photo of a &#123;label&#125;, a type of pet or a satellite photo of a &#123;label&#125; ensembling several different prompts improve performance: use different context prompts such as A photo of a big &#123;label&#125; and A photo of a small &#123;label&#125;. Authors construct the ensemble over the embedding space instead of probability space. In this way, they cache a single set of averaged text embedding so compute cost doesn’t increase in amortized time. scaling law Zero-shot CLIP scales wrt model compute Scaling law is the law that empirically shows that performance is predictable as a function of important quantities such as training compute and dataset size. On 36 different datasets, ResNet CLIP’s average zero-shot error is well modeled by a log-log linear scaling trend. However, performance on individual evaluations is much more varied despite the smooth overall trend. Authors did not report ViT CLIP scaling results. 3.2 Representation Learning To use CLIP as a representation of the image, there are two common approaches: Fitting a linear classifier on a representation extracted from the model End-to-end fine-tuning of the model. Fine-tuning increases flexibility, and prior work has convincingly demonstrated that fine-tuning outperforms linear classification on most image classification datasets. However, OpenAI chooses to use linear classifier to measure CLIP performance for the following reasons: the more official reason: we chose it because it’s weak and therefore better shows how dataset-agnostic CLIP is Our work is focused on developing a high-performing task and dataset-agnostic pre-training approach. Fine-tuning, because it adapts representations to each dataset during the fine-tuning phase, can compensate for and potentially mask failures to learn general and robust representations during the pre-training phase. Linear classifiers, because of their limited flexibility, instead highlight these failures and provide clear feedback during development the more practical reason: Fine-tuning opens up a much larger design and hyper-parameter space, which makes it difficult to fairly evaluate and computationally expensive. By comparison, linear classifiers require minimal hyper-parameter tuning and have standardized implementations and evaluation procedures. bonus reason: Linear classifier has the added benefit of being very similar to the approach used for its zero-shot classifiers which enables extensive comparisons and analysis approach: Appendix A.3 provides a full guideline of training such a linear classifier, including details on hyper-parameter search, solver method, and train-valid-test split. Notably, the input to the Logistic Regression is the image embedding (output of the image encoder I_f), not the multi-modal embedding (image embedding that went through the multi-modal linear projection) results: when comparing to other models of similar compute requirement, small CLIP have wins and loses. However, CLIP scales very well and the largest model achieves both SOTA score and compute efficiency. ViT vs ResNet: The authors found CLIP ViT is about 3x more compute efficient than CLIP ResNet. This is aligned with ViT paper’s finding Out-of-Domain Performance and Natural Distribution Shift: Researchers often find models exceeding human on ImageNet test set can still make simple mistakes on other test data and score much lower than human. A common explanation is these models are adept at finding patterns within dataset, so improve in-distribution performance. However many of these patterns are spurious and do not hold for other distributions and result in large drops in performance on other datasets. Most of the studies that reach the above explanation limited their evaluation model to those trained on ImageNet. Therefore, the authors want to know to what degree are these failures attributable to deep learning, ImageNet, or some combination of the two? They explore this by evaluating ImageNet models on natural distribution shifted dataset. Natural distribution shift means testing trained models on data that is different in e.g. image style, image blurriness, geographic location, and camera operation (Hendrycks et al. The many faces of robustness). “Natural” is used to make a distinction from synthetic distribution shift made through style-transferred or adversarially generated. Authors found CLIP perform much better on these natural distribution shifted dataset. However, this doesn’t necessarily mean supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also produce robust models. Therefore, OpenAI measured how the performance of CLIP models change after adapting to the ImageNet distribution via an L2 regularized logistic regression classifier fit to CLIP features on the ImageNet training set. This improved accuracy on ImageNet by 9.2% to 85.4%, but average accuracy under distribution shift slightly decreases. To me this doesn’t say much. If you fine-tune (or fit a linear classifier) to a specific dataset, of course you’d expect its behavior to be bad on some other dataset. But on the contrary, these natural-distribution-shifted dataset is not that different from ImageNet. Yes, there are some animations / sketches, but most are just some more pictures of that class. And CLIP with an ImageNet linear head cannot get them right. I guess what the authors want to say is that ImageNet is not just A arbitrary dataset, but has almost become a machine learning benchmark dataset. It is supposed to be general because all models train on it and these models will be deployed to all sorts of scenario. The authors didn’t go far to attack the generality of ImageNet or even draw any conclusion on why fitting an ImageNet classification head hurts natural distribution shift performance. The authors just prompt to caution that though prior work has also pre-trained models on distributions other than ImageNet, it is common to study and release models only after they have been fine-tuned to ImageNet. And it would be wise to also study the models pre-trained on distributions other than ImageNet. Results: Taken together, these results suggest that the recent shift towards large-scale task and dataset agnostic pre-training combined with a reorientation towards zero-shot and few-shot benchmarking on broad evaluation suites promotes the development of more robust systems and provides a more accurate assessment of performance. 5 Data Overlap Analysis A concern with pre-training on a very large internet dataset is unintentional overlap with downstream evals. One option to prevent this is to identify and remove all duplicates before training a model. While this guarantees reporting true hold-out performance, it requires knowing all possible data which a model might be evaluated on ahead of time. This has the downside of limiting the scope of benchmarking and analysis. Therefore, OpenAI instead built a duplicate detector, document how much overlap occurs, and run experiments on dataset with and without these overlaps to measure how performance changes due to these overlaps. So instead of simply removing them, they record performance of before and after removing them. They found that there is a median overlap of 2.2% and an average overlap of 3.2%. Due to this small amount of overlap, overall accuracy is rarely shifted by more than 0.1% with only 7 datasets above this threshold. It would be useful if OpenAI also releases their duplicate detector model. Appendix C discusses it in more details but it doesn’t seem like OpenAI ever released it. 6 Limitations Performance: CLIP cannot beat dataset-specific trained &amp; designed models: CLIP zero-shot performs better than a pre-trained ResNet-50 feature + a linear classifier, but on most datasets, CLIP is well below the SOTA for that specific dataset. zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it: CLIP simply has a super large domain, not really a general model. For example, MNIST digits are not at all in its web-scraped huge dataset, so CLIP does surprisingly bad on this super simple dataset. CLIP is limited to “choosing”: CLIP cannot just take in a picture and spit out its class. You need to give CLIP a range to choose from. CLIP is based on “choosing”, not “generating” (image captioning model) Training Methodology: In training time, CLIP repeatedly queried performance on full validation sets to guide optimization. These validation sets often have thousands of examples, which is unrealistic for true zero-shot scenarios. On the contrary, LLM in training time doesn’t do this (?) Training dataset comes from Internet. Its image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases. Supervision with Natural Language: Many complex tasks and visual concepts can be difficult to specify just through text. Actual training examples are undeniably useful but CLIP does not optimize for few-shot performance directly. In our work, we fall back to fitting linear classifiers on top of CLIP’s features. This results in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. 7 Broader Impacts In this section, the authors mainly introduces the bias exists in CLIP and what kind of surveillance it can be used for. Nothing too interesting, but they discussed how tweaking the category system can improve model’s performance. This reminds me of what I did in Xiaomi’s oversea app store tagging project, where I added new category and modified existing category’s definition to improve the cos-similarity based zero-shot classification model performance. Given that we observed that people under 20 were the most likely to be classified in both the crime-related and non-human animal categories, we carried out classification for the images with the same classes but with an additional category ‘child’ added to the categories. We found that this drastically reduced the number of images of people under 20 classified in either crime-related categories or non-human animal categories (Table 7). This points to how class design has the potential to be a key factor determining both the model performance and the unwanted biases or behavior the model may exhibit The authors then go on to conclude that Decisions about things like class design are a key determiner not only of model performance, but also of how and in what contexts model biases manifest Takeaways Data is still the king in ML. It is possible to transfer the success of task-agnostic web-scale pre-training in NLP to CV. The key to scaling &amp; training efficiency is how compact your output space is (word permutation - &gt; word combination -&gt; batch_size) We can use prompt ensembling to improve CLIP’s performance. To use CLIP as the feature extractor and put a linear classifier on top of it, we use the image embedding (image encoder’s output), not he multi-modal embedding (image embedding went through the multi-modal linear projection); On the other hand, for zero-shot classification, you use multi-modal embedding, the same as the training process except now you only have one image and calculate the cos similarity with all class names. Decisions about things like class design are a key determiner not only of model performance, but also of how and in what contexts model biases manifest","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Gradient Scaling","slug":"2024-04-08-Gradient-Scaling","date":"2024-04-08T04:00:00.000Z","updated":"2025-09-02T23:51:12.133Z","comments":true,"path":"2024-04-08-Gradient-Scaling/","permalink":"https://yao-lirong.github.io/blog/2024-04-08-Gradient-Scaling/","excerpt":"Loss Scaling / Gradient Scaling was mentioned in Mixed-Precision Training as one of the 3 techniques, but there are many points to be careful with when in practice.","text":"Loss Scaling / Gradient Scaling was mentioned in Mixed-Precision Training as one of the 3 techniques, but there are many points to be careful with when in practice. Overview: Typical Use Case Here’s an overview of how to use amp.GradScaler adapted from PyTorch official doc. Background If the forward pass for a particular op has float16 inputs, under Automatic Mixed Precision package - torch.amp, the backward pass for that op will produce gradients of the same data type - float16 . Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost. Code scaler.scale(loss).backward(): To prevent underflow, “gradient scaling’ multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). In this way, the gradients on all parameters are scaled by this same factor and we don’t have to worry about them flush to zero. scaler.scale(loss) multiplies a given loss by scaler’s current scale factor. We then call backward on this scaled loss. scaler.step(optimizer): After back-propagation, all learnable parameters get their gradients, which are scaled to prevent underflow. Before applying whatever learning algorithm (Adam, SGD, …) on them, we have to unscale them so the amount to be updated is correct. scaler.step(optimizer) 1. unscales gradients, 2. calls optimizer.step(), and does the previous two points safely: Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs to prevent overflow/underflow (For why overflow can happen, check point 3 scaler.update) If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params. scaler.update(): It would be great if we could just multiply all gradients by a super big number so absolutely no underflow happens, but doing so can cause overflow. The scaler estimates a good scaling factor for each iteration, so neither underflow nor overflow happens. scaler.update() updates scaler’s scale factor for next iteration. 1234567891011scaler = torch.cuda.amp.GradScaler()for epoch in epochs: for input, target in data: optimizer.zero_grad() with autocast(device_type=&#x27;cuda&#x27;, dtype=torch.float16): output = model(input) loss = loss_fn(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() Working with Unscaled Gradients - Gradient clipping gradient clipping manipulates a set of gradients such that their global norm torch.nn.utils.clip_grad_norm_() or maximum magnitude torch.nn.utils.clip_grad_value_() is &lt;= some user-imposed threshold. The “gradients” here of course refer to the original, unscaled gradients. Therefore, you need to call scaler.unscale_(optimizer) before clipping. 123456789101112131415161718192021scaler = GradScaler()for epoch in epochs: for input, target in data: optimizer.zero_grad() with autocast(device_type=&#x27;cuda&#x27;, dtype=torch.float16): output = model(input) loss = loss_fn(output, target) scaler.scale(loss).backward() # Unscales the gradients of optimizer&#x27;s assigned params in-place scaler.unscale_(optimizer) # Since the gradients of optimizer&#x27;s assigned params are unscaled, clips as usual: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) # optimizer&#x27;s gradients are already unscaled, so scaler.step does not unscale them, # although it still skips optimizer.step() if the gradients contain infs or NaNs. scaler.step(optimizer) scaler.update() Working with Scaled Gradients - Gradient accumulation Gradient accumulation adds gradients over an effective batch of size batch_per_step * gradient_accumulation_steps (* num_procs if distributed). Operations related to scaled gradients should occur at effective batch granularity. The following happens at the end of each effective batch: inf/NaN checking step skipping if inf/NaN grads are found parameter update scale update Within an effective batch, all grads you accumulate should all be scaled and the scale factor should remain unchanged. 123456789101112131415161718scaler = GradScaler()for epoch in epochs: for micro_step in range(gradient_accumulation_steps): input, target = get_data(epoch, micro_step) with autocast(device_type=&#x27;cuda&#x27;, dtype=torch.float16): output = model(input) loss = loss_fn(output, target) loss = loss / gradient_accumulation_steps # Accumulates scaled gradients. scaler.scale(loss).backward() # If you need to work with unscaled gradients, # after all (scaled) grads for the upcoming step have been accumulated # may unscale_ here if desired (e.g., to allow clipping unscaled gradients) scaler.step(optimizer) scaler.update() optimizer.zero_grad() These examples may seem too vanilla, check out nanoGPT’s mixed precision training loop for a lively combination of gradient accumulation and gradient clipping. Working with Scaled Gradients - Gradient penalty What? Why? https://discuss.pytorch.org/t/whats-the-use-of-scaled-grad-params-in-this-example-of-gradient-penalty-with-scaled-gradients/199741/3 Epilogue This wiki page from Deepgram provides a detailed view of what gradient scaling is about, but I don’t know why it just reads like AI-generated content. Maybe because it gives too many unnecessary details.","categories":[],"tags":[]},{"title":"Decoupled Weight Decay Regularization (SGDW & AdamW)","slug":"2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&-AdamW)","date":"2024-03-13T04:00:00.000Z","updated":"2025-09-02T23:51:12.124Z","comments":true,"path":"2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&-AdamW)/","permalink":"https://yao-lirong.github.io/blog/2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&-AdamW)/","excerpt":"The paper Decoupled Weight Decay Regularization mainly introduces AdamW, which is the SOTA optimizer since then. It investigates why Adam with L2 regularization sometimes performs worse than SGD with L2 regularization. It demonstrates weight decay and L2 regularization, two things people usually draw an equal sign, are not the same. And it shows weight decay is the ultimate go-to choice.","text":"The paper Decoupled Weight Decay Regularization mainly introduces AdamW, which is the SOTA optimizer since then. It investigates why Adam with L2 regularization sometimes performs worse than SGD with L2 regularization. It demonstrates weight decay and L2 regularization, two things people usually draw an equal sign, are not the same. And it shows weight decay is the ultimate go-to choice. Weight decay and L2 regularization are equivalent in SGD when set L2 regularizer $\\lambda' = \\frac \\lambda \\alpha$, which is our common practice. The situation is more complicated with adaptive gradient algorithms like Adam. Adam performs much better with weight decay and the authors propose the new SOTA optimizer AdamW (Adam with decoupled weight decay). All the conclusions and main finding can be found in the first 2 pages of the paper and mostly in the Introduction section. I did not read the math. This blogpost from Fast.ai demonstrates how the two methods are different in code, a bit easier to understand than the paper which doesn’t provide a comparision. Weight Decay in Transformers AdamW is the go-to optimizer for LLM these days. Researchers chose it because LLMs are hard to train and rarely overfit, and Adam is the best choice when convergence speed is considered (reference). People have also found AdamW usually performs best with big weight decay coefficient like 0.05 or 0.1 (zhihu question, ViT paper: Training &amp; Fine-tuning section) When we apply weight decay in transformers, we apply it to all layers except LayerNorm and bias layers. In nanoGPT, Karpathy filtered them out using: 12345678# create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.# i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#x27;t.decay_params = [p for n, p in param_dict.items() if p.dim() &gt;= 2]nodecay_params = [p for n, p in param_dict.items() if p.dim() &lt; 2]optim_groups = [ &#123;&#x27;params&#x27;: decay_params, &#x27;weight_decay&#x27;: weight_decay&#125;, &#123;&#x27;params&#x27;: nodecay_params, &#x27;weight_decay&#x27;: 0.0&#125;] One caveat is that, in earlier versions, Karpathy did NOT weight decay embeddings: 12345blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)...elif pn.endswith(&#x27;weight&#x27;) and isinstance(m, blacklist_weight_modules): # weights of blacklist modules will NOT be weight decayed no_decay.add(fpn) I couldn’t find any instruction on whether you should decay embeddings or not when training a transformer, but Hugging Face’s transformer implementation also decays embeddings, in line with Karpathy’s latest implementation. 123# get_parameter_names(model, name) excludes layers with `name`decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)decay_parameters = [name for name in decay_parameters if &quot;bias&quot; not in name]","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Mixed-Precision Training","slug":"2024-03-01-Mixed-Precision-Training","date":"2024-03-01T05:00:00.000Z","updated":"2025-09-02T23:51:12.127Z","comments":true,"path":"2024-03-01-Mixed-Precision-Training/","permalink":"https://yao-lirong.github.io/blog/2024-03-01-Mixed-Precision-Training/","excerpt":"Mixed-precision training was introduced in Nvidia and Baidu’s research. The blogpost from Nvidia gave a nice summary of how it’s done and why it works. Nvidia also gave a more in-depth coverage of the same points in their tutorial on training with mixed precision.","text":"Mixed-precision training was introduced in Nvidia and Baidu’s research. The blogpost from Nvidia gave a nice summary of how it’s done and why it works. Nvidia also gave a more in-depth coverage of the same points in their tutorial on training with mixed precision. I decided to learn this as I was reading nanoGPT’s code: 12torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmultorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn Benefits Decrease the required amount of memory: FP32 -&gt; FP16 Shorten the training or inference time: memory bandwith: half-precision halves the number of bytes need to be accessed, thus reducing time-spent in memory-limited operations arithmetic bandwidth: half-precision arithmatic is inherintely faster than single-precision 3 Techniques in Original Paper Accumulation into FP32: then convert to FP16 for storage Loss Scaling (Gradient Scaling): There are four types of tensors encountered when training DNNs: activations, activation gradients, weights, and weight gradients. In experience activations, weights, and weight gradients can be represented with half precision. However, for some networks activation gradients are too small to be represented in half-precision range (underflow) Therefore, we need to scale up the activation gradients. This can be done by simply multiply the training loss with the scale factor. This adds just a single multiplication and by the chain rule it ensures that all the gradients are scaled up at no additional cost. FP32 Master Copy of Weights: Weight gradient magnitudes are smaller than corresponding weights, especially after multiplication with the learning rate. So sometimes no update takes place. The remedy is to store the weights in single precision, but do computation in half precision. Update this master copy of weights after each computation. More Recent Update In NVIDIA Ampere GPU architecture, Nvidia introduced TensorFloat32 (TF32) with FP32 range (8bit) and FP16 precision (10bit). With the additional sign bit, it is a novel 19 bit representation of floats. On an A100, it brings 8x speed up compared to FP32, while FP16/BF16 brings 16x speedup. Therefore, mixed-precision training with a native 16-bit format (FP16/BF16) is still the fastest option. TF32 is only exposed as a Tensor Core operation mode, not a type. Internally, all storage in memory and other operations remain completely in FP32, only convolutions and matrix-multiplications convert their inputs to TF32 right before multiplication. Therefore, it does not provide the memory benifits or the native arithmetic speed up brought by 16-bit format. Its benefit is that it brings Tensor Core acceleration to single-precision DL workloads, without needing any changes to model scripts. It needs to be noted that TF32 gives less accurate computation results. Therefore, PyTorch decided to set toggle torch.backends.matmul.allow_tf32 = False by default starting from 1.12. Read more about PyTorch’s official comparison of speed and numerical stability. Best Practices PyTorch gave some official best practices tips to developers. Please do check them out here.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Parameter and FLOP Count in Transformer Model","slug":"2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model","date":"2024-02-22T05:00:00.000Z","updated":"2025-09-02T23:34:26.325Z","comments":true,"path":"2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/","permalink":"https://yao-lirong.github.io/blog/2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/","excerpt":"We borrow the results of decoder-only transformer models from OpenAI’s paper Scaling Laws for Neural Language Models Section 2.1","text":"We borrow the results of decoder-only transformer models from OpenAI’s paper Scaling Laws for Neural Language Models Section 2.1 We use the following notations: L = number of layers of transformer blocks (N in Attention is All You Need) dmodel = dimension of the input &amp; output of a transformer block, also the output of the text encoder and input of the decoder dff = dimension of the feed-forward network’s bottleneck. We defined the feed-forward network as fc1 = fc(d_model, d_ff), fc2 = fc(d_ff, d_model) dattn = dimension of the multi-head attention output (In Attention is All You Need, we have h number of heads. Queries and keys have dimension dk. Values have dimension dv. In practice, we usually have dk = dv. dattn we have here is defined as dk × h) Part Parameters Explanation Embed $n_{vocab}\\times d_{model} \\\\ +n_{ctx} \\times d_{model}$ One word embedding matrix (mapping each token to corresponding embedding ) and one positional embedding matrix Attention: Q K V Matrix L3dmodeldattn WQ has shape (dmodel, dattn). There’re also WK and WV Attention: Multi-head Projection $L d_{attn} d_{model} $ After we concat the output from all heads, there’s one projection from all-head output to the final output. This is that matrix. It was defined as WO in Attention is All You Need 3.2.2. Feedforward Network L2dmodeldff Explained in the definition of dff above. Total (Non-Embedding) 2Ldmodel(2dattn + dff) If we have the standard dattn = dmodel = dff/4, we can get N = 12Ldmodel2 Put this into practice, let’s calculate a rough estimate of number of parameters the vanilla transformer has. The vanilla transformer base, per the paper Attention is All You Need Table 3, L = 6, dmodel = 512, dff = 2048, dattn = h × dk = 8 × 64 = 512, nvocab = 37000. I didn’t find info about nctx, but is probably 512. Note that different from OpenAI’s favorite decoder-only transformer, the vanilla transformer has an encoder-decoder architecture and the decoder block has an additional attention block. Therefore, the encoder has a total 2Ldmodel(2dattn + dff) parameters, the decoder has a total 2Ldmodel(4dattn + dff) params, and the embedding part has a total nvocab × dmodel + nctx × dmodel params. The final result is ∼ 63 × 106. I tried hard to figure out where went off from the paper’s 65 × 106 but had no luck. Adding the parameters of LayerNorm still didn’t even out the numbers. But it’s close enough so I’ll call it a day.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Memory Pinning and Transfer Data between Host (CPU) and Device (GPU)","slug":"2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)","date":"2024-02-09T05:00:00.000Z","updated":"2025-09-02T23:51:12.127Z","comments":true,"path":"2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/","permalink":"https://yao-lirong.github.io/blog/2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/","excerpt":"PyTorch official documentation explains this concept very briefly and we go into more detail here.","text":"PyTorch official documentation explains this concept very briefly and we go into more detail here. What is memory pinning and why we use it First, let’s go back to our OS class and remind what “paged memory” means. Process always wants contiguous memory. The OS uses memory paging to enable logically contiguous memory that is not physically contiguous. When a process requests memory, OS allocates page frames to the process. These page frames look contiguous to the process, but are actually not so in physical memory. The OS then maps the process’s logical pages to the physical page frames. This Nvidia blog on data transfer explains what this has to do with GPU: The GPU cannot access data directly from pageable host memory (logically contiguous), so when a data transfer from pageable host memory to device memory is invoked, the CUDA driver must first allocate a temporary page-locked, or “pinned”, physically contiguous host array, copy the host data to the pinned array, and then transfer the data from the pinned array to device memory. pinned memory Therefore, we can avoid the cost of the transfer between pageable and pinned host arrays by directly allocating our host arrays in pinned memory. To my understanding, “directly allocating in pinned memory” corresponds to what’s described in DataLoader’s documentation as: 1loader = DataLoader(dataset, pin_memory=True) pin_memory() and non_blocking=True On the other hand, while reading nanoGPT’s code, I saw the following code: 123if device_type == &#x27;cuda&#x27;: # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True) x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True) pin_memory is familiar to us while non_blocking is something new. It tells the program that it can perform other operations on this data while it being trasferred from host to device. (so don’t block till transfer is done to start the operation) This async copy usually speeds things up. This Stack Overflow answer gives a detaild example of the async part. Here in the code, we are explicitly calling pin_memory() on something already initialized, which really confused me. Since according to the above quoted Nvidia blog, “when a data transfer from pageable host memory to cuda device memory is invoked, the CUDA driver must first allocate a pinned host array, copy the host data to the pinned array, and then transfer the data from the pinned array to device memory.” That is to say: even without such an explicit pin_memory() call, CUDA will do it for us. I found this exchange on PyTorch’s forum and also asked this question myself, but didn’t receive a super clear answer. But inferring from what @ptrblck said, I think it is correct to say that the following two commands are equal in speed (with the first pinning memory implicitly and the second does it explicitly) t.to(\"cuda\", non_blocking=False) t.pin_memory().to(\"cuda\", non_blocking=False) and explicit memory pinning call is only useful when used together with to(device, non_blocking=True) Someone in this Zhihu discussion also argues paged memory can be exchanged into disk swap when physical memory is not enough. Explicitly pinning memory avoids this problem and saves the time of finding these pages in disk for every query (pinning brings them all out into physical memory). The poster did not give a reference though.","categories":[],"tags":[]},{"title":"Switching Personal Homepage Theme to al-folio","slug":"2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio","date":"2024-01-27T05:00:00.000Z","updated":"2024-01-28T09:22:54.542Z","comments":true,"path":"2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/","permalink":"https://yao-lirong.github.io/blog/2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/","excerpt":"Since long before have I realized I do need a more official and more academic homepage in addition to a personal blog site, but I didn’t find time to do it until I started working. Now after this switch, I have my personal homepage of al-folio in Jekyll and at the same time my blog of archer in Hexo.","text":"Since long before have I realized I do need a more official and more academic homepage in addition to a personal blog site, but I didn’t find time to do it until I started working. Now after this switch, I have my personal homepage of al-folio in Jekyll and at the same time my blog of archer in Hexo. Migrating Hexo Blog to Sub-Directory I want to keep this archer theme for my blog while creating a new personal homepage. Therefore, I have to move this repo out of the GitHub page repo and into some other blog repo. That is, this blog will now be served as a GitHub project page, like my ARC 40. What needs to be done was surprisingly simple: Create a GitHub repo to store your webpages: This repo will contain the exact same information as my previous Yao-Lirong/Yao-Lirong.github.io. I created Yao-Lirong/blog. Change your Hexo _config.yml. You can reference the official guide on deploying, but they didn’t mention how to one-command deploy a project page. 123456789101112# URL## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;url: https://yao-lirong.github.io/blogroot: /blog/# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: git repo: https://github.com/Yao-Lirong/blog branch: master message: Updated at &#123;&#123; now(&#x27;YYYY-MM-DD HH:mm&#x27;) &#125;&#125; hexo clean &amp;&amp; hexo deploy Configuring al-folio Local Install Since this migration was a long process, when I was working on al-folio, my archer page was still up and I have to first try out al-folio locally to not break my current GitHub page. To do this, the only feasible way is to use docker as suggested in their install instruction. Don’t bother to try other ways. You won’t have any luck running them. 1234git clone --depth 1 https://github.com/alshedivat/al-folio.gitcd al-folio/docker compose pulldocker compose up Personalization _pages/*.md will be shown on the navigation bar. I deleted most of them and only kept _pages/about.md, cv.md, projects.md _news/ directory contains what will be shown on the homepage “news” section. Do change those _sass/_themes.scss has a variable --global-theme-color controlling the theme of this site. I had 12--global-theme-color: #&#123;$cyan-color&#125;;--global-hover-color: #&#123;$light-cyan-color&#125;; In _config.yaml, we see that projects is listed under collections. This means we can build the following file structure, where we have _pages/projects.md to be displayed on the navigation bar as an entry point and projects/p1.md for a specific project. Find more about collections in README 123456_pages/├─ projects.md - mywebsite.com/projects/projects/├─ p1.md - mywebsite.com/projects/p1├─ p2.md - mywebsite.com/projects/p2├─ .... Integrating Hexo Blog into New Homepage Delete any blog-related feature that came along with al-folio: reference Delete all the example posts: rm -rf ./_posts/ Delete the template generating mywebsite.com/blog page: rm -rf ./blog/ Comment out the following entries under Jekyll Archives section and Blog section in _config.yml to make sure absolutely nothing is generated under mywebsite.com/blog address: (I was about to comment out everything, but it turns out this would break some content generation features, so after some experiments I found you only need to comment out the followings:) 1234567891011121314151617181920212223242526# -----------------------------------------------# Blog# -----------------------------------------------permalink: /blog/:year/:title/related_blog_posts: enabled: true max_related: 5# -----------------------------------------------# Jekyll Archives# -----------------------------------------------jekyll-archives: enabled: [year, tags, categories] # enables year, tag and category archives (remove if you need to disable one of them). layouts: year: archive-year tag: archive-tag category: archive-category permalinks: year: &quot;/blog/:year/&quot; tag: &quot;/blog/tag/:name/&quot; category: &quot;/blog/category/:name/&quot;display_tags: [&quot;formatting&quot;, &quot;images&quot;, &quot;links&quot;, &quot;math&quot;, &quot;code&quot;] # these tags will be displayed on the front page of your blogdisplay_categories: [&quot;blockquotes&quot;] # these categories will be displayed on the front page of your blog Set the blog entry on the navigation bar link to my actual blog address: set _config.yml blog_nav_title: to empty, so the al-folio’s blog entry will not show on the navigation bar, reference 1, reference 2 Find in _includes/header.html the if statement &#123;% if site.blog_nav_title %&#125; go to the end of this if statement and add something: reference 123456789101112131415&#123;% if site.blog_nav_title %&#125;&lt;!-- Blog --&gt;&lt;!-- Empty here because blog_nav_title attribute is empty --&gt;&lt;li class=&quot;nav-item &#123;% if page.url contains &#x27;blog&#x27; %&#125;active&#123;% endif %&#125;&quot;&gt; &lt;a class=&quot;nav-link&quot; href=&quot;&#123;&#123; &#x27;/blog/&#x27; | relative_url &#125;&#125;&quot;&gt;&#123;&#123; site.blog_nav_title &#125;&#125; &#123;%- if page.url contains &#x27;blog&#x27; -%&#125; &lt;span class=&quot;sr-only&quot;&gt;(current)&lt;/span&gt; &#123;%- endif -%&#125; &lt;/a&gt;&lt;/li&gt;&#123;%- endif %&#125;&lt;!-- Actual Blog Takes Place Here --&gt;&lt;li class=&quot;nav-item&quot;&gt; &lt;a class=&quot;nav-link&quot; href=&quot;https://yao-lirong.github.io/blog/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; One thing really good about al-folio is that it allows including external blog posts. You can do this by adding the RSS feed in _config.yml: reference 123external_sources: - name: blog rss_url: https://yao-lirong.github.io/blog/atom.xml These external posts can also be displayed on your homepage if you set latest_posts: true in the front matter of _pages/about.md TBD how about sitemap? GA4 tags check do a photowall?","categories":[],"tags":[]},{"title":"Visual Information Theory","slug":"2024-01-21-Visual-Information-Theory","date":"2024-01-21T05:00:00.000Z","updated":"2025-09-02T23:24:43.202Z","comments":true,"path":"2024-01-21-Visual-Information-Theory/","permalink":"https://yao-lirong.github.io/blog/2024-01-21-Visual-Information-Theory/","excerpt":"This blog post is adapted from ex-OpenAI researcher, Anthropic co-founder Christopher Olah’s wonderful work. I removed parts that are generally commonsense to a CS kid and added some of my own notes &amp; explanations.","text":"This blog post is adapted from ex-OpenAI researcher, Anthropic co-founder Christopher Olah’s wonderful work. I removed parts that are generally commonsense to a CS kid and added some of my own notes &amp; explanations. Visualizing Probability Distribution The author did a really cool job visualizing probability distribution here, but doesn’t provide any more knowledge than basic probability theory, so removed this part. Code I want to communicate with my friend Bob, so we establish a code, mapping each possible word we may say to sequences of bits. fixed length code However, Bob is a dog lover, with very high probability, he talks about dogs. Bob’s word frequency Incorporating the code we defined above into this graph, we get the following diagram, with the vertical axis to visualize the probability of each word, p(x), and the horizontal axis to visualize the length of the corresponding codeword, L(x). Notice the total area is the expected length of a codeword we send. Expected length of fixed-length code Variable-Length Code Perhaps we could be very clever and make a variable-length code where codewords for common words are made especially short. The challenge is that there’s competition between codewords – making some shorter forces us to make others longer. To minimize the message length, we especially want the commonly used ones to be. So the resulting code has shorter codewords for common words (like “dog”) and longer codewords for less common words (like “bird”). Variable length code Looking at this code format with word frequency, on average, the length of a codeword is now 1.75 bits! Expected length of variable-length code It turns out that this code is the best possible code. There is no code which, for this word frequency distribution, will give us an average codeword length of less than 1.75 bits. There is simply a fundamental limit. Communicating what word was said, what event from this distribution occurred, requires us to communicate at least 1.75 bits on average. No matter how clever our code, it’s impossible to get the average message length to be less. We call this fundamental limit the entropy of the distribution – we’ll discuss it in much more detail shortly. The Space of Codewords To make our codewords uniquely decodable, we want them to follow the prefix property: no codeword should be the prefix of another codeword. i.e. If we see a particular codeword, there shouldn’t be some longer version that is also a codeword. Codes that obey this property are also called prefix codes. One useful way to think about this is that every codeword requires a sacrifice from the space of possible codewords. If we take the codeword 01, we lose the ability to use any codewords it’s a prefix of. We can’t use 010 or 011010110 anymore because of ambiguity – they’re lost to us. The following graph shows we in effect lost $\\frac {1} {2^{L(01)}} = \\frac {1} {2^2} = \\frac {1} {4}$ of our codeword space. Code space sacrificed Since a quarter of all codewords start with 01, we’ve sacrificed a quarter of all possible codewords. That’s the price we pay in exchange for having one codeword that’s only 2 bits long! In turn this sacrifice means that all the other codewords need to be a bit longer. There’s always this sort of trade off between the lengths of the different codewords. A short codeword requires you to sacrifice more of the space of possible codewords, preventing other codewords from being short. What we need to figure out is what the right trade off to make is! Cost of Codeword You can think of this like having a limited budget to spend on getting short codewords. In fact, we have a budget = 1 to spend, where 1 is the area of the whole codeword space. We pay for one codeword by sacrificing a fraction of possible codewords. We define the cost c of having a code word x with length L as $\\frac 1 {2^{L(x)}}$. The cost of buying a codeword of length 0 is 1, all possible codewords – if you want to have a codeword of length 0, you can’t have any other codeword. $x = \\emptyset, c = \\frac 1 {2^{L(\\emptyset)}} = \\frac 1 {2^0}$ The cost of a codeword of length 1, like “0”, is 1/2 because half of possible codewords start with “0”. $x = 0, c = \\frac 1 {2^{L(\"0\")}} = \\frac 1 {2^1}$ The cost of a codeword of length 2, like “01”, is 1/4 because a quarter of all possible codewords start with “01” $x = 0, c = \\frac 1 {2^{L(\"01\")}} = \\frac 1 {2^2}$ … (ignore the gray area in the following graph, the function we plot is $\\frac 1 {2^L}$ and cost is the height) Code cost To make the calculation simpler, instead of base 2, we imagine we have the natural number e. It now is no longer a binary code, but a “natural” code with the cost becomes $\\frac 1 {e^L}$. It is both the height and the gray area: $\\frac 1 {e^L} = \\int^\\infty_L \\frac 1 {e^L} \\; dL$ Optimal Encoding Recall the expected length of the codeword we pictured above, if we look at each codeword’s contribution to this expected length, each codeword makes the average message length longer by its probability times the length of the codeword. For example, if we need to send a codeword that is 4 bits long 50% of the time, our average message length is 2 bits longer than it would be if we weren’t sending that codeword. We can picture this as a rectangle. Contribution to expected length Now, we have two values related to the length of a codeword and we picture them together in the following graph: The amount we pay decides the length of the codeword: gray part decides the width of the purple rectangle The length of the codeword controls how much it adds to the average message length: width of the rec decides the area of the rec (height is fixed for a given word because height represents the word frequency) Contribution and cost Short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheap. short vs long What’s the best way to use our limited budget? Just like one wants to invest more in tools that one uses regularly, we want to spend more on frequently used codewords. There’s one particularly natural way to do this: distribute our budget in proportion to how common an event is - c(x) = p(x). The following graph shows such encoding system. Additionally, that’s exactly what we did in our codes with Bob: “dog” appears $\\frac 1 2$ of the time, so we give it codeword “0” with the cost $c = \\frac 1 {2^{L(\"0\")}} = \\frac 1 {2^1}$; “bird” appears $\\frac 1 8$ of the time, so we give it codeword “111” with the cost $c = \\frac 1 {2^{L(\"111\")}} = \\frac 1 {2^3}$; If an event only happens 1% of the time, we only spend 1% of our budget. Contribution and cost line up The author then proved it is the optimal thing to do. Honestly I didn’t understand the proof, but this code system we’re talking about here is no more than a general version of Huffman Encoding. So I guess it’s fine as long as you understand how to prove the greedy Huffman Encoding is optimal. Calculating Entropy In the Cost of Codeword section, we defined the cost c of having a code word x with length L as $\\frac 1 {2^{L(x)}}$. By inverting this definition, given c, we can infer the length $L(x) = \\log_2 \\frac 1 c$ Furthermore, in the Optimal Encoding section, we proved the optimal cost to spend on each word is c(x) = p(x), so the length of the optimal encoding system is $L(x) = \\log_2 \\frac 1 {p(x)}$ Earlier, we said: given a probability distribution of what we want to communicate, there is a fundamental limit of expected code length we need to send no matter how smart our code is. This limit, the expected code length using the best possible code, is called the entropy of p: H(p). $$ H(p) = \\mathbb{E}_{p(x)} [L(x)] = \\sum_x p(x) \\log_2 \\frac 1 {p(x)} $$ People usually write entropy in the following way, which makes it much less intuitive. H(p) = −∑xp(x)log2p(x) The entropy, which by definition is the shortest possible code, has clear implications for compression. In addition, it describes how uncertain I am and gives a way to quantify information: If I knew for sure what was going to happen, I wouldn’t have to send a message at all! p(x) = 1 ⟹ H(x) = 0 If there’s two things that could happen with 50% probability, I only need to send 1 bit. ∀xi, p(xi) = 0.5 ⟹ H(x) = 1 If there’s 64 different things that could happen with equal probability, I’d have to send 6 bits. $\\forall x_i, p(x_i) = \\frac 1 {64} \\implies H(x) = 6$ The more concentrated the probability, the more I can craft a clever code with short average messages. The more diffuse the probability, the longer my messages have to be. Cross Entropy Imagine now a cat-lover Alice talks about animals with the following word frequency: Alice loves cats When Alice sends a message to Bob using Bob’s codes, her messages were longer than they needed to be. Bob’s code was optimized to his probability distribution. Alice has a different probability distribution, and the code is suboptimal for it: the expected codeword length when Bob uses his own code is 1.75 bits, while when Alice uses his code it’s 2.25. It would be worse if the two weren’t so similar! The expected length of communicating an event from distribution q(x) with the optimal code for another distribution p(x) is called the cross-entropy. Formally, we define cross-entropy as: (people usually write H(q, p) instead) $$ H_p(q) = \\mathbb E_{q} \\log_2\\left(\\frac{1}{p(x)}\\right)= \\sum_x q(x)\\log_2\\left(\\frac{1}{p(x)}\\right) $$ To lower the cost of Alice sending message, I asked both of them to now use Alice’s coding, but this made it suboptimal for Bob and surprisingly, it’s worse for Bob to use Alice’s code than for Alice to use his! Cross-entropy isn’t symmetric. Bob using his own code (H(p) = 1.75 bits) Alice using Bob’s code (Hp(q) = 2.25 bits) Alice using her own code (H(q) = 1.75 bits) Bob using Alice’s code (Hq(p) = 2.375 bits) In the following diagram, if the messages are coming from the same distribution the plots are beside each other, and if they use the same codes they are on top of each other. This allows you to kind of visually slide the distributions and codes together. 4 cases of cross entropy So why one is bigger than the other? This is because When Bob uses Alice’s code Hq(p), Bob says “dog” with high probability but “dog” is a word Alice happens to say the least so assigned longest length to. On the other hand for Hp(q), Bob didn’t dislike “cats” that much, so didn’t assign it with too big a word length. KL Divergence Cross-entropy gives us a way to express how different two probability distributions are. When using q’s codewords for p’s distribution, the more different the distributions p and q are, the bigger the difference is between cross-entropy of p with respect to q and the entropy of p. In math, Dq(p) = Hq(p) − H(p) is bigger. H_q(p) Similarly when using p’s codewords for q’s distribution. H_p(q) The really interesting thing is the difference Dq(p). That difference is how much longer our messages are because we used a code optimized for a different distribution q. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger. This difference is the Kullback–Leibler divergence, or just the KL divergence. The really neat thing about KL divergence is that it’s like a distance between two distributions. It measures how different they are! (If you take that idea seriously, you end up with information geometry) People usually write it as DLK(p||q), but our more intuitive way writes: Dq(p) = Hq(p) − H(p) If you expand the definition of KL divergence, you get: $$ \\begin{align} D_q(p) &amp;= \\sum_x p(x)\\log_2\\left(\\frac{p(x)}{q(x)} \\right) \\\\ &amp;= \\sum_x p(x) \\left[ \\log_2\\left(\\frac{1}{q(x)} \\right) - \\log_2\\left(\\frac{1}{p(x)} \\right)\\right]\\\\ &amp;= \\sum_x p(x) \\left[ L_q(x) - L_p(x)\\right] \\end{align} $$ And we see the $\\log_2\\left(\\frac{p(x)}{q(x)} \\right)$ is simply the difference between how many bits a code optimized for q and a code optimized for p would use to represent x. The expression as a whole is the expected difference in how many bits the two codes would use Multiple Variables and Joint Entropy Let’s return to our weather and clothing example from earlier. My mother, like many parents, worries that I don’t dress appropriately for the weather. So, she often wants to know both the weather and what clothing I’m wearing. How many bits do I have to send her to communicate this? To send both pieces of information, we can flatten the probability distribution (calculate the joint probability) flattened probability Now we can figure out the optimal codewords for events of these probabilities and compute the average message length: joint word length Everything is the exact same as our normal definition, except with two variables instead of one. We call this the joint entropy of X and Y, defined as $$ H(X,Y) = \\mathbb E_{p(x,y)} \\log_2\\left(\\frac{1}{p(x,y)}\\right) = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{1}{p(x,y)}\\right) $$ It’s in fact more intuitive not to flatten it, but Instead to keep the 2 dimensional square and add word length as a 3rd dimension height. Now the entropy is the volume. 3D code length Conditional Entropy Suppose my mom already knows the weather. She can check it on the news. Now how much information do I need to provide? I actually need to send less, because the weather strongly implies what clothing I’ll wear! Conditional code length separated When it’s sunny, I can use a special sunny-optimized code, and when it’s raining I can use a raining optimized code. In both cases, I send less information than if I used a generic code for both. For example in case of raining, I send code length of 4 when I wear t-shirt and code length of 4/3 when I wear coat. $$ H(X \\mid \\text{Y = raining}) = \\frac 1 4 \\log_2 \\frac 1 4 + \\frac 3 4 \\log_2 \\frac 3 4 = 0.81 $$ (Don’t worry for now why the entropy, representing length of an optimal codeword, can be fractional, we will explain it later) With a similar calculation, we show H(X ∣ Y = sunny) = 0.81. To get the average amount of information I need to send my mother, I just put these two cases together: $$ \\begin{align} H(X \\mid Y) &amp;= P(\\text{Y = rainy})\\,H(X \\mid \\text{Y = rainy})+ P(\\text{Y = sunny})\\,H(X \\mid \\text{Y = sunny}) \\\\ &amp;= \\frac 1 4 \\times 0.81 + \\frac 3 4 \\times 0.81 = 0.81 \\end{align} $$ Conditional code length together We call this the conditional entropy: $$ \\begin{align} H(X|Y) &amp;= \\sum_y p(y) H(X \\mid Y=y) \\\\ &amp;= \\sum_y p(y) \\sum_x p(x|y) \\log_2\\left(\\frac{1}{p(x|y)}\\right) \\\\ &amp;= \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{1}{p(x|y)}\\right) \\end{align} $$ Mutual Information In the previous section, we observed that knowing one variable can mean that communicating another variable requires less information. One nice way to think about this is to imagine amounts of information as bars. These bars overlap if there’s shared information between them: some of the information in X and Y is shared between them, so H(X) and H(Y) are overlapping bars. And since H(X, Y) is the information in both, it’s the union of the bars H(X) and H(Y). Multi-variate bar 1 We rank the information needed to communicate these 3 kinds of things in descending order: Both X and Y: joint entropy H(X, Y) X alone: marginal entropy H(X) X when Y is known: conditional entropy H(X|Y) Ranking information contained In the bar perspective, H(X|Y) is the information we need to send to communicate X to someone who already knows Y - it is the information in X which isn’t also in Y. Visually, that means H(X|Y) is the part of H(X) bar which doesn’t overlap with H(Y). Multi-variate bar 2 From this, we get another identity: the information in X and Y is the information in Y plus the information in X which is not in Y. (sounds like set, doesn’t it?) H(X, Y) = H(Y) + H(X|Y) To wrap up, we have information in each variable: H(X) and H(Y) union of the information in both: H(X, Y) information in one but not the other: H(X|Y) and H(Y|X) We can further define mutual information: information both in X and Y, or in set terms, the intersection of information: I(X, Y) = H(X) + H(Y) − H(X, Y) If you expand the definition of mutual information out, you get: $$ I(X,Y) = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)} \\right) $$ That looks suspiciously like KL divergence! Well, it is KL divergence. It’s the KL divergence of P(X, Y) and its naive approximation P(X)P(Y). That is, it’s the number of bits you save representing X and Y if you understand the relationship between them instead of assuming they’re independent. and the variation of information. The variation of information is the information which isn’t shared between the variables. It gives a metric of distance between different variables. The variation of information between two variables is zero if knowing the value of one tells you the value of the other and increases as they become more independent. V(X, Y) = H(X, Y) − I(X, Y) How does this relate to KL divergence, which also gave us a notion of distance? Well, KL divergence gives us a distance between two distributions over the same variable or set of variables. In contrast, variation of information gives us distance between two jointly distributed variables. KL divergence is between distributions, variation of information within a distribution. In summary, we put them all in a single diagram: Multi-variate bar 3 Fractional Bits A careful reader may have noticed that in previous calculations, we had fractional length of message. Isn’t that weird? How should we interpret such a message? How is it done in real life? The answer is: you can think of them as expected length of a message. If half the time one sends a single bit, and half the time one sends two bits, on average one sends one and a half bits. There’s nothing strange about averages being fractional. That’s a quick but vague answer. Let’s look at an example: consider a probability distribution where one event a happens 71% of the time and another event b occurs 29% of the time. Fractional bit single The optimal code would use 0.5 bits to represent a, and 1.7 bits to represent b. Well, if we want to send a single one of these codewords, it simply isn’t possible. We’re forced to round to a whole number of bits, and send on average 1 bit. … But if we’re sending multiple messages at once, it turns out that we can do better. Let’s consider communicating two events from this distribution. If we send them independently, using the code we established for a single event, we’d need to send two bits. Can we do better? Fractional bit double Half the time, we need to communicate aa, 21% of the time we need to send ab or ba, and 8% of the time we need to communicate bb. Again, the ideal code involves fractional numbers of bits. Fractional bit double code length If we round the codeword lengths, we’ll get something like this: Fractional bit double code length rounded This codes give us an average message length of 1.8 bits. That’s less than the 2 bits when we send them independently. Another way of thinking of this is that we’re sending 1.8/2 = 0.9 bits on average for each event. If we were to send more events at once, it would become smaller still. As n tends to infinity, the overhead due to rounding our code would vanish, and the number of bits per codeword would approach the entropy. Further, notice that the ideal codeword length for a was 0.5 bits, and the ideal codeword length for aa was 1 bit. Ideal codeword lengths add, even when they’re fractional! So, if we communicate a lot of events at once, the lengths will add. Conclusion Entropy is optimal length .. KL divergence .. Entropy over multiple variables can be interpreted as sets of information","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Quantization","slug":"2023-12-01-Quantization","date":"2023-12-01T05:00:00.000Z","updated":"2025-09-02T23:51:12.130Z","comments":true,"path":"2023-12-01-Quantization/","permalink":"https://yao-lirong.github.io/blog/2023-12-01-Quantization/","excerpt":"","text":"K-bit Inference Scaling Laws This paper and its Appendix serves as a good summary of SOTA quantization techniques and their results. Why should we quantize? The overall computation latency – the time it takes from start to finish of a computation – is mainly determined by two factors: (1) how long does it take to load the data from main memory into caches and registers, (2) how long does it take to perform the computation. Therefore, reducing the time spent loading data from main memory is often the best way to accelerate overall computation latency. Such reductions can be achieved mainly through caching and lower precision numbers. Note though, to do computation, we dequantize the weight in the cache and perform a 16-bit floating point multiplication with the 16-bit input. That’s because no CPU / GPU supports these weird data type computation. In this work, the author used blocking, a zero-shot quantization method, for 3, 4, 5, 6, 7, and 8 bits. By plotting out perplexity vs total bits of the model, they found that lowering the bit precision generally improves scaling. However, this trend stops across all models at 3-bit precision, where performance degrades. Therefore, 4-bit precision is optimal for almost all models at all scales, with few exceptions. BLOOM and BLOOMZ show almost the same quantization behavior, indicating that fine-tuning an existing model does not change its quantization properties. Data types: The quantile quantization and float data types provide better scaling than integer and dynamic exponent quantization. The author concluded that quantile quantization is the best. zero-shot quantization: directly quantize a model without any additional information. Can be used immediately, which makes them easy to use, but zero-shot quantization methods often fail at lower precisions. one-shot quantizationL need a mini-batch of data for quantization. more accurate, such as GPTQ, which optimizes the rounding during quantization via a mini-batch of data. But they are also more complex and may require hours of optimization before a model can be used. LLM.int8() In this quantization paper, the author discovered an emergent outlier feature in tranformers that totally wreck quantization. They also plotted a scaling law for this emergent outlier feature. By doing so, he proposed the LLM.int8() no-performance-degradation quantization method. I didn’t read the paper, but looked at the author’s blog post instead. How Quantization works First, how do we quantize a number? Imagine the following example: you have a data type I5 with values [0, 1, 2, 3, 4, 5] and a data type I3 with values [0, 2, 4]. We want to quantize I5 vector [3, 1, 2, 3] to I3: map from original domain to unit domain [-1, 1] find absolute maximum 3 = max(abs([3, 1, 2, 3])), divide the vector by 3 to get [1.0, 0.33, 0.66, 1.0] map from unit domain [-1, 1] to quantized domain Multiply by the range of the target data type I3, which is 4: [1.0, 0.33, 0.66, 1.0] -&gt; [4.0, 1.33, 2.66, 4.0] round to the nearest representable number in this quantized domain [4.0, 1.33, 2.66, 4.0] -&gt; [4, 0, 2, 4] To dequantize, we reverse this process: map from quantized domain to unit domain [-1, 1] Divide the vector by range 4 to get [1.0, 0.0, 0.5, 1.0] map from unit domain [-1, 1] to original domain Multiply by the stored absolute maximum 3: [1.0, 0.0, 0.5, 1.0] -&gt; [3.0, 0.0, 1.5, 3.0] round to the nearest representable number in the original domain [3.0, 0.0, 1.5, 3.0] -&gt; [3, 0, 2, 3] We see that our dequantization and quantization led to one error at the second element. Emergent Outlier Features Since we are using the absolute, it is obvious that if we have an outlier, there will be more errors. So the authors go on to discover the distribution of outliers. They call such outliers emergent outlier features. The authors explain such outlier features as to select only a single feature. At the same time, the other small value part brings the unimportant values down. The authors also found this very interesting emergent phenomenom, which I directly quote below: However, this full “coordination” through a single dimension only happens after the phase shift. Before the phase shift, in transformers with less than 6.7B parameters some layers disagree which dimension to use for these large features (no prominent outliers). … The phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers. At this point, a couple of things happen rapidly: Transformers become more stable. If you treat the outlier features separately, I believe you can probably run and even train transformers in less than 8-bit precision without degradation in performance. Note the model perplexity rather than mere model size determines the phase shift. GGML MLabonne did a great explanation to how GGML did quantization. GGML quantizes weights in a rather naive way. Basically, it groups blocks of values and rounds them to a lower precision. Some techniques, like Q4_K_M and Q5_K_M, implement a higher precision for critical layers. In this case, every weight is stored in 4-bit precision, with the exception of half of the attention.wv and feed_forward.w2 tensors. Experimentally, this mixed precision proves to be a good tradeoff between accuracy and resource usage. If we look into the ggml-quants.h file, we can see how the blocks are defined. For example, the block_q4_0 structure is defined as: 12345#define QK4_0 32typedef struct &#123; ggml_fp16_t d; // delta uint8_t qs[QK4_0 / 2]; // nibbles / quants&#125; block_q4_0; In GGML, weights are processed in blocks, each consisting of 32 values. For each block, a scale factor (delta) is derived from the largest weight value. All weights in the block are then scaled, quantized, and packed efficiently for storage (nibbles). Oobabooga, the author of text generation webui, did an in-depth survey of inference time, model size, vRAM usage on different types of quantization formats (GPTQ, GGUF, EXL2, …)","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"Fine-Tuning LLMs: Prompt Tuning, Adapter, LoRA","slug":"2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA","date":"2023-11-20T05:00:00.000Z","updated":"2025-09-03T01:42:46.391Z","comments":true,"path":"2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/","permalink":"https://yao-lirong.github.io/blog/2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/","excerpt":"This article A Guid to Parameter-efficient Fine-tuning (PEFT) made a very good summary with nice drawings. There are some differences between its explanation with the original paper but the basic architecture is all good.","text":"This article A Guid to Parameter-efficient Fine-tuning (PEFT) made a very good summary with nice drawings. There are some differences between its explanation with the original paper but the basic architecture is all good. Prompt Tuning Prefix tuning, prompt tuning, and p-tuning all prepend some vectors as prefixes / soft prompts to the vector inputs to transformers. Their goal is to find a context that steers the language model toward generating text that solves a particular task. Adapter Before Adapter, when performing vanilla fine-tuning, a modification is made to the top layer of the network because the label spaces and losses for the upstream and downstream tasks differ. Now, Adapter modules perform more general architectural modifications to re-purpose a pretrained network for a downstream task: it injects new layers into the original network. In standard fine-tuning, the new top-layer and the original weights are co-trained. In contrast, in Adapter tuning, the parameters of the original network are frozen and therefore may be shared by many tasks. Adapter Architecture Left: We add the adapter module twice to each Transformer layer: after the projection following multiheaded attention and after the two feed-forward layers. Right: The adapter consists of a bottleneck which contains few parameters relative to the attention and feedforward layers in the original model. The adapter also contains a skip-connection. During adapter tuning, the green layers are trained on the downstream data, this includes the adapter, the layer normalization parameters, and the final classification layer Therefore, we can denote the Adapter layer as: y = B(σ(Ax)) + x Define bottleneck dimension r (to be consistent with LoRA, in the original Adapter paper, this was m), so $A \\in \\R^{r \\times d}, B \\in \\R^{d \\times r}$. Including biases, we add a total 2dr + d + r parameters with r ≪ d. In initialization, we initialize the adapters to a near-identity function, so original network is unaffected when training starts. Adapter achieves similar results with only 1% needed parameters as compared to full fine-tuning. LoRA: Low-Rank Adaptation of LLM Before LoRA, SOTA techniques have some drawbacks: Adapter Layers Introduce Inference Latency: Adapter layers have few parameters, but “large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one.” I actually don’t understand how to parallelize an LLM inference even if without Adapter Directly Optimizing the Prompt is Hard: Prompt tuning and prefix tuning both require adding a prefix (to either the input vector or to the hidden vector in middle). In this way, it “reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods.” Its performance changes non-monotonically in trainable parameters, too. So it’s hard to optimize. LoRA’s architecture is simply a matrix multiplication - Adapter without non-linearity or skip connection. So instead of y = B(σ(Ax)) + x, we do Δy = BAx. There is one fundamental difference though: Adapter is an extra layer added into the original network, but LoRA is a layer added along side with the original network. That’s why I have a delta in LoRA’s formula. In fact, LoRA was specifically designed for low-rank matrix multiplication decomposition, note for any pretrained weight $W_0 \\in \\R^{d \\times k}$ with y = W0x and we want to update this weight matrix in fine-tuning: W = W0 + ΔW, we define ΔW = BA, so W = W0 + BA. This simple design yields a unimaginable good result: note matrix BA is also of dimension $\\R^{d \\times k}$. Therefore, we can directly add this result matrix to the original matrix and inferencing with LoRA, when we add this matrix in, gives us no additional inference latency. Similar to Adapter, LoRA uses a random Gaussian initialization for A and zero for B, so ΔWx = BAx is zero at the beginning of training and W = W0 + ΔW yields the same result (identity) as before. Thanks to the matrix decomposition nature, we can apply LoRA to any matrix multiplication in principle. However, we found that in transformers, imagine if we have 8 ranks to distribute, when r(Wq) = r(Wv) = 4 or r(Wq) = r(Wk) = r(Wv) = r(Wo) = 2 gives best result. The author also found that the fine-tune matrix actually has a very low rank, so in practice even if we set r = 1 can give good enough results. Go to Section 7 for more interesting experiments they conducted. In diffusion models, we use LoRA on the stable diffusion’s cross attention layer.","categories":[],"tags":[]},{"title":"Graph Networks & GraphCast","slug":"2023-11-16-Graph-Networks-&-GraphCast","date":"2023-11-16T05:00:00.000Z","updated":"2025-09-03T01:42:46.384Z","comments":true,"path":"2023-11-16-Graph-Networks-&-GraphCast/","permalink":"https://yao-lirong.github.io/blog/2023-11-16-Graph-Networks-&-GraphCast/","excerpt":"","text":"Graph Networks This is a very detailed and clear intro to Graph Networks by Deepmind. Graph Definition (Box 3 &amp; 3.2.1) We define a graph to have node, edge, and global attributes. Global attribute, u, for example can be the gravitational field in a three body problem setting. Global attribute is there to give a chance to any local node / edge to know what’s happening in a global perspective (mostly places far awy from it). If we exclude the global u (which aggregates information from across the nodes and edges), the information that a node has access to after m steps of propagation is determined by the set of nodes and edges that are at most m hops away (Figure 7). This can be interpreted as breaking down a complex computation into smaller elementary steps. Graph Update (3.2.2 &amp; 3.2.3 &amp; 4.2) Graph Network Algorithm The whole GN consists of the following 6 functions: $$ \\begin{array}{l} {\\mathbf{e}_{k}^{\\prime}=\\phi^{e}\\left(\\mathbf{e}_{k},\\mathbf{v}_{r_{k}},\\mathbf{v}_{s_{k}},\\mathbf{u}\\right)}\\\\ {\\mathbf{v}_{i}^{\\prime}=\\phi^{v}\\left({{\\bar{\\mathbf{e}}}}_{i}^{\\prime},\\mathbf{v}_{i},\\mathbf{u}\\right)}\\\\ {\\mathbf{u}^{\\prime}=\\phi^{u}\\left({\\bar{\\mathbf{e}}}^{\\prime},{\\bar{\\mathbf{v}}}^{\\prime},\\mathbf{u}\\right)} \\end{array} \\begin{array}{l} {\\bar{{\\mathbf{e}}}_{i}^{\\prime}=\\rho^{e\\to v}\\left(E_{i}^{\\prime}\\right)}\\\\ {\\bar{{\\mathbf{e}}}^{\\prime}=\\rho^{e\\to u}\\left(E^{\\prime}\\right)}\\\\ {\\bar{{\\mathbf{v}}}^{\\prime}=\\rho^{v\\to u}\\left(V^{\\prime}\\right)} \\end{array} $$ ϕe is an edge specific function and is applied to every edge, same for node and global. ϕ can be any function, people usually use Neural Networks. On the other hand, ρ takes in a set as input, so needs to be an aggregate function invariant to permutations, like sum, mean, max, min, etc. These functions are not affected by the order they are sent in. Composing Multi Graph Block Architecture (4.3) A GN is simply three aggregate functions and three update functions for node, edge, global respectively. It always takes in a graph comprised of edge, node, and global attributes as input, and returning a graph comprised of edge, node, and global attribute. The GN operates on any graph without caring about size of the graph, so it is extremely easy to stack multiple GN block together. Figure 6 shows 3 commons ways of stacking GN blocks. Graph Networks Advantage (3.2.4) First, graphs can express arbitrary relationships among entities, so the way input interacts is not limited to a fixed model architecture. Graphs represent entities and their relations as sets, which are invariant to permutations. However, we can still impose ordering by encoding the indices in the node or edge attribute. A GN’s per-edge and per-node functions are reused across all edges and nodes, respectively. Therefore, GN can easily operate on graphs of any size. Limitations (5.2) Notions like recursion, control flow, and conditional iteration are not straightforward to represent with graphs. GraphCast GraphCast by Deepmind is a perfect example to understand encode-process-decode GN structure and it is clearly written. Note the model architecture is inside the supplementary material. The following notes mostly come from Section3 GraphCast model. They defined two space for this task. One on the grid space based on earth’s latitude and logitude, where the atmospheric information comes from. The other on a “mesh space” based on a regular sphere, where the actual processing happens. The design was made on the observation that “Using the latitude-longitude grid is not an advisable representation due to its spatial inhomogeneity, and high resolution at the poles which demands disproportionate compute resources.” Mesh Space As for the mesh space, it is not only 1 mesh space, but 7 mesh space layered together. The first mesh space (M0) is a regular icosahefron (12 nodes and 20 faces). The second (M1) is the first layer divided 6 times. The third (M2) is the second divided 6 times and so forth. The M0 layer is the base and the coarsest one. Among all layers, It has the fewest points and the greatest distance between each point. On the other hand, M6 layer has the finest resolution, most points, and each point is only a few units away from each other. When we update a particular point on the net, we look at each layer: M0, M1, …, M6. For each layer Mi, we look at this point’s neighbor on this mesh and use these edges to update this point’s value. On M0, the neighbors are far away, so you get information from away. On M6, the neighbors are very close, so you get information from close points. This is exactly what depicted on paper’s Fig1 e) - every point has input from different distance. What’s most important is that since the finer resolution layer is obtained by dividing the coarser layer, every point and edge on the coarser layer can be directly found on the finer layer. Therefore, in memory we only have to store a single layer M6. You might ask what if a point on M6 is not originally on M0? Will it still have neighbors on M0 properly defined? Yes (I think), because you can think of the mesh construction process in reverse: if you start with an arbitrary point on the finest mesh and make the resolution coarser, you will always be able to get back to some regular icosahefron with the same strcuture as the M0 we started with (though not the exact same one because this point you started with might indeed not be on the M0 we built the multi-mesh upon). The encoder is a graph mapping grid nodes to mesh nodes. The processer happens on mesh nodes only. The decoder maps mesh nodes back to the grid nodes.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"}]},{"title":"First Time Debugging with ChatGPT","slug":"2023-04-04-First-Time-Debugging-with-ChatGPT","date":"2023-04-04T04:00:00.000Z","updated":"2025-09-03T01:42:46.391Z","comments":true,"path":"2023-04-04-First-Time-Debugging-with-ChatGPT/","permalink":"https://yao-lirong.github.io/blog/2023-04-04-First-Time-Debugging-with-ChatGPT/","excerpt":"I was trying out the sampling in MMM music generation model today and encountered the problem described in this issue I proposed. I have no experience writing C in python with ctypes, so I figured why not ask the magic conch shell ChatGPT?","text":"I was trying out the sampling in MMM music generation model today and encountered the problem described in this issue I proposed. I have no experience writing C in python with ctypes, so I figured why not ask the magic conch shell ChatGPT? I’ve asked him several “how to write …” questions since its release, but this was the first time I actually ask him to help me understand a snippet of code so I can proceed to debugging. It did pretty good in my first question, as it should do as the SOTA LLM model. q0 a0 What strikes me is the context-aware ability it showed in my second question. It is well known it can do so from all the demos, but seeing it actually work with a real example of your own is really a different story. Here, I just asked an absolutely arbitrary question and it knew what I was referring to q1 a1 It became mostly clear to me where the bug was, but to make sure q2 a2 q3 a3 At last, I conveniently asked it how to fix this bug q4 a4","categories":[],"tags":[{"name":"Journal","slug":"Journal","permalink":"https://yao-lirong.github.io/blog/tags/Journal/"}]},{"title":"2022 Web Journal","slug":"2022-12-31-2022-网络日志","date":"2022-12-31T05:00:00.000Z","updated":"2024-01-07T06:45:42.749Z","comments":true,"path":"2022-12-31-2022-网络日志/","permalink":"https://yao-lirong.github.io/blog/2022-12-31-2022-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/","excerpt":"","text":"“石老人”坍塌 因常年风化、海水侵蚀，加上当天的风雨雷击，山东省青岛市著名地标“石老人”海蚀柱坍塌，官方称已采取临时性保护措施，修复方案正在论证。 Tech - Tools VSCode Users can now type @lang:languageId in the Settings editor search box to view and edit all settings that can be configured for the language with ID languageId. This way, users can view language-specific settings, also known as language overrides. Keyboard shortcuts: ctrl + pgup and ctrl + pgdn to navigate between opened tabs ctrl + \\ go to pairing bracket (self-defined) Enable switching between terminal and editor 12&#123; &quot;key&quot;: &quot;ctrl+`&quot;, &quot;command&quot;: &quot;workbench.action.terminal.focus&quot;&#125;,&#123; &quot;key&quot;: &quot;ctrl+`&quot;, &quot;command&quot;: &quot;workbench.action.focusActiveEditorGroup&quot;, &quot;when&quot;: &quot;terminalFocus&quot;&#125; To jump by function with ctrl + uparrow and ctrl + downarrow, install extension “Go to Next/Previous Member” and change its setting to 12 &quot;gotoNextPreviousMember.symbolKinds&quot;: [&quot;interface&quot;, &quot;class&quot;, &quot;function&quot;, &quot;event&quot;, &quot;method&quot;, &quot;module&quot;, &quot;object&quot;, &quot;struct&quot;] Others A powerful spell that manipulates file names in shell: we want to extract the task ids from html files, which have this format like zz688732.html. Based on the task id, we construct filenames for output files, which have the format zult_688732.out. Finally, we move all these output files to directory results. 1ls zz*.html | grep 688 | sed -E &#x27;s/zz([0-9]+)\\.html/zult_\\1.out/&#x27; | xargs -n 1 -I &#x27;&#123;&#125;&#x27; mv &#x27;&#123;&#125;&#x27; ./results/ Find Out What Framework a Website Uses Markdown 树形目录可视化生成器 Draw your Latex Symbol: draw the symbol you are looking for into the square area above and the program will find all corresponding LaTeX commands. OneMark - Markdown in Onenote nbconvert: jupyter notebook built-in converter. It can convert ipynb file to html/markdown/LaTeX format and so on. 1jupyter nbconvert --to markdown lecture2.ipynb Solve Firefox reports “Secure Connection Failed” when accessing YouTube: Kaspersky doing its job I guess Tech - Knowledge 中文档案编写排雷指南: 讲解了引号，空格，全角半角符号等使用规范 Browser in the Browser Phishing Attack: The author introduces how to simulate a pop-up login window to maliciously acquire user’s password. When use GitHub, use https to track remote branch. SSH works weird when you have multiple accounts on your local machine. Basic and extended regular expression: In short, ERE syntax is the regex that we are more familiar with, where where + ? () have special meanings. Hiding a photo inside another photo: by changing the least significant bit of every pixel point of the original photo to the photo we want to hide. When we reconstruct, simply extract all these least significant bits and put them back into a picture. This naive approach only works with square photos for now as it can’t store metadata. It also becomes incomprehensible if go through compression like jpeg. Tech - Advances Microsoft Azure Text-to-Speech Engine: really amazing stuff, indistinguishable from real human voice The BRAIN Initiative: almost a whole brain simulation effort? Get Codex to Produce the Code You Want with Prompt Engineering: Software 3.0 is to design the prompt? FDA gives safety nod to ‘no kill’ meat, bringing it closer to sale in the U.S.: Scientists “grow” meat: they extract cells from an animal, place them in tanks, feed them the nutrients, let them divide and reproduce, and end up with meat. Ruanyifeng 259 Glorious To View 康奈尔天下第一 Glorious to View Where do Famous Cornell Alumni Live? Brain Collection Preserves Figures in History: Now on Uris 2nd floor. The most fascinating one is the Edward Rulloff’ brain - a notorious murderer but claimed to have the 2nd largest brain ever found in human history Some very familiar melodies: Alma Mater, The Evening Song Virtual Tour to Cornell Teaching Dairy Barn, found in this CALS page Alright, so where does “Cornell Note Taking” come from? Here’s a post from Cornell Learning Strategy Center on “Cornell Note Taking” - it was really a thing. Cosmos, Johnson Museum of Art: my favorite view on campus Cornell Sesquicentennial Grove: never realized these benches have a name 1865 Society Wallpaper, Cornell Logo Digital Downloads, Schedule at Tour at Cornell’s Particle Accelerator Haunted Stories at Cornell - A Halloween Video: talks about Morrill Hall dissection for Zoology, pumpkin mystery, sleeping founders at Sage Chapel, and other interesting ghost stories. #1 Subreddit Can somehow explain how meal plans aren’t an absolute scam? Cornell kids doing the math :cloud_with_lightning: lightning strike on hill, context here 电来！ Daily Crime Log: try check this when you’re really bored Dexter responded to a reddit post: both sides had a point To Do the Greatest Good 3D-printing robot enables sustainable construction: Random story reminds me of von Neumann’s plan of using self-reproducing automata to colonize moon Smart thermostats inadvertently strain electric power grids Request a Purchase The following are the books I requested Cornell to purchase. 房思琪的初戀樂園 占星术杀人魔法 首无·作崇之物 三岔口 Talks 一席 YiXi 李涵：少年的你 7:00：「问题少年」的社会工作 我认为降低刑事责任年龄表达了一种需要，这种需要是独立的少年司法体系的需要，而并不仅仅是真的降低就能解决的。我们把刑事责任年龄降低到10岁又怎么样呢？不会有9岁的未成年人实施违法犯罪行为吗？ 胡立德（David Hu）：浪費科學家：类似原来订阅的小哥白尼杂志里面各种乱七八糟的研究，但实际上这可能才是真正的「生物学家」 现在大多数的生物学家都在研究细胞和基因，真正研究动物的人越来越少了 何袜皮：小區保安，恐懼和被恐懼的：中国的小区为什么需要保安，保安到底在干什么，以及他们是什么样的人 我们说起恐惧感，都会认为这是一个非常消极的、负面的、被动的情绪，大家都避之不及，但是从理论上讲它其实也可能是一种权利，它甚至有时候是有利于某些群体内部的团结的。 德国社会学家贝克就说，在一个对上帝、阶级、国家和进步的信任和信仰已基本消失的年代，共同恐惧已经被证明是人类仅剩的能通过矛盾来制造新联系的资源了。 大家都知道中产阶级形成的时间很短，他们之间共享的东西很少，恐惧感是少数他们能够共享的东西之一，甚至可以称之为文化资本。业主不太会说我想要更多保安是因为我想抬高房价，我想体现我的身份地位，业主只会说我想更多保安是因为我觉得不安全。 他们其实是把自己放在了一个脆弱的恐惧的担忧的角色当中，去合理化这样一种炫耀性消费的需求。但这种需求并不是真正源自于对小概率犯罪的一个恐惧，很大程度上它是源于在今天这样一个高速发展的时代，他对自身的经济状况和社会地位的没有安全感。 我如果跟业主坐下来聊恐惧的话，我们可以聊很长时间，他们会跟我讲，他们会很害怕空气污染、食品安全、社区犯罪、小孩拐卖、转基因等等。如果我去问一个保安，你害怕什么？我通常是得不到答案的。 保安们似乎从来没有准备好要被问这样一个问题，也就是默认了他们是没有资格恐惧的。但是每个人都有恐惧，恐惧是人性的一部分，如果你否认了一个人的恐惧，就相当于否认了这个人的人性。 声东击西 能影响一亿人的 1 秒又怎样？ 很有洞见的资深内容创作者谈话。虽然从技术自由角度我不喜欢他的大部分观点，但是有一点非常可取，就是他提出我们现在单纯的「流量」，即观看时长来衡量内容价值太片面；需要用立体的 观看量 × 观看时长 的计量单位取代 群众基础和通往夺冠之路 讲中国女足或者更广泛到足球以及整个大体育的问题。15:38 左右谈到我们现在的体育发展是一个恶性的内循环：训练很多体育生，但是最后能靠它吃饭的只有一小部分人，于是大多数人投入了人生的黄金时间无法进入国家队成为职业运动员，只能最终去做教练或者体育管理的相关职业，转型是很难的，最终再由这些人培养新的体育生。可见如果选择走体育，你的人生道路并不是越走越宽，而是越走越窄的。 我们习以为常的和平为何难以降临阿富汗？ 后发国家通过援助发展，首先都是外部世界（发达国家）拉动一个城市，但最终结果不尽相同：1.城市成功拉动乡村：如韩国城市人口占总人口一半，常发生在规模小的国家 2. 乡村拉平城市：如国土面积相比来说巨大的越南「西贡时刻」，城市更像是一块悬浮在落后乡村之上的先进飞地，没有力量拉动乡村，最后北越将南越拉了下来 伯内斯和操纵大众情绪的手 We are the World 明天会更好 “双层”卧铺动车组春运首秀: 太炫了 咱的铁路文化在资源允许的情况下要是能打日本就太好了 《咬文嚼字》历年十大流行语: 突然想起来「至于你信不信，我反正信了。」这句名言，查了一下后咬文嚼字原来也是权威且寓教于乐的一本小月刊，从历年十大流行语可以追忆下往昔。原链接已404。archive Doomsday Clock 末日时钟 葡萄采摘和樱花盛开日期证明全球变暖 African penguins endangered by shipping noise in Algoa Bay 中文版:因航运活动噪音 非洲企鹅濒临灭绝 “Hunger Stone” Emerges With Grim Warning, last time a hunger stone emerges was just 4 years ago back in 2018 Critics and Fans Have Never Disagreed More About Movies: especially in those featuring Black people and women as the main characters. Woops :hushed: Cyberpunk 2077 太赛博了 Goodbye V, And Never Stop Fighting 中国网民规模超10亿 实际是2021年的新闻，第一次看到的时候想得不是增长得好快，而是还有4亿人不使用互联网，他们在这个去哪都要健康码的时候怎么生存呢？ Microsoft is testing ads in the Windows 11 File Explorer GitHub suspends accounts of Russian devs at sanctioned companies, original blog post mentioned here In parallel with our efforts to make sure GitHub is available to developers in all countries, we are continuing to ensure free open source services are available to all, including developers in Russia 悄悄拍摄行人，算法指挥员工：便利蜂的“系统”是否越界 Backup Archive 这是一场颇具科幻意味的试验，一家名为便利蜂的连锁经营便利店正在研发一套算法，从选址、订货、物流、陈列，甚至打扫卫生，都交给“系统”决策。 “系统”是便利蜂的大脑，通过一台电脑向店员们发号施令，每个任务都附有极为细致的标准操作规范。店里有摄像头全方位无死角覆盖，它们是系统的眼睛，实时监督着店员们。通过人工和AI自动识别店内的画面，一旦不符合要求，便会自动报警。 顾客们的一举一动也会被收集起来，成为系统的决策依据。让人不安的是，这样的摄像头从店内延伸到了店外。为了建立一套智能化的选址算法，便利蜂正在悄悄进行一场大规模的公共图像采集。 自2017年起，在长达五年的时间里，便利蜂招募大量信息采集员将摄像头悄悄地放在了多个城市的居民楼、写字楼，甚至国家机关的门口，完整捕捉下当天出入的每一个人。每一个便利蜂的目标店铺附近，都会遭到一轮录像。便利蜂后台至少累积了数十万条拍摄数据。 科技爱好者周刊（第 199 期）：俄罗斯的 HTTPS 证书问题 夫风者，天地之气，溥畅而至，不择贵贱高下而加焉。 它的意思是，天地间的风，无差别地吹拂，不会因为贵贱高下，而有所不同。 我一直认为，互联网是中立的技术基础设置，应该像风一样，无差别地吹拂到每个人，不应该区分穷人、富人、坏人、好人，人人都有权使用互联网。 一旦因为政治原因吊销 HTTPS 证书，或者让俄罗斯断网，互联网的中立性就荡然无存，不再是人人都可以使用的技术基础设施了。这等于把”互联网武器化”，只要你是我眼中的恶棍，我就不同意你使用互联网。 开了这种危险的先例，把互联网当作武器，互联网从此就变成了国防设施。国与国之间都搞自己的证书、自己的域名、自己的国际网关，严格区分网络国境线。互联网创立时的开放、自由、统一、造福人类的梦想，灰飞烟灭。 Why Vivaldi will never create ThinkCoin When you strip away the hype, these virtual currencies have very real repercussions for people, society, and the environment. By creating our own cryptocurrency or supporting cryptocurrency-related features in the browser, we would be helping our users to participate in what is at best a gamble and at worst a scam. It would be unethical, plain and simple. Noisy: Random DNS, HTTP/S traffic noise generator - Y Combinator first he warned us: make sure you always use the -c flag if you’re pinging something on the internet. This is to specify the count of pings sent out. If you didn’t it would ping forever and generate too much traffic, and this useless noise would make you a “bad netizen”. He explained this and everything to us so kindly and with such sincerity it was like watching Fred Rodgers speak. Now we have to randomly barf noise onto the network to maybe have a better chance at some privacy. How did we let the internet become this awful? Musk Buying Twitter Is Not About Freedom of Speech Yes, Twitter will wind up with different rules, results and outcomes—and it may be the better or worse for it. Along the way, some people will cheer, and others will jeer. But framing the discussion as a “free speech” issue is entirely disingenuous. This is simply a billionaire attempting to etch his world view into an algorithm—even if he brands himself a swashbuckling digital freedom fighter. 王局专访围棋国手柯洁: 偶然YouTube推送的视频，20分钟充满着绝望的感觉。阐述自己悲观的原因时他说机器学一个月可能敢上一个人学十年，其实这对于很多人在平常的学习工作中可能都有一样的感觉：天才学半年顶你四五年。柯洁在以前的学习中必然也遇到过这种情况，总发现有比自己聪明或者努力的，但自己再努力超过他们就行了，毕竟顶尖的人智力差别都不大。但这次更像孙悟空和如来佛一样，上面有一个被生物规律严格规定的不可超越的存在，一个筋斗十万八千里也翻不出如来的五指，就算你再怎么精进自己，即使到百万里千万里也不可能，这才是最绝望的事。 我们职业棋手是非常大的震撼的。因为那个下法从来没有出现过，过去觉得这是要被老师教训的。有的下法我们想都没敢去想过，是它自己创造的棋。这种感觉其实是对方是一个特别高大的巨人，哪怕你装再多的武器，蚂蚁哪能撼动大象，真的是动不了。我当时觉得，人类真是可悲啊，就这样被自己创造的东西击败了。怎么都赢不了这个东西的存在。 Is Social Media Training Us to Please a Machine? Though most of the article is cliche, this idea itself is very interesting. Content generators were used to please editors or reviewers, but now they have to please an algorithm. 中文版：社交媒体是否在训练我们取悦机器？ 平台：现代社会的遥远巫术: 「平台」，算法黑箱，无形的大手。很有意思的一个小时谈话，与上文的取悦算法有些相关 Did the Pandemic Normalize Employee-Monitoring Software? 论文工厂狂造假 800 多篇材料学论文: 剑桥晶体学数据中心 的结构数据库发现，近 1000 个晶体数据结构条目（涉及810篇文章）涉嫌来自中国的论文工厂。因为中国医院要求，医生只有发表论文才能评职称，结果就产生了这些不存在的小分子结构。 深圳某公司在每个工位都装上了监控 Give Up GitHub: The Time Has Come! Software Freedom Conservancy in response to GitHub announcing Copilot, which trained on FOSS software’s code, as a commercial for-profit product. The ever-expanding job of preserving the internet’s backpages: Internet Archive is on the brink of surpassing 100 petabytes and it’s only getting harder to archive a webpage. Bypass paywall at archive.today Chinese Company Pirated VSCode: a very interesting one to read Wizz Air Charges Extra for Users with Ad-Blockers Danish Political Party Led by an AI Tim Cook says ‘buy your mom an iPhone’ if you want RCS support in iMessage: 何不食肉糜？ The people making money from just surfing the internet: Browser extension that sells your data directly to retail brands instead of Google. It gives you money in return. Northeastern University installed heat sensors under graduate student workers’ desks: Northeaster be really have some thought here Generation Digital 混迹于互联网 虚拟身份生成 Generate a Random Name - 随机身份生成 Fake Address, Random Address Generator - 随机身份生成 Behind the Name - Random Name Generator Easy Random Name Picker - Random Name Generator ElfQrin - Fake Identity ID Random Name Generator Random User Generator 在线身份证号码生成器 中国大陆内地姓名、身份证号、银行卡号生成器 在线身份证号码生成器 airob0t/idcardgenerator 身份证图片生成工具 gh0stkey/RGPerson - 随机身份生成脚本 naozibuhao/idcard - 身份证生成器 Just Delete Me - 假身份生成器(这个网站的图标,好像在哪里看过🤔) Fake Person/Name Generator | User Identity, Account and Profile Generator faker.js Fake Person/Name Generator Full Contact Information Generator My Fake Information Generator and Validator User Information Generator Articles 图片生成 伪造人像 Artbreeder Comixify This Waifu Does Not Exist - Gwern 虚拟猫咪 Which Face is Real? SPADE Project Page Selfie2Anime Reflect.tech Gallery of AI Generated Faces | Generated.photos ピクセルミー | ドット絵ジェネレーター PaddleGAN 人像生成、编辑、融合、动作迁移 国产流氓软件必须使用时的解决方案: Windows 7 Ultimate SP1 7601（老毛子のlopatkin 改装的 Windows 7 SP1 企业版简体中文精简版） 实装 TIM+微信后消耗 1G 内存，分配 1.5G 替代 Typora: Typora于今年七月开始强制所有用户收费，虽然想到了禁止联网这种方法但是无效，后来才知道是它早前在注册表里安了一个日期，所以不管我删掉 Appdata\\Roaming 里的数据或者是禁网都没用。于是开始找替代的 markdown editor: marktext: 几乎完美的替代品，但是对于 front matter 和一些其他语法的编辑很奇怪，无法做到所见即所得。开发者对于这个项目几乎是放养的状态，open issue上千，所以我觉得这问题永远也解决不了了，不过也是大 FOSS 软件的通病。还有一个小问题是这软件又是 electron 架构的，太恶心人了。 abricotine: 更轻量的替代品，就是渲染风格比较简陋不太喜欢 Zettlr: 比起编辑更侧重于笔记整理，最大好处是可以自定义新建文件的文件名保存格式。因为是笔记整理软件所以也更重一点，体积比 marktext 小，内存占用三个最大。 最终绕了一圈用的还是 Typora 的学习版，其实不想这样做的，主要是电脑上大部分 md 文件已经保存为 Typora 默认的 %date-%title.md 的格式，再更改太麻烦。 Download Abema TV Videos InControl: Easily manage Windows 10 and 11 out-of-control updating and upgrading Windows 11 Classic Context Menu v1.1 Privacy Redirect: A web extension that redirects Twitter, YouTube, Instagram, Google Maps, Reddit, Google Search &amp; Google Translate requests to privacy friendly alternative frontends Copy Files: use teracopy for stability, use fastcopy for speed. Also teracopy can directly replaces windows copy hotkeys &amp; drag drop, so use teracopy in general. Quote How to Stop Worrying and Learn to Love the Internet, from Douglas Adams, also collected in The Salmon of Doubt everything that’s already in the world when you’re born is just normal anything that gets invented between then and before you turn thirty is incredibly exciting and creative and with any luck you can make a career out of it anything that gets invented after you’re thirty is against the natural order of things and the beginning of the end of civilization as we know it until it’s been around for about ten years when it gradually turns out to be alright really. David Hilbert’s Radio Address For us there is no ignorabimus. Wir müssen wissen, Wir werden wissen. Are you optimistic about the free software movement? - Richard Stallman I am a pessimist by nature. Many people can only keep on fighting when they expect to win. I’m not like that, I always expect to lose. I fight anyway, and sometimes I win. Remarks by President Biden Before Meeting with the White House Competition Council - Joe Biden, 46th President of the United States No, it’s a great asset. More inflation. What a stupid son of a bitch. A meeting with Enrico Fermi “There are two ways of doing calculations in theoretical physics”, he said. “One way, and this is the way I prefer, is to have a clear physical picture of the process that you are calculating. The other way is to have a precise and self-consistent mathematical formalism. You have neither.” In desperation I asked Fermi whether he was not impressed by the agreement between our calculated numbers and his measured numbers. He replied, “How many arbitrary parameters did you use for your calculations?” I thought for a moment about our cut-off procedures and said, “Four.” He said, “I remember my friend Johnny von Neumann used to say, with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” Your Map is Wrong, Mark Zuckerberg on Web 2.0 Summit (2010/11) Your map is wrong. The biggest part of the map has to be uncharted territory. This map makes it seem like it’s zero-sum, but it’s not. We’re building value, not just taking it away from someone else. the not yet lizard Zuck A friend who had just finished his PhD Don’t squander your ignorance. Once you learn something, you end up taking it for granted and it becomes so much harder to overcome your tacit knowledge and ask simple, but important, questions. Why Craigslist Still Looks the Same After 25+ Years: Solidot Because that serves people better. I’ve learned that people want stuff that is simple and fast and gets the job done. People don’t need fancy stuff. Sometimes you just want to get through the day. For me as an engineer, simple is beautiful. Functional is beautiful. Prospect 观天下 Why Chicago Is Lighting Its Railroads on Fire Emergent City Flyby with Colossus: just some arbitrary dots when see in still, but it becomes cities when see in motion. Ascent, world’s tallest timber building 嘟嘟宝贝 陈怡馨 Archive: 我各种 Archive 也看过一些，但是这么完整精美的真是头一次见。最难能可贵的是本网站的关于页和 GitHub 中 Readme 写得特别详细，作者一定也是个非常心细善良的人。（在写下上面一段文字后我手贱去看了一下，作者是个普林的高能物理博士，斯坦福的本科…行吧，果然是这种人……我该说惊讶呢还是毫不惊讶呢） Others 杂七杂八 《塞尔达传说时之笛》PC 移植版准备下个月发布: 完整的时之笛反编译 The rise of performative work - the Economist, more like College Classroom 101(Bypass paywall at http://archive.today/RJQQX) Satya Nadella, the boss of Microsoft, says that comments in chat help him to meet colleagues he would not otherwise hear from. Maybe so, but that is an irresistible incentive to pose questions that do not need answering and offer observations that are not worth making. 为语言多样性现象点灯之作——《〈役割語〉小辞典》读后感：从文字层面保护语音语调层面的「役割語」，或者说方言 以上海为主题的视频创作者G僧东在不少视频里提倡要认识上海方言里的正字，即那些普通话中不常听、不常见但频频出现在方言口头表述中的词所对应的汉字。我觉得这是一个有益的提议，找出了方言里的正字，便有助于探索其传播和演变的过程，及其所象征的人群形象。而这些考察就如同本书《〈役割語〉小辞典》所提到的那样，会反过来被运用到更多作品的创作中，形成一个个固定的角色形象用词，为语言的丰富和多样性注入活力。 胡萝卜周因一氧化碳中毒而意外身亡 一路走好 End of Life of Captura: 偶然找到的实用录屏软件，但是由于可耻的将开源软件套壳收费行为，作者选择了停止更新 王巍 - 萌娘百科: 什么绝对的六边形战士，果宝特攻猪猪侠的编剧导演配音曲词3D引擎开发动画制作，全你一个人啊？！","categories":[],"tags":[{"name":"Journal","slug":"Journal","permalink":"https://yao-lirong.github.io/blog/tags/Journal/"}]},{"title":"触乐 & RPG Codex: RPG文本写作讨论","slug":"2022-11-01-触乐-&-RPG-Codex-RPG文本写作讨论","date":"2022-11-01T04:00:00.000Z","updated":"2025-09-03T00:06:15.566Z","comments":true,"path":"2022-11-01-触乐-&-RPG-Codex-RPG文本写作讨论/","permalink":"https://yao-lirong.github.io/blog/2022-11-01-%E8%A7%A6%E4%B9%90-&-RPG-Codex-RPG%E6%96%87%E6%9C%AC%E5%86%99%E4%BD%9C%E8%AE%A8%E8%AE%BA/","excerpt":"本文是对触乐翻译的原载于RPG Codex的一篇文章的摘抄。作者主要喷了现代RPG存在的几个问题，篇幅过长触乐将其分为 1. 怎样避免“奇幻病”？ 2. 创作者的水平问题 3. 怎样才是合格的“游戏文本”？ 4. 余论和结论 四个部分","text":"本文是对触乐翻译的原载于RPG Codex的一篇文章的摘抄。作者主要喷了现代RPG存在的几个问题，篇幅过长触乐将其分为 1. 怎样避免“奇幻病”？ 2. 创作者的水平问题 3. 怎样才是合格的“游戏文本”？ 4. 余论和结论 四个部分 我也不知道为什么这种超长文我竟然从头看到尾还摘抄了，全文都是作者的主观观点，但是我竟然都很认同。原来是准备放在网络日志里面的，但是这个B是这能写，喷了好几张纸，导致摘抄篇幅也很长。作者主要讨论了如下几个问题，第三点媒介力量我是完全同意的，一二部分同意。 对游戏设定背景知识的过度展现以及过度描述: 过度详细的背景故事摧毁了故事的神秘感，剥夺了玩家的想象空间。 在旧三部曲电影中，每当达斯·维达谈及“星际魔法”时，他通常都会故意使用一些语义模糊的“大词”。比如他提及死星时会说，“没有东西能和原力相比”。为什么？“因为你不知道黑暗面的力量。”你需要知道的就是这么多了。“星际魔法”强大又神秘，非常牛×。这就够了。与之相反的是，在《星球大战前传1：幽灵的威胁》当中，原力这种神秘力量的运使水平最终得到了解释——它取决于一个人身体里纤原体数量的多少。那么，这个解释有必要么？有了这个解释之后，“星球大战”世界就变得更酷了么？ 并且有时作者可以使用场景动作等描述，但是却选择了文字，没有利用游戏这个媒介。 同样的矛盾还发生在那些兴致勃勃地跟你描述他们故乡的NPC身上。他们的故乡有无数让人心驰神往的地点，比如有水底古墓的湖区、火山喷发形成的群岛、气候严酷的苔原。这些地方要是我真的都能亲身到访，那不知该有多开心。然而我不能，我还是得跟着无聊的主线畅游乏味的奇幻世界，NPC的描述再怎么炫酷也与我无关。更惨的是，我已经很清楚这些炫酷的玩意一丝一毫都不会真正地出现在游戏可体验的内容里了。 开发者的知识和写作水平: 有现实基础的背景知识研究没做足就会影响到具体的故事情节，于是在编写这些情节的时候，作者们往往会想当然地加入一些他们自己的意见或议题，而不是去编写真正和游戏中的背景知识相符合的内容。这么一来，玩家在游戏里的一切所见所闻、所知所感似乎就都需要多一层（甚至多几层）源自现代社会知识的解释，无论这些见闻多么琐碎。 要描述一个青铜时代的社会中的邪恶力量，但你脑子里塞的却都是现代的观点和概念——最多也就上溯到19世纪。为啥你觉得这些观点和概念是自古以来就有的？你好歹也该去读一本关于古希腊历史或神话方面的入门手册吧？去看看那些真实的古代历史上曾经发生过的毫无人性的事件，这些事件做10部《暴君》也够了。 写手们在运使语言方面的短板：现如今，很多“精心打造的RPG大作”都会去撰写一些篇幅不短的、散文式的文段填充到游戏里。当你尝试往文段里塞入更多含义复杂的大词或尝试糅合进更多意象时，要保持文段行文风格一致和内容前后连贯就会更加困难。 突然间，一阵古怪的噪声在你共有的世界中响起，就像一口钟，如果这钟会腐烂的话。 （Suddenly, a grotesque noise rings through your shared worlds, like a bell if bells could rot.） ——想想最后这个小句，“如果这钟会腐烂的话”，然后想想这大概是个什么场面。这一小句到底表达了什么？钟的腐烂会对声音产生什么样的影响？会使声音变得更“古怪”？而且说回来，这个“古怪”所要表达的意思也相当模糊。这钟的声音会变得更高亢？更低沉？或是有些走样，被扭曲了？又或者像是被捂住了那样，变得低沉而含混？所以，到底应该是啥样呢？到这里我还没提这个钟要怎样腐烂的问题。当然，我觉得从修辞的角度来说，说这个钟“腐烂”也不是完全不能接受，那大概就是指金属制成的钟因锈蚀变得残缺不全。只是，如果说“这钟锈坏了”听着就非常没有感觉，尤其是没有那种超凡脱俗进入异世界的感觉。就跟前面提到的对纽蒙拉世界的空气进行描述的句式一样，“某样东西就像X一样，如果X可以Y的话”这个不知所谓的句式出现得实在太多了。 特别辣眼睛的句式还不光这一个，还有一个：“你感受到了某种……有些异样的东西。这是一种气味，如果你的情绪能被闻见的话。” 没能完全发挥电子游戏这一媒介的潜力: 别再畸形地重视文本了，文本是懒政，是最简单的最没有技术含量的解决方案。 更大的文字量不等于更好的游戏: 《战争与和平》共有约1890页，总共57万词。折磨：纽蒙拉之潮》的文字量则是《战争与和平》的两倍？！这些文本实际上充斥着如上的长难句 形式上模仿小说: 插进对话当中的描述性和感叹性的语句了，比如：「…，说着，他叹了口气。」为什么不让演员对着做相应动作呢？这些描述性的语句在书里是必要的，因为书里只有文字。但游戏有画面、有对话框，有比只包含文字的书籍多得多的表现手法，这些描述性语句还有什么用呢？ 所谓的“做得像一本小说一样”的RPG，其实只不过是对过往游戏的形式不动脑子的复制。它们并没有真的去思考，如果要把一个RPG做得像一本小说一样的话，应该花些什么样的工夫。正面例子: 《传说之下》的角色都拥有各自对话文本的专属字体，且不仅如此，他们说话的时候还有各种哔哔卟卟的声音作为对角色的标记。 制作者们明明可以用一些更妙的方式来叙事，比如语音、图像以及互动设计故布疑阵，肆意捉弄玩家。这样的可能性是电子游戏独有的，但大多数情况下，制作者竟然选择语音日志和连语音都不带的日志。而且这些日志写得根本不像日志，毕竟你真的在日常生活中写过日记么？ 配音演员的声音出演、精细的角色面部表情动画，以及在对话过程中角色们的动作都可以用来代替文字。诚然这是要花很多钱的，并且在这上头花钱就必然意味着减少对话的文字量以及出场角色的数量。但是，这可能是意见好事，逼迫你反过来把钱花在刀刃上，减少点角色，更注重每个人的塑造。 选择与后果其实应该通过更隐晦、微妙的方式呈现给玩家，而不是仅仅摆明车马，让玩家在对话框里点选。正面例子: 《耻辱》如果杀人太多，每个关卡里遭遇的警卫数量都变多了，而且NPC们也会谈论你四处杀人的盛举，关卡中的布景也会有所变化，而不是弹出一个巨大的“混乱度大幅上升！” 但最令我不爽的，还是玩家圈子里的虚伪。就我的观察来看，对RPG中写作的讨论通常是按照这么个轨迹来进行的。先是有人说：“这游戏的写作好棒啊，就像一本书一样。”然后有人说：“不，它实际上很烂。”再之后，又有人假装理客中说：“嗐，这不过就是游戏里的写作，游戏里的写作本来就不能跟书比，所以你在这杠个什么劲呢？”所以，这讨论产出了什么结果？游戏里的写作到底是真的烂，还是说换个评估的角度以后，它其实也有可取之处呢？ 如果游戏里的写作都很烂的话，那这个行业里那么多雇佣写手，是怎么一边持之以恒地产出华丽而无物的文字，一边还能美滋滋地把这个套路玩下去的？甚至很多时候，还不是玩下去这么简单，而是真的能得到很多人的喝彩。所以，他们是怎么能凭借这么垃圾的写作水准得到近乎摇滚明星的追捧的？他们又是怎么能继续接到这种创作性的工作，而不是去做点更适合他们的工作，比如去麦当劳翻肉饼的呢？ 答案显而易见。那就是很多人并没有深入地去观察和思考现代RPG中存在的问题，仅仅是把RPG当个快餐一样啃过去就算了。这么一来，他们自然不会抱怨。但那些真正的大师却不会为这种水准的写作唱赞歌，这就跟那些所谓的电子游戏记者和专家很不一样。这伙记者和专家可是能舔得很，但凡BioWare出了个啥游戏，他们一定会第一时间赶到现场，献上各种溢美之词，说这个游戏又带来了什么叙事上的革命性创新。他们这么干的原因也不难理解——他们跟那些写手根本就来自同一个烂透了的伪学术共同体。除了抱团取暖之外，另一个原因是，他们供职的游戏新闻网站从发行商那里拿到的大张支票。要拿到这些钱并不困难，只要他们能顺着发行商的意思说话，并且不带半点犹豫地给那些被推上市场的庸作打出满分就好了。 看起来这个业界完全就是由无能的人、自恋的人、见利忘义的人和满嘴火车的人构成的。 石黑正数《睡觉的笨蛋》","categories":[],"tags":[]},{"title":"Deploy a Reddit Bot on Heroku","slug":"2022-09-04-Deploy-a-Reddit-Bot-on-Heroku","date":"2022-09-04T04:00:00.000Z","updated":"2022-09-07T01:20:18.000Z","comments":true,"path":"2022-09-04-Deploy-a-Reddit-Bot-on-Heroku/","permalink":"https://yao-lirong.github.io/blog/2022-09-04-Deploy-a-Reddit-Bot-on-Heroku/","excerpt":"I’ve created a bot to send warm welcomes to newly admitted Cornell prefrosh.","text":"I’ve created a bot to send warm welcomes to newly admitted Cornell prefrosh. Simple Auto-Reply Bot This Let Me Google It For You Bot tutorial perfectly explains how to listen to reddit post stream and reply to a post. We use the following code to declare a reddit bot: 1234567reddit = praw.Reddit( client_id=&quot;CLIENT_ID&quot;, client_secret=&quot;CLIENT_SECRET&quot;, username=&quot;USERNAME&quot;, password=&quot;PASSWORD&quot;, user_agent=&quot;LMGTFY (by u/USERNAME)&quot;,) If we want to put our code onto GitHub, this will expose our client secret and account password. Therefore, we can make a separate file called praw.ini, where we specify these private information. Note an ini file cannot contain most special characters, so you need to change your password to only words and digits. 123456[bot527]client_id=d123071240924wclient_secret=D123412541254username=Harmonyanopassword=YouthinkIwillTellYouThisHuhuser_agent=bot With this, we can declare our bot with the following command. Note the customize name “bot527” need to match in both the ini file and the declaration . 1reddit = praw.Reddit(&quot;bot527&quot;) Deploy on Heroku Create a Heroku app, follow the instructions in Deploy/Heroku Git tab. For a Heroku app to run, it needs several additional files: Runtime.txt: Though it’s in question whether this is really needed or not (the app runs normally too without it?) 1python-3.7.9 Procfile: This is a Heroku specific file that declares what command Heroku should execute to start your app. A detailed explanation in here 1worker: python bot.py requirements.txt: python package requirements, obtained by pip freeze &gt; requirements.txt 1praw==7.6.0 Finally, after you deploying all the codes to Heroku, DON”T FORGET THE MOST IMPORTNAT PART: you need to turn on the resources tab on Heroku for your worker (spent 1h figuring this out). For where to find it, watch this video Submit &amp; Edit a Post Submit a Post onto a Subreddit: submission = reddit.subreddit(\"test\").submit(\"title\", selftext=text) Edit an Existed Post: submission.edit(body=submission.body + \"edited\")","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"How to Succeed in CS6784 (also in Academic Life in General)","slug":"2022-08-23-How-to-Succeed-in-CS6784-(also-in-Academic-Life-in-General)","date":"2022-08-23T04:00:00.000Z","updated":"2022-10-25T18:21:18.000Z","comments":true,"path":"2022-08-23-How-to-Succeed-in-CS6784-(also-in-Academic-Life-in-General)/","permalink":"https://yao-lirong.github.io/blog/2022-08-23-How-to-Succeed-in-CS6784-(also-in-Academic-Life-in-General)/","excerpt":"","text":"Write a Good Peer Review Paragraph 1/5: Summary Summarize the paper in your own words, no quote. Be concise. Boil it down to the very essence. Be neutral. Do not yet talk about problems, concerns. Paragraph 2/5: High Level Evaluation This is the place for general comments that are subjective in nature: Did you find the paper well-written? Did the idea strike you as incremental or highly original? What does it build on / are the original parts? Were there sufficient experiments? Is the paper likely to be of interest to many people or of great interest to a small community? Paragraph 3/5: High Level Technical Some possible topics to consider are: Were any important parts missing? Does the approach make sense? Any evaluations that should have been included? What part excited you the most? Don’t judge. For each criticism you have, give advice instead. Speak as if you are their advisors. Paragraph 4/5: Low Level Technical This is a place for very specific comments. It shows the authors you read the paper carefully. Most papers have at least one (small) mistake per page. For example: Any sections or equations that weren’t clear? Any figures that are hard to read? Captions of it given wrong? X tilde labeled incorrectly? Any mistakes with indices / equations? Paragraph 5/5: Review Summary This is a summary of your own review (not the paper). Do it in one or two sentences. Make Good Slides Overview Number of Slides: Allow roughly 1 minute per slide Sections: Problem statement, Approach, Results, Conclusion Dos and Don’ts Never change notation throughout a talk. Keep notation consistent across figures Avoid appearance of only an equation. When you have an equation, also include illustrations of it to explain how it behaves. If that’s not possible, include some intuitive explanations. Label each figure very clearly. Don’t use unfinished figures. Always attach semantic meaning to symbols and animation effects and be consistent through the talk. e.g. If you want to use arrow → to represent “reduce to”, then use it with this meaning through the talk. Never assume people remember previous slides. Call back to meanings of symbols and other background information. Don’t assume your audience pays attention at all. If there is something important to say, say it repeatedly. Hammer the point home. Make it crystal clear. Other Helpful Tips Each slide should have a single “take home” message. You should be able to verbalize that when asked. Try to stick to one running example. Explain every aspect of the paper in the same setting. (e.g. assume x is movies, y is movie ratings; x is images of faces, y is the age of the person; …) Use images, movies, etc (unlike this file) Don’t just do texts Write a Good Research Paper Layout Abstract: Summarize the paper in a few sentences (5-8) Introduction: Provide Motivation, explain why what you are doing is important Related Work: What have people done so far, what should the reader know about the literature Background: Summarize some tools that you use but your readers may not understand Method: Explain your method. Try to keep it as simple and clear as possible! Results: Evaluate your method. For each experiment be clear what point it is making, what are you demonstrating Conclusion/Discussion: What have you learned from your research. Where is this going? Any high level observations that are interesting? Style Citations Cite generously. Always mention other work in positive light. Writing style NEVER try to make something look more complicated than it is. Always simplify!! Equations The more important an equation, the more text you should have around it Formality Avoid informal wording (“When we ran these jobs we got these results.”) Graphs / Tables When you discuss graphs / tables, say clearly what the reader should pay attention to. Don’t assume the reader draws the correct conclusion by themselves. Explain Why, not What you did What you did is meaningless without providing the reason. Avoid at all cost. E.g. Now we integrate the function f(x,z) and obtain f(z). As a next step we add a constant term. The result is a loss function that we use for training. You should explain why: As our model is independent of x we can simplify our setup by marginalizing over x. The resulting function f(z) no longer depends on x, which allows us to …. 5 General Tips Do extensive background reading (you should know every paper ever published on the topic) Have a clear research question. Propose a solution and explain every step of the derivation. (Make it look like it is completely natural.) The more important an equation, the more text should be around it. Conduct clear experiments and make sure each one proves a well-defined point. (Be honest, you are researchers not salespeople!!!) Write well! Polish, polish, polish!","categories":[],"tags":[]},{"title":"JavaScript Manual","slug":"2022-06-11-JavaScript-Manual","date":"2022-06-11T04:00:00.000Z","updated":"2023-04-20T21:05:30.000Z","comments":true,"path":"2022-06-11-JavaScript-Manual/","permalink":"https://yao-lirong.github.io/blog/2022-06-11-JavaScript-Manual/","excerpt":"I hate web programming, but looks like I really have to learn it. Notes from Liao Xuefeng’s JS course","text":"I hate web programming, but looks like I really have to learn it. Notes from Liao Xuefeng’s JS course Introduction Variables and Objects Declare a variable with var: var x = 3 JavaScript has both null and undefined, but most of the time we just use null Print out value in browser: console.log() Ternary if-else Operator: condition ? then : else Strict Mode JavaScript在设计之初，为了方便初学者学习，并不强制要求用var申明变量。这个设计错误带来了严重的后果：如果一个变量没有通过var申明就被使用，那么该变量就自动被申明为全局变量： 1i = 10; // i现在是全局变量 在同一个页面的不同的JavaScript文件中，如果都不用var申明，恰好都使用了变量i，将造成变量i互相影响，产生难以调试的错误结果。 与之相对，使用var申明的变量是局部变量。 在strict模式下运行的JavaScript代码，强制通过var申明变量，未使用var申明变量就使用的，将导致运行错误。启用strict模式的方法是在JavaScript代码的第一行写上： 1&#x27;use strict&#x27;; Strings Strings are immutable in JS. 123456789var s = &#x27;Hello, world!&#x27;;s.length; // 13s[6]; // &#x27; &#x27;s[13]; // undefined 超出范围的索引不会报错，但一律返回undefined// 可以用 + 链接多个字符串，但也有一种更简洁的方法可以在字符串中嵌套其他变量var name = &#x27;小明&#x27;;var message = &quot;你好, $&#123;name&#125;&quot;; Array 12var arr = [1, 2, 3.14, &#x27;Hello&#x27;, null, true];arr.length; // 6 You can assign values to array, but 如果通过索引赋值时，索引超过了范围，同样会引起Array大小的变化： 123var arr = [1, 2, 3];arr[1] = 99; // arr == [1, 99, 3]arr[5] = &#x27;x&#x27;; // arr == [1, 2, 3, undefined, undefined, &#x27;x&#x27;] arr.push(e) adds variable e to the end of the array, arr.pop() delete an element from the end of the array arr.unshift(e) adds variable e to the beginning of the array, arr.shift() delete an element from the beginning of the array Object JavaScript Object is a key-value map that can be accessed both in traditional object way and the python dict way. 12345var xiaohong = &#123; name: &#x27;小红&#x27;, birth: 1990, tags: [&#x27;js&#x27;, &#x27;web&#x27;, &#x27;mobile&#x27;], &#x27;middle-school&#x27;: &#x27;No.1 Middle School&#x27;&#125;; xiaohong 的属性名 middle-school 不是一个有效的变量，就需要用''括起来。访问这个属性也无法使用.操作符，必须用 ['xxx'] 来访问。其他规则的变量虽然也可以用 xiaohong['name'] 来访问 xiaohong 的 name 属性，不过 xiaohong.name 的写法更简洁。 123xiaohong[&#x27;middle-school&#x27;]; // &#x27;No.1 Middle School&#x27;xiaohong[&#x27;name&#x27;]; // &#x27;小红&#x27;xiaohong.name; // &#x27;小红&#x27; 给不存在的属性赋值以声明新的属性，也可以使用 delete 删除一个既有的属性 1234xiaoming.age = 18; // 新增一个age属性xiaoming.age; // 18delete xiaoming.age; // 删除age属性xiaoming.age; // undefined Use in to detect whether an object has a certain property. 不过要小心，如果in判断一个属性存在，这个属性不一定是xiaoming的，它可能是xiaoming继承得到的。比如 toString 定义在 object 对象中，而所有对象都继承 object，所以 xiaoming 也拥有 toString 属性。要判断一个属性是否是xiaoming自身拥有的，而不是继承得到的，可以用hasOwnProperty()方法： 1234&#x27;name&#x27; in xiaoming; // true &#x27;toString&#x27; in xiaoming; // truexiaoming.hasOwnProperty(&#x27;name&#x27;); // truexiaoming.hasOwnProperty(&#x27;toString&#x27;); // false Loop for loop: for (i=0; i&lt;arr.length; i++) &#123;...&#125; for of loop: for (var key of xiaoming) &#123;...&#125;. This is a better version of for in loop JS has. To filter out those inherited properties, use 1234for (var key of xiaoming) &#123; if (xiaoming.hasOwnProperty(key)) &#123; console.log(key); // &#x27;name&#x27;, &#x27;age&#x27;, &#x27;city&#x27; &#125; &#125; while loop: while (n &gt; 0) &#123; ... &#125; do while loop: do &#123; ... &#125; while() Function 1234567var abs = function (x) &#123; return x;&#125;;function abs(x) &#123; return x;&#125; 由于JavaScript允许传入任意个参数而不影响调用，因此传入的参数比定义的参数多也没有问题，虽然函数内部并不需要这些参数： 12abs(10, &#x27;blablabla&#x27;); // 返回10abs(-9, &#x27;haha&#x27;, &#x27;hehe&#x27;, null); // 返回9 传入的参数比定义的少也没有问题：此时abs(x)函数的参数x将收到undefined，计算结果为NaN。 1abs(); // 返回NaN JSON JSON 是 JavaScript Object Notation 的缩写，实际上是 JavaScript 的一个子集。所以我们可以非常方便地将 Java Object 转换成 JSON。 1var s = JSON.stringify(xiaoming, null, &quot; &quot;); 要输出得好看一些，可以加上参数，按缩进输出： 1JSON.stringify(xiaoming, null, &#x27; &#x27;); 第二个参数用于控制如何筛选对象的键值，如果我们只想输出指定的属性，可以传入Array： 1JSON.stringify(xiaoming, [&#x27;name&#x27;, &#x27;birth&#x27;], &#x27; &#x27;); 结果： 12&#123; &quot;name&quot;: &quot;小明&quot;, &quot;birth&quot;: 1990&#125; 还可以传入一个函数，这样对象的每个键值对都会被函数先处理： 12345678function convert(key, value) &#123; if (typeof value === &#x27;string&#x27;) &#123; return value.toUpperCase(); &#125; return value;&#125;JSON.stringify(xiaoming, convert, &#x27; &#x27;); 上面的代码把所有属性值都变成大写： 12345&#123; &quot;name&quot;: &quot;小明&quot;, &quot;birth&quot;: 1990, &#x27;middle-school&#x27;: &#x27;No.1 MIDDLE SCHOOL&#x27;&#125; 如果我们还想要精确控制如何序列化小明，可以给xiaoming定义一个toJSON()的方法，直接返回JSON应该序列化的数据： 1234567891011121314var xiaoming = &#123; name: &#x27;小明&#x27;, age: 14, birth: 1990, middle-school: &#x27;No.1 MIDDLE SCHOOL&#x27; toJSON: function () &#123; return &#123; // 只输出name和age，并且改变了key： &#x27;Name&#x27;: this.name, &#x27;Age&#x27;: this.age &#125;; &#125;&#125;;JSON.stringify(xiaoming); // &#x27;&#123;&quot;Name&quot;:&quot;小明&quot;,&quot;Age&quot;:14&#125;&#x27; 拿到一个JSON格式的字符串，我们直接用JSON.parse()把它变成一个JavaScript对象： 1234JSON.parse(&#x27;[1,2,3,true]&#x27;); // [1, 2, 3, true]JSON.parse(&#x27;&#123;&quot;name&quot;:&quot;小明&quot;,&quot;age&quot;:14&#125;&#x27;); // Object &#123;name: &#x27;小明&#x27;, age: 14&#125;JSON.parse(&#x27;true&#x27;); // trueJSON.parse(&#x27;123.45&#x27;); // 123.45 jQuery $ is an alias to jQuery. Therefore, if you encounter error Uncaught TypeError: $ is not a function, you can just replace the $ with jQuery instead (e.g. jQuery('#abc')) Intro to Selector jQuery Object 一个 jQuery 命令返回的对象是 jQuery 对象。它类似数组，每个元素都是一个引用了DOM节点的对象。以下面的查找为例， 如果id为abc的&lt;div&gt;存在，返回的jQuery对象如下： 1[&lt;div id=&quot;abc&quot;&gt;...&lt;/div&gt;] 如果id为abc的&lt;div&gt;不存在，返回的jQuery对象如下： 1[] 总之jQuery的选择器不会返回undefined或者null，这样的好处是你不必在下一行判断if (div === undefined)。 jQuery对象和DOM对象之间可以互相转化： 123var div = $(&#x27;#abc&#x27;); // jQuery对象var divDom = div.get(0); // 假设存在div，获取第1个DOM元素var another = $(divDom); // 重新把DOM包装为jQuery对象 通常情况下你不需要获取DOM对象，直接使用jQuery对象更加方便。如果你拿到了一个DOM对象，那可以简单地调用$(aDomObject)把它变成jQuery对象，这样就可以方便地使用jQuery的API了。 按ID查找 在ID前加上 # 以进行ID查找 12// 查找&lt;div id=&quot;abc&quot;&gt;:var div = $(&#x27;#abc&#x27;); 按tag查找 直接写上tag名称进行tag查找： 12var ps = $(&#x27;p&#x27;); // 返回所有&lt;p&gt;节点ps.length; // 数一数页面有多少个&lt;p&gt;节点 按class查找 在class名称前加一个. 进行按class查找 1234var a = $(&#x27;.red&#x27;); // 所有节点包含`class=&quot;red&quot;`都将返回// 例如:// &lt;div class=&quot;red&quot;&gt;...&lt;/div&gt;// &lt;p class=&quot;green red&quot;&gt;...&lt;/p&gt; 通常很多节点有多个class，我们可以查找同时包含red和green的节点： 1234var a = $(&#x27;.red.green&#x27;); // 注意没有空格！// 符合条件的节点：// &lt;div class=&quot;red green&quot;&gt;...&lt;/div&gt;// &lt;div class=&quot;blue green red&quot;&gt;...&lt;/div&gt; 按属性查找 一个DOM节点除了id和class外还可以有很多属性，很多时候按属性查找会非常方便，比如在一个表单中按属性来查找： 123var email = $(&#x27;[name=email]&#x27;); // 找出&lt;??? name=&quot;email&quot;&gt;var passwordInput = $(&#x27;[type=password]&#x27;); // 找出&lt;??? type=&quot;password&quot;&gt;var a = $(&#x27;[items=&quot;A B&quot;]&#x27;); // 找出&lt;??? items=&quot;A B&quot;&gt; 当属性的值包含空格等特殊字符时，需要用双引号括起来。 按属性查找还可以使用前缀查找或者后缀查找： 1234var icons = $(&#x27;[name^=icon]&#x27;); // 找出所有name属性值以icon开头的DOM// 例如: name=&quot;icon-1&quot;, name=&quot;icon-2&quot;var names = $(&#x27;[name$=with]&#x27;); // 找出所有name属性值以with结尾的DOM// 例如: name=&quot;startswith&quot;, name=&quot;endswith&quot; 这个方法尤其适合通过class属性查找，且不受class包含多个名称的影响： 12var icons = $(&#x27;[class^=&quot;icon-&quot;]&#x27;); // 找出所有class包含至少一个以`icon-`开头的DOM// 例如: class=&quot;icon-clock&quot;, class=&quot;abc icon-home&quot; AND查找 直接写多个条件，条件间不加空格来执行AND查找。 如果我们查找$('[name=email]')，很可能把表单外的&lt;div name=\"email\"&gt;也找出来，但我们只希望查找&lt;input&gt;，就可以这么写： 1var emailInput = $(&#x27;input[name=email]&#x27;); // 不会找出&lt;div name=&quot;email&quot;&gt; 前文中的同时查找多个class也是一个例子。同样的，根据tag和class来组合查找也很常见： 1var tr = $(&#x27;tr.red&#x27;); // 找出&lt;tr class=&quot;red ...&quot;&gt;...&lt;/tr&gt; OR查找 把多个选择器用,组合起来，查找所有符合任一选择器条件的 DOM 节点。 12$(&#x27;p,div&#x27;); // 把&lt;p&gt;和&lt;div&gt;都选出来$(&#x27;p.red,p.green&#x27;); // 把&lt;p class=&quot;red&quot;&gt;和&lt;p class=&quot;green&quot;&gt;都选出来 要注意的是，选出来的元素是按照它们在HTML中出现的顺序排列的，而且不会有重复元素。例如，&lt;p class=\"red green\"&gt;不会被上面的$('p.red,p.green')选择两次。 Descendent Selector 以如下结构为例 12345678&lt;!-- HTML结构 --&gt;&lt;div class=&quot;testing&quot;&gt; &lt;ul class=&quot;lang&quot;&gt; &lt;li class=&quot;lang-javascript&quot;&gt;JavaScript&lt;/li&gt; &lt;li class=&quot;lang-python&quot;&gt;Python&lt;/li&gt; &lt;li class=&quot;lang-lua&quot;&gt;Lua&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; 层级选择器（Descendant Selector） 如果两个DOM元素具有层级关系，就可以用$('ancestor descendant')来选择，层级之间用空格隔开。 要选出上例中的 JavaScript，可以用层级选择器： 12$(&#x27;ul.lang li.lang-javascript&#x27;); // [&lt;li class=&quot;lang-javascript&quot;&gt;JavaScript&lt;/li&gt;]$(&#x27;div.testing li.lang-javascript&#x27;); // [&lt;li class=&quot;lang-javascript&quot;&gt;JavaScript&lt;/li&gt;] 因为&lt;div&gt;和&lt;ul&gt;都是&lt;li&gt;的祖先节点，所以上面两种方式都可以选出相应的&lt;li&gt;节点。 要选择所有的&lt;li&gt;节点，用： 1$(&#x27;ul.lang li&#x27;); 这种层级选择器相比单个的选择器好处在于，它缩小了选择范围，因为首先要定位父节点，才能选择相应的子节点，这样避免了页面其他不相关的元素。例如： 1$(&#x27;form[name=upload] input&#x27;); 就把选择范围限定在name属性为upload的表单里。如果页面有很多表单，其他表单的&lt;input&gt;不会被选择。 多层嵌套也是允许的： 1$(&#x27;form.test p input&#x27;); // 在form表单选择被&lt;p&gt;包含的&lt;input&gt; 子选择器（Child Selector） Child Selector $('parent&gt;child') 与 Descendant Selector 几乎完全一样，但是限定了层级关系必须是父子关系，就是&lt;child&gt;节点必须是&lt;parent&gt;节点的直属子节点。还是以上面的例子： 12$(&#x27;ul.lang&gt;li.lang-javascript&#x27;); // 可以选出[&lt;li class=&quot;lang-javascript&quot;&gt;JavaScript&lt;/li&gt;]$(&#x27;div.testing&gt;li.lang-javascript&#x27;); // [], 无法选出，因为&lt;div&gt;和&lt;li&gt;不构成父子关系 过滤器（Filter） 过滤器一般不单独使用，它通常附加在选择器上，帮助我们更精确地定位元素。观察过滤器的效果： 1234567$(&#x27;ul.lang li&#x27;); // 选出JavaScript、Python和Lua 3个节点$(&#x27;ul.lang li:first-child&#x27;); // 仅选出JavaScript$(&#x27;ul.lang li:last-child&#x27;); // 仅选出Lua$(&#x27;ul.lang li:nth-child(2)&#x27;); // 选出第N个元素，N从1开始$(&#x27;ul.lang li:nth-child(even)&#x27;); // 选出序号为偶数的元素$(&#x27;ul.lang li:nth-child(odd)&#x27;); // 选出序号为奇数的元素 查找和过滤 本节使用如下例子 12345678&lt;!-- HTML结构 --&gt;&lt;ul class=&quot;lang&quot;&gt; &lt;li class=&quot;js dy&quot;&gt;JavaScript&lt;/li&gt; &lt;li class=&quot;dy&quot;&gt;Python&lt;/li&gt; &lt;li id=&quot;swift&quot;&gt;Swift&lt;/li&gt; &lt;li class=&quot;dy&quot;&gt;Scheme&lt;/li&gt; &lt;li name=&quot;haskell&quot;&gt;Haskell&lt;/li&gt;&lt;/ul&gt; 查找 用find()查找： 123var ul = $(&#x27;ul.lang&#x27;); // 获得&lt;ul&gt;var dy = ul.find(&#x27;#swift&#x27;); // 获得 Luavar equiv = $(&#x27;ul.lang&gt;#swift&#x27;); // 与前两行等效 节点中移动 此小节介绍的所有函数都是当无参数调用时，返回目标节点。传入参数时，参数是过滤条件，当符合条件时返回目标节点，不符合时返回空 jQuery 对象 如果要从当前节点开始向上查找，使用parent()方法： 123var swf = $(&#x27;#swift&#x27;); // 获得Swiftvar parent = swf.parent(); // 获得Swift的上层节点&lt;ul&gt;var a = swf.parent(&#x27;.red&#x27;); // 获得Swift的上层节点&lt;ul&gt;，同时传入过滤条件。如果ul不符合条件，返回空jQuery对象 对于位于同一层级的节点，可以通过next()和prev()方法，例如当我们已经拿到Swift节点后： 1234567var swift = $(&#x27;#swift&#x27;);swift.next(); // Schemeswift.next(&#x27;[name=haskell]&#x27;); // 空的jQuery对象，因为Swift的下一个元素Scheme不符合条件[name=haskell]swift.prev(); // Pythonswift.prev(&#x27;.dy&#x27;); // Python，因为Python同时符合过滤器条件.dy 函数式编程 filter()方法可以过滤掉不符合选择器条件的节点： 12var langs = $(&#x27;ul.lang li&#x27;); // 拿到JavaScript, Python, Swift, Scheme和Haskellvar a = langs.filter(&#x27;.dy&#x27;); // 拿到JavaScript, Python, Scheme 或者传入一个函数，要特别注意函数内部的this被绑定为DOM对象，不是jQuery对象： 1234var langs = $(&#x27;ul.lang li&#x27;); // 拿到JavaScript, Python, Swift, Scheme和Haskelllangs.filter(function () &#123; return this.innerHTML.indexOf(&#x27;S&#x27;) === 0; // 返回S开头的节点&#125;); // 拿到Swift, Scheme map()方法把一个jQuery对象包含的若干DOM节点转化为其他对象： 1234var langs = $(&#x27;ul.lang li&#x27;); // 拿到JavaScript, Python, Swift, Scheme和Haskellvar arr = langs.map(function () &#123; return this.innerHTML;&#125;).get(); // 用get()拿到包含string的Array：[&#x27;JavaScript&#x27;, &#x27;Python&#x27;, &#x27;Swift&#x27;, &#x27;Scheme&#x27;, &#x27;Haskell&#x27;] Modifying DOM Contents HTML 本小节使用以下例子： 12345&lt;!-- HTML结构 --&gt;&lt;ul id=&quot;test-ul&quot;&gt; &lt;li class=&quot;js&quot;&gt;JavaScript&lt;/li&gt; &lt;li name=&quot;book&quot;&gt;Java &amp;amp; JavaScript&lt;/li&gt;&lt;/ul&gt; 使用 text() 和 html() 方法分别获取节点的文本和原始HTML文本。无参数调用是获取文本/HTML，传入参数就变成设置文本/HTML。 123456$(&#x27;#test-ul li[name=book]&#x27;).text(); // &#x27;Java &amp; JavaScript&#x27;$(&#x27;#test-ul li[name=book]&#x27;).html(); // &#x27;Java &amp;amp; JavaScript&#x27;var j1 = $(&#x27;#test-ul li.js&#x27;);j1.html(&#x27;&lt;span style=&quot;color: red&quot;&gt;JavaScript&lt;/span&gt;&#x27;); // 第一行被设置成红色的 JavaScript$(&#x27;#test-ul li[name=book]&#x27;).text(&#x27;书&#x27;); // 第二行被设置成 &quot;书&quot; 一个jQuery对象可以包含0个或任意个DOM对象，它的方法实际上会作用在对应的每个DOM节点上。如果作用在一个空的 jQuery 节点上，也不会报错 12$(&#x27;#test-ul li&#x27;).text(&#x27;JS&#x27;); // 两个节点都变成了JS$(&#x27;#not-exist&#x27;).text(&#x27;Hello&#x27;); // 不会报错，没有节点被设置成 &#x27;Hello&#x27; CSS 和 HTML 类似，对 jQuery 对象下的 css('name') 读取 CSS 属性， css('name', 'value') 方法设置 CSS 属性 12345var div = $(&#x27;#test-div&#x27;);div.css(&#x27;color&#x27;); // &#x27;#000033&#x27;, 获取CSS属性div.css(&#x27;color&#x27;, &#x27;#336699&#x27;); // 设置CSS属性div.css(&#x27;color&#x27;, &#x27;&#x27;); // 清除CSS属性div.css(&#x27;background-color&#x27;, &#x27;#ffd351&#x27;).css(&#x27;color&#x27;, &#x27;red&#x27;); // 连续设置两个 CSS 属性 Modifying DOM Structure 对于如下HTML片段， 12345678&lt;div id=&quot;test-div&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;span&gt;JavaScript&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;Python&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span&gt;Swift&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; 添加 DOM 首先要拿到&lt;ul&gt;节点： 1var ul = $(&#x27;#test-div&gt;ul&#x27;); 然后，调用append()传入HTML片段。append()把DOM添加到最后，prepend()则把DOM添加到最前。 1ul.append(&#x27;&lt;li&gt;&lt;span&gt;Haskell&lt;/span&gt;&lt;/li&gt;&#x27;); 除了接受字符串，append()还可以传入原始的DOM对象或jQuery对象 12345678// 创建DOM对象:var ps = document.createElement(&#x27;li&#x27;);ps.innerHTML = &#x27;&lt;span&gt;Pascal&lt;/span&gt;&#x27;;// 添加DOM对象:ul.append(ps);// 添加jQuery对象:ul.append($(&#x27;#scheme&#x27;)); 同级节点可以用after()或者before()方法。即如果要把新节点插入到指定位置，例如，JavaScript和Python之间，那么，可以先定位到JavaScript，然后用after()方法： 12var js = $(&#x27;#test-div&gt;ul&gt;li:first-child&#x27;);js.after(&#x27;&lt;li&gt;&lt;span&gt;Lua&lt;/span&gt;&lt;/li&gt;&#x27;); 删除 DOM 要删除DOM节点，拿到jQuery对象后直接调用remove()方法就可以了。如果jQuery对象包含若干DOM节点，实际上可以一次删除多个DOM节点： 12var li = $(&#x27;#test-div&gt;ul&gt;li&#x27;);li.remove(); // 所有&lt;li&gt;全被删除 Event 假设要在用户点击了超链接时弹出提示框，我们用jQuery这样绑定一个click事件： 1234567891011/* HTML: * * &lt;a id=&quot;test-link&quot; href=&quot;#0&quot;&gt;点我试试&lt;/a&gt; * */// 获取超链接的jQuery对象:var a = $(&#x27;#test-link&#x27;);a.on(&#x27;click&#x27;, function () &#123; alert(&#x27;Hello!&#x27;);&#125;); on方法用来绑定一个事件，我们需要传入事件名称和对应的处理函数。 另一种更简化的写法是直接调用click()方法：两者完全等价。我们通常用这种写法。 123a.click(function () &#123; alert(&#x27;Hello!&#x27;);&#125;); 事件类型 鼠标事件 click: 鼠标单击时触发； dblclick：鼠标双击时触发； mouseenter：鼠标进入时触发； mouseleave：鼠标移出时触发； mousemove：鼠标在DOM内部移动时触发； hover：鼠标进入和退出时触发两个函数，相当于mouseenter加上mouseleave。 键盘事件 键盘事件仅作用在当前焦点的DOM上，通常是&lt;input&gt;和&lt;textarea&gt;。 keydown：键盘按下时触发； keyup：键盘松开时触发； keypress：按一次键后触发。 其他事件 focus：当DOM获得焦点时触发； blur：当DOM失去焦点时触发； change：当&lt;input&gt;、&lt;select&gt;或&lt;textarea&gt;的内容改变时触发； submit：当&lt;form&gt;提交时触发； ready：当页面被载入并且DOM树完成初始化后触发。 ready 其中，ready仅作用于document对象。由于ready事件在DOM完成初始化后触发，且只触发一次，所以非常适合用来写其他的初始化代码。假设我们想给一个&lt;form&gt;表单绑定submit事件，下面的代码没有预期的效果： 1234567891011121314&lt;html&gt;&lt;head&gt; &lt;script&gt; // 代码有误: $(&#x27;#testForm&#x27;).on(&#x27;submit&#x27;, function () &#123; alert(&#x27;submit!&#x27;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;form id=&quot;testForm&quot;&gt; ... &lt;/form&gt;&lt;/body&gt; 因为JavaScript在此执行的时候，&lt;form&gt;尚未载入浏览器，所以$('#testForm)返回[]，并没有绑定事件到任何DOM上。所以我们自己的初始化代码必须放到document对象的ready事件中，保证DOM已完成初始化。这样写就没有问题了。因为相关代码会在DOM树初始化后再执行。 123456789101112131415&lt;html&gt;&lt;head&gt; &lt;script&gt; $(document).on(&#x27;ready&#x27;, function () &#123; $(&#x27;#testForm).on(&#x27;submit&#x27;, function () &#123; alert(&#x27;submit!&#x27;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;form id=&quot;testForm&quot;&gt; ... &lt;/form&gt;&lt;/body&gt; 由于ready事件使用非常普遍，所以可以这样简化： 123456$(document).ready(function () &#123; // on(&#x27;submit&#x27;, function)也可以简化: $(&#x27;#testForm).submit(function () &#123; alert(&#x27;submit!&#x27;); &#125;);&#125;); 甚至还可以再简化为： 123$(function () &#123; // init...&#125;); 上面的这种写法最为常见。如果你遇到$(function () &#123;...&#125;)的形式，牢记这是document对象的ready事件处理函数。 完全可以反复绑定事件处理函数，它们会依次执行： 123456789$(function () &#123; console.log(&#x27;init A...&#x27;);&#125;);$(function () &#123; console.log(&#x27;init B...&#x27;);&#125;);$(function () &#123; console.log(&#x27;init C...&#x27;);&#125;); 事件参数 有些事件，如mousemove和keypress，我们需要获取鼠标位置和按键的值，否则监听这些事件就没什么意义了。所有事件都会传入Event对象作为参数，可以从Event对象上获取到更多的信息： 12345$(function () &#123; $(&#x27;#testMouseMoveDiv&#x27;).mousemove(function (e) &#123; $(&#x27;#testMouseMoveSpan&#x27;).text(&#x27;pageX = &#x27; + e.pageX + &#x27;, pageY = &#x27; + e.pageY); &#125;);&#125;); 取消绑定 一个已被绑定的事件可以解除绑定，通过off('click', function)实现： 12345678910function hello() &#123; alert(&#x27;hello!&#x27;);&#125;a.click(hello); // 绑定事件// 10秒钟后解除绑定:setTimeout(function () &#123; a.off(&#x27;click&#x27;, hello);&#125;, 10000); 需要特别注意的是，下面这种写法是无效的： 123456789// 绑定事件:a.click(function () &#123; alert(&#x27;hello!&#x27;);&#125;);// 解除绑定:a.off(&#x27;click&#x27;, function () &#123; alert(&#x27;hello!&#x27;);&#125;); 这是因为两个匿名函数虽然长得一模一样，但是它们是两个不同的函数对象，off('click', function () &#123;...&#125;)无法移除已绑定的第一个匿名函数。 为了实现移除效果，可以使用off('click')一次性移除已绑定的click事件的所有处理函数。 同理，无参数调用off()一次性移除已绑定的所有类型的事件处理函数。 事件触发条件 一个需要注意的问题是，事件的触发总是由用户操作引发的。例如，我们监控文本框的内容改动： 1234var input = $(&#x27;#test-input&#x27;);input.change(function () &#123; console.log(&#x27;changed...&#x27;);&#125;); 当用户在文本框中输入时，就会触发change事件。但是，如果用JavaScript代码去改动文本框的值，将不会触发change事件： 12var input = $(&#x27;#test-input&#x27;);input.val(&#x27;change it!&#x27;); // 无法触发change事件 有些时候，我们希望用代码触发change事件，可以直接调用无参数的change()方法来触发该事件： 123var input = $(&#x27;#test-input&#x27;);input.val(&#x27;change it!&#x27;);input.change(); // 触发change事件 input.change()相当于input.trigger('change')，它是trigger()方法的简写。 为什么我们希望手动触发一个事件呢？如果不这么做，很多时候，我们就得写两份一模一样的代码。","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"博客SEO优化","slug":"2022-04-23-博客SEO优化","date":"2022-04-23T04:00:00.000Z","updated":"2022-06-08T19:37:38.000Z","comments":true,"path":"2022-04-23-博客SEO优化/","permalink":"https://yao-lirong.github.io/blog/2022-04-23-%E5%8D%9A%E5%AE%A2SEO%E4%BC%98%E5%8C%96/","excerpt":"谷歌和我网站有仇吗，弄了好几天，怎么别人等一天就行了，我等一个礼拜也搞不定。换了域名以后还不如原来，原来谷歌自动就给我 index 了。这样想还是要感谢营销号和爬虫，爬了我一个我自己都看不下去的题解，竟然让我原来的博客被收录了，可惜这个新的弄起来就麻烦了…","text":"谷歌和我网站有仇吗，弄了好几天，怎么别人等一天就行了，我等一个礼拜也搞不定。换了域名以后还不如原来，原来谷歌自动就给我 index 了。这样想还是要感谢营销号和爬虫，爬了我一个我自己都看不下去的题解，竟然让我原来的博客被收录了，可惜这个新的弄起来就麻烦了… 验证所有权 首先我们需要验证网站所有权，选用 HTML tag 方式 在 Hexo - Archer 主题下找到 layout - _partial - base-head.ejs 中在 &lt;head&gt; tag 下添加需要的验证 tag 对于不同的主题，一个快速找到 &lt;head&gt; 在哪里生成的方法就是直接查找 &lt;head&gt; tag 本地插件生成必要文件 使用 npm install &lt;name&gt; --save 安装以下几个插件: hexo-generator-robotstxt hexo-generator-sitemap hexo-generator-baidu-sitemap hexo-autonofollow Add the following plugin’s settings to root directory _config.yml: 123456789101112131415161718192021222324252627sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xmlrobotstxt: useragent: &quot;*&quot; disallow: - /assets/ - /css/ - /avatar/ - /scripts/ - /font/ - /lib/ allow: - / - /archives/ - /categories/ - /tags/ - /about/ - /page/ sitemap: - https://yao-lirong.github.io/sitemap.xml - https://yao-lirong.github.io/baidusitemap.xmlnofollow: enable: true Google 有些时候如果不进行这些所谓『优化』，你只能等有人链接到你的页面谷歌才会收录，所以这些优化实际上是必要的。 Canonical Tag 优化: 和前文一样，找到在 &lt;head&gt; tag 中对应位置，添加以下代码 123456&lt;% var base_url = config.url; if (config.url.charAt(config.url.length - 1) !== &#x27;/&#x27;) base_url += &#x27;/&#x27;; var canonical_url = base_url + page.canonical_path.replace(&#x27;index.html&#x27;, &#x27;&#x27;);%&gt;&lt;link rel=&quot;canonical&quot; href=&quot;&lt;%= canonical_url %&gt;&quot;&gt; 提交 sitemap.xml: 首先从 Search Console 中提交，在通过 ping 提交上双保险 提交 robots.txt: 从这里可以提交并看到谷歌最新抓取到的 robots.txt 提交了这些东西以后需要等好久，这时为了确认我们网站啥的确实没问题，心理求个安慰，使用 URL Inspection in Search Console, or directly at this link https://search.google.com/search-console/inspect?resource_id=&lt;url you want to check rn&gt;. 如果显示”URL is not on Google”，选择 “TEST LIVE URL”, 只要我们看到 “URL is available to Google” 以及 “User-declared canonical” 这一栏确实是本文网址一般就没问题。 此时，为了加快 index 进程，我们可以 “Request Indexing” 虽然只有这一个 page，但芝麻肉也是肉… Google Sitemap 的问题 上面用了 URL inspection tool 全是因为老显示我 sitemap couldn’t fetch. 查询了一下能做的只有等待… 谷歌工作人员的回复, 一个描述问题比较全的网站 Reference Hexo搜索引擎优化: 大部分有用的 Google SEO 优化步骤都来自这里 Get Google to Index Your Site: 以及国外的一个比较全的 Google 排雷网站","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Video Editing (FFmpeg DaVinci)","slug":"2022-04-09-视频编辑-(FFmpeg-DaVinci)","date":"2022-04-09T04:00:00.000Z","updated":"2023-05-18T09:48:32.000Z","comments":true,"path":"2022-04-09-视频编辑-(FFmpeg-DaVinci)/","permalink":"https://yao-lirong.github.io/blog/2022-04-09-%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91-(FFmpeg-DaVinci)/","excerpt":"主要记录视频的一些相关知识以及 FFmpeg 和 DaVinci 的常见用法","text":"主要记录视频的一些相关知识以及 FFmpeg 和 DaVinci 的常见用法 FFmpeg - Video File Format Conversion For most simple tasks, you can just do 1ffmpeg -i input.avi output.mp4 and ffmpeg will figure out how to perform that conversion. FFmpeg 合并文件: 12for f in *.flv; do echo &quot;file &#x27;$f&#x27;&quot; &gt;&gt; mylist.txt; doneffmpeg -f concat -i mylist.txt -c copy output.flv 更详细的官方文档在这里。具体地来看，我们这里用的是 concat demuxer, 这项协议支持不同的容器格式，甚至是本身不支持 concat 操作的容器格式的合并 1ffmpeg -i &quot;concat:input1|input2&quot; -codec copy output.mkv 对于 ts 之类本身支持 file-level concat 的文件格式，可以直接使用如上的 concat protocol (参考 stackoverflow 答案1, 答案2) FFmpeg 转换格式: 12345# stream copy all streamsffmpeg -i input -map 0 -c copy output.mp4 # re-encode the video to H.264 and stream copy the audioffmpeg -i input.ts -c:v libx264 -c:a copy output.mp4 FFmpeg 批处理转换格式: 12345#!/bin/bashfor i in *.avi;do ffmpeg -i &quot;$i&quot; -c:v libx265 -c:a copy X265_&quot;$i&quot;done Editing, Clipping, Encoding Extract Audio from Video: where -vn disables video processing 12ffmpeg -i input.mp4 -vn -acodec copy output-audio.aac # extract aacffmpeg -i input.mp4 -vn -acodec copy output-audio.opus # extract opus FFmpeg查看媒体信息: 使用 ffprobe Extract Subtitles: On hdmv_pgs_subtitle, where we need sup format. 12ffmpeg -i video.mkv -map 0:s:0 -c copy subs.supffmpeg -i Movie.mkv -map 0:s:0 -c copy subs.srt FFmpeg H.265 Reencode: if you use all the settings as default, just do: 1ffmpeg -i input -c:v libx265 -c:a copy output.mp4 Clip Video: you should primarily read this wiki page, which introduces you to seeking in ffmpeg. In short, you should use -ss before -i in most cases. 1234567891011# -ss used before -i: parse input using keyframes, which is very fast# gives exactly the same outputffmpeg -ss 00:40:30 -i 20170301.mp4 -t 310 -c copy 1.mp4 ffmpeg -ss 00:40:30 -to 00:45:40 -i 20170301.mp4 -c copy 2.mp4# -ss used after -i: decodes but discards input until the timestamps reach position. # which is done very slowly, frame by frameffmpeg -i 20170301.mp4 -ss 00:40:30 -to 00:45:40 -c copy 3.mp4# Doesn&#x27;t work, will output something at most 45:40 longffmpeg -ss 00:40:30 -i 20170301.mp4 -to 00:45:40 -c copy 4.mp4 Broken File Fix Fill Missing Time Stamps with Empty Audio: 1ffmpeg -i input -af aresample=async=1 output.wav Fill Missing Frames (Change Variable Frame to Constant Frame): 1ffmpeg -i input -vf &quot;fps=30&quot; output.mp4 Create a Silent Audio Track 1ffmpeg -f lavfi -i anullsrc=channel_layout=stereo:sample_rate=44100 -i video.mov -c:v copy -c:a aac -shortest output.mov Others ffmpeg download m3u8 file with custome user-agent: If flag -user_agent is not working, you can use -headers, referenced here 12ffmpeg -user_agent &quot;SNH48 ENGINE&quot; -i &quot;https://xxx.m3u8&quot; -codec copy file.mp4ffmpeg -headers &#x27;User-Agent: Mozilla&#x27; -i &quot;https://xxx.m3u8&quot; -codec copy file.mp4 https://superuser.com/questions/1041816/combine-one-image-one-audio-file-to-make-one-video-using-ffmpeg FFmpeg - Audio ！！！整理！！！ https://stackoverflow.com/questions/46508055/using-ffmpeg-to-cut-audio-from-to-position https://stackoverflow.com/questions/71158575/output-file-is-empty-nothing-was-encoded-check-ss-t-frames-parameters-i 123ffmpeg -ss 60 -i presentation.aac -t 240 -c copy presentation_song.aac 好用ffmpeg -ss 60 -t 240 -i presentation.aac -c copy presentation_song.aac 不好用 convert file format to .ogg with specififed sample rate: here we specified sample rate to be 44.1K 1ffmpeg -i input.wav -c:a libvorbis -ar 44100 output.ogg 1234567ffmpeg -framerate 30 -i z_Blue1_1_60_%d.png -c:v libx264 -r 30 output.mp4for color in &quot;Blue&quot; &quot;Red&quot; &quot;Green&quot; &quot;Yellow&quot;; do for ((i=1; i&lt;=3; i++)); do printf &quot;%s%d_1_60.png\\n&quot; &quot;$color&quot; &quot;$i&quot;; done; doneffmpeg -framerate 15 -i z_Blue1_1_60.png_%d.png -c:v libx264 -r 15 output.mp4记得看 aphelion-defense/assets/textures/60/test.sh DaVinci Import and Bake Subtitles in DaVinci Zoom In and Zoom Out in DaVinci: achieve with key frames How to Fade in and Out Video Vertical timeline (Tiktok style) How to Control Audio Volume Levels 编码解码格式 用 IDM 下载 YouTube 上视频会有两种格式 mkv 和 mp4 两种格式，结论：mkv 格式更优 都下载下来后使用 PotPlayer 播放时查看发现 mkv 格式需要使用 FFmpeg libdav1d decoder mp4 格式需要使用 FFmpeg h264 decoder 使用 ffprobe 发现 mkv 格式使用 av1 格式编码 mp4 格式使用 h264 格式编码 查询资料得知 av1 是谷歌新发布的编解码规范，相比 h265 压缩优势都很大，就不用说 h264 了。IDM 官方也推荐使用 av1 编码的 mkv 格式。（官方FAQ: Can I download MP4 instead of MKV or what should I do to play MKV videos correctly? | I cannot play downloaded MKV video. What should I do?） One caveat: Google seems to be using vp9 as the encoder of live stream, but this is still better than h264 in mp4. 对于 SNH48 公演录播源，发现官网源使用 h264 ts 编码，YouTube 源大概是单独推流，谷歌编码为 vp9，且 YouTube 源有 1080P，官网只有720P。故优先选择 YouTube 源 （但是后来发现 YouTube 源好像音频是 opus 格式，Davinci 识别不了，最后用的还是别人的 bilibili 源）","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"2021 Web Journal","slug":"2021-12-31-2021-网络日志","date":"2021-12-31T05:00:00.000Z","updated":"2023-12-26T14:25:01.578Z","comments":true,"path":"2021-12-31-2021-网络日志/","permalink":"https://yao-lirong.github.io/blog/2021-12-31-2021-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/","excerpt":"开始学习阮一峰记录自己看到的有意思的文章和收集有用的工具的第一个整年 The best science images of 2021","text":"开始学习阮一峰记录自己看到的有意思的文章和收集有用的工具的第一个整年 The best science images of 2021 工具的使用 VScode根据不同语言设置不同tab代表的空格个数 在LINUX系统下使用SSH登陆上路由器: 注意可以使用flagfox看一眼路由器的IP address到底是什么，username就是登陆路由器使用的用户名，如果两个都填写正确是不会出现 “port XX refused connection”这种情况的 （记得开 merlin 的 ssh 许可：Administration - System - Enable SSH） 1ssh &lt;username&gt;@&lt;IP_address&gt; 批量合成bilibili的m4s缓存文件为MP4格式 禁用Windows Terminal多行粘贴的警告 apt vs apt-get: 简单来说，apt是更新的集合版的 apt-get，应该尽量使用 apt 为日文文档添加振り仮名的word宏 CodeBlocks 运行问题解决: ld.exe: cannot open output file … : Permission denied Convert HTML back to Markdown with Pandoc: pandoc -f html --wrap=none -t markdown -o test.md &lt;filename&gt;. Experiment with --wrap=none/preserve/auto to choose the best. showdownjs: online two-way markdown and html converter GitHub 今年对于 Windows 变为强制需要 token，且只有 HTTPS 才能使用 Token Capture Group in Regex: When matching a pattern, use parenthesis (some_pattern_you_try_to_match) to define a capture group. (Two references: succinct, more detailed) Zotero 在论文对应原始网页直接使用浏览器插件获取的信息最全，如果没有对这么全面的信息的要求，可以在 Google Scholar 页面选择 Cite -&gt; Refman (Refman 比 Endnote 格式获取信息更多一些) 涨技术知识 The intuition behind Shannon’s Entropy BT术语的解释 协议混乱的 USB-C What is SSH Port Forwarding and how does it work: 图示写的特别好 混迹于互联网 一线个人破解及整合 大眼仔旭, 落尘之木, 423down, 果壳剥壳 二线网赚搬运 Nite07的小窝, 小冰下载站, 佛系软件 Adobe 大神破解版 by vposy, 天极下载 PC 软件历史版本, apkdownload(比 apkpure 全), 破解apk Moddroid 伊朗PC软件破解, from 胡萝卜周 网站推荐 折腾 Pot+LAV+madVR配置教程: 使用 Icaros 显示多种视频文件缩略图 禁用Win10自动更新: configure automatic updates 板块 Disable Firefox Auto-Update: Go to about:policies to check flags that could be set. Create a policies.json, create a directory called distribution where the EXE is located, and place the json file there on Windows. 在五种不同系统上部署Rime并同步 Web Annotation Tools: diigo: 超了最高容量了 weava: 正在用 beanote: 可以试试，记录保存在本地，无云端服务 additor, Hypothesis: 重点并不在自己的服务保存，而是在协作，想等他们一阵子看看发展效果，结果最近又用了一次还是不好用 值得纪念的新闻 优秀的付费新闻资讯：FT中文网，财新网，财经网， 能处的日本新闻：日本经济新闻, 朝日新闻, 赞叹｜锤子手机未必最好，但他们的设计师都值得致敬 （现在的社会）不是为了人的发展，而是人为了这些东西而发展，反过来了。这些钢筋水泥，一些数据华丽的办公楼，一些冰冷的没有什么价值的东西，已经成为了人追求的全部了。 – “躺平学大师”：一个好的社会是可上可下的 末日时钟 全球人口衰退期临近，经济方程式在变, 30万年历史上两次革命让人类繁荣，到了转折点？ 改变人口增长趋势的是女性的教育程度提高和加速进入社会导致生育率下降。1名女性一生生育的孩子数量（总和生育率）2017年为2.4，逼近人口无法增长的2.1。少子化是每个人的人生选择不断累积的结果，几乎没有一个国家能扭转局面。人口增长率在20世纪60年代后半期达到2.09％的峰值，现在已经下降到1％左右。 脱碳之路存在能源供需断裂风险: 无论以保存现状或推动减碳为前提，预测2050年石油天然气需求仍然高于现在。即便如此，欧美国际石油资本还是开始减少对油田和气田的投资。然而中东由于他国减产，正在逐步扩大产能。如果仅仅是中东增加投资，垄断度提高的话将给世界的政治和经济带来很大风险。","categories":[],"tags":[{"name":"Journal","slug":"Journal","permalink":"https://yao-lirong.github.io/blog/tags/Journal/"}]},{"title":"Look Back on Cornell 21FA","slug":"2021-12-15-Look-Back-on-Cornell-21FA","date":"2021-12-15T05:00:00.000Z","updated":"2023-02-23T05:16:18.000Z","comments":true,"path":"2021-12-15-Look-Back-on-Cornell-21FA/","permalink":"https://yao-lirong.github.io/blog/2021-12-15-Look-Back-on-Cornell-21FA/","excerpt":"回来了，总体来说是挺努力的一学期","text":"回来了，总体来说是挺努力的一学期 CS3410 Computer System Organization 必修课，没啥好说的，上课之前暑假和在飞机上自己看了几章Code预习了一下，感觉是Code讲了的部分比Bracy讲的好多了。幸亏前期有Code，真不知道其他同学能不能弄懂前面逻辑门那一部分。只有两个 Prelim 没有 Final，所以我考完 Prelim 2 之后就摆烂了，上课跟听故事一样。 A1 A2：画逻辑电路图，其实大部分还好，但是最后连mux都要全程自己连实在太折磨了 A3：通过内存溢出改变运行程序让它打印一段文字，应该是最好玩的一个作业 A4 A5：模拟 RISCV interpreter 和 Cache 操作 比画电路图还折磨一些的是本课的 quiz，可以无限次尝试但是每次尝试题目不一样而且有强制时间间隔。经常我是第一次就有9／10，结果最后那个题要么我自己不会做然后反复出现，要么又出了大部分的新题其中又有我不懂的，要么就是我明明全会结果手滑选错了，下一次就会随到前两种情况不会的题。反正就是我为了其中的一道题要把所有题过个7,8遍，但是过程中大部分的题我已经反复做反复填答案填了无数次，每次填它的答案顺序还不一样，还要仔细看好了防止手滑选错，折磨王之王中王。 CS4780 Intro to Machine Learning 我当年上我的一生之敌4710就是为了 21SP 能上 4780，但是那年 4780 是播片，所以拖到了我回来康奈尔的这一学期。大部分是经典的ML算法，Deep Learning 和 Neural Network 涉及比较少。Assignment 和 project 都很水，基本就是填空题（好吧不全是，前期 assignment 太变态导致后来直接把作业成绩改成 S/U 了；project 用的平台 vocareum 也是各种乱七八糟的问题一大堆）所以最终成绩大概全是靠考试来排的。上之前大家都吹 KW 讲得特别好，但我真觉得也就那样，可能是其他的几个老师讲得太烂了？或者这门课有很多别的专业的来学而他们的老师一般讲课水准不高？反正我觉得也就是CS院平均水平吧，比我上的课的老师平均水平还稍微低一点点。因为前面确实涉及太多数学，所以我几乎每次下课都会问两个老师问题，一开始都对我挺耐心的，后来AD还好，但KW我能明显感觉到不耐烦了（哦当然了可能是我日常敏感了），有次我去找他问个扩展问题他跟我说：你不太需要担心这个。唉，好吧，你说啥就是啥。 sxy 因为 prelim 考砸了所以 drop 了这节课，下班学期没她带上得比较累（ 但是学期末因为一波较大型的 COVID 爆发，本课的 Final 改成 optional 了，可给她悔死了。她下学期又上了一遍这门课，觉得 22SP 的老师比 KW 教得清楚多了，我 22FA 要上他的课，看看到底咋样。 CS6850 Structure of Information Networks Jon Kleingerg真的是神，人聪明课讲得还清晰。各种看上去很难的推导经他的课就连我也能听明白。每节课下课的时候我总会追着这个麦克阿瑟天才奖得主问些很傻逼的基础数学问题，每当这种时候我就会觉得我在康奈尔的学费全都花在了刀刃上。（不我其实是很对不起 Jon 用这种傻逼问题折磨他的） 这节课上下来我学到的统计知识比我在4710一个学期学的都要多，而且很多证明技巧都是 CS 或者说工科通用，非常实用。课上讲了很多有意思的图上理论，不过一学期下来其实我也就记着 small world property，和以我校唯一数学名师 Steven Strogatz 及其弟子命名的 Watts Strogatz Model。作业的比较简单 project 也很自由，就算只是为了学统计证明技巧也很值得上的一节课。 Project 做的是 Complexity 里面看到过的 Random Boolean Network，其实本来是想做一个 DNA Regulatory Network 模拟的，但是怎么都找不到数据，所以改做了 RBN，其实实验和我的猜想也根本对不上，到了后面基本就是摆烂了瞎写的。 因为这学期另外两个课确实压力有点大，前期是 3410 恶心人的电路图，后期是没有 sxy 的 4780，而且我还非常认真地给 4780 整理笔记，所以这节课到后半段的时候我会去上课，会去记笔记，但是笔记就不整理誊抄到另一个本子上了。结果就是后半部分让我再回忆指定就回忆不起来了。不过幸好这节课同步 NYC 直播所以有录播，把 lecture recording 下了下来。可能下辈子会去再看一遍吧… Research 学期初的时候我跟 Joe 谈话，跟他说我还是纠结要去做research还是找工作。他跟我说我都大三了现在还在纠结是不是有点太晚了… 我本来刚回康奈尔心情挺好的，又让这老东西给我干焦虑了。不过经过他的介绍找到了新来的教授 Kevin Ellis。去年春天我和本校 PhD Spencer 谈话的时候他就和我提起来过 Kevin，他做的东西确实是我一看就觉得很有意思。原来大一的时候我也找过康奈尔校内老师的研究项目但是总觉得都很无聊，Kevin 给我介绍的 Abstraction and Reasoning Corpus 确实让人觉得这个问题才是 AI 应该着手解决的问题。可能因为是新来老师的原因，非常幸运和他谈了以后他也高兴让我跟他一起做研究。ARC 里的 log.md 更加详细地记录了关于 research 我这一学期都干了什么。 PE1340 Juggling 神级减压课，没上过这课的人无法想象你用了几节课就学会了在空中扔三个球是多么有成就感的一节事，在康奈尔这个充满挫败感的B地方，它是我这个学期的唯一慰藉。不仅如此，你要是学得快的话还能学会扔棒子，转盘子，不过我都不会。唯一一个我会的扩展项目是空竹（我会的同学们也都会，这玩意确实好学）等回国去公园和老大爷斗技去。 PE1628 Unicycling 小丑学期，既学扔球又学独轮车，全让我给丑完了。上面那节课是慰藉，这节课完全相反，我毫不客气地说这是我在康奈尔几年以来遭到的最沉重的自信心打击。一个学期上下来7节课21个小时，我还是不会骑这B玩意，别人都能骑着车跳了，我还搁墙角那扶着呢。我学习能力怎么样咱另说，这玩意隔得蛋是真疼。 NES2276 Sensational Religion 为了别毕不了业做打算随便选的一门课，和众多文科课一样，它也是那么得扯。课上最扯的一个瞬间大概就是看这部名为塑料袋的纪录片，看完之后课上同学一本正经地讨论是我觉得我降生以来离正常世界距离最远的时候。 Winter Break 最后浅谈一下假期。感恩节的时候偶然和gjt说上了话，gjt热情地邀请我去他们那。他是和cqc和大帅住一个宿舍，ls寒假也去了。这几个人真是电竞宿舍，成天打倒晚上一两点，起来就直接吃中午饭，写作业考试都找代写，反正就是 stereotypical 的中国来的在美留学生。寒假和他们去了几趟LA还去了一趟拉斯维加斯，去的时候他们几个开的一号公路我在后面直接给我晃吐了，biang的不就看个海吗，在哪不是看。死亡谷还是挺有看头的，还一起去维加斯看了看不好说的节目。 回来以后拿他们几个的顶配电脑9天连干70个通了2077，后面又通了生化危机2,3重置版，反正就是每天干到凌晨五点，然后中午起床，在国内的qsq都经常让我整蒙逼，毕竟CA 5点是国内八九点钟，他都快要睡了，直言我人生已经玩完了。我刚去的时候还笑他们，最后发现原来我才是网瘾最重的那个。跟他们出去的时候因为闲着没事就看书看小说，被他们调侃爱学习，回来以后他们几个都开学了我就成天占着电脑打游戏。我确实一直这样，主要出去的时候没电脑，我也不爱在外面刷手机，就会变成我每次出门都会看书的这种奇怪情况，不过只要有台好电脑我就原形毕露了。 非常感谢这几个人让我度过了来美国以后最快乐的一段时光，太过快乐导致我第一天回来竟然戒断性抑郁了（当天回来的飞机上有点缺氧也是主要原因）","categories":[],"tags":[{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"}]},{"title":"SQL Manual","slug":"2021-09-16-Intro-to-SQL","date":"2021-09-16T04:00:00.000Z","updated":"2022-06-08T19:54:02.000Z","comments":true,"path":"2021-09-16-Intro-to-SQL/","permalink":"https://yao-lirong.github.io/blog/2021-09-16-Intro-to-SQL/","excerpt":"提交实习申请后发来个小测验，给我做崩溃了，第一个要我做这种级别的SQL，我最多也就会个 select from where，超纲过于严重，只能回头补习","text":"提交实习申请后发来个小测验，给我做崩溃了，第一个要我做这种级别的SQL，我最多也就会个 select from where，超纲过于严重，只能回头补习 Data Type 名称 类型 说明 INT 整型 4字节整数类型，范围约+/-21亿 BIGINT 长整型 8字节整数类型，范围约+/-922亿亿 REAL 浮点型 4字节浮点数，范围约+/-1038 DOUBLE 浮点型 8字节浮点数，范围约+/-10308 DECIMAL(M,N) 高精度小数 由用户指定精度的小数，例如，DECIMAL(20,10)表示一共20位，其中小数10位，通常用于财务计算 CHAR(N) 定长字符串 存储指定长度的字符串，例如，CHAR(100)总是存储100个字符的字符串 VARCHAR(N) 变长字符串 存储可变长度的字符串，例如，VARCHAR(100)可以存储0~100个字符的字符串 BOOLEAN 布尔类型 存储True或者False DATE 日期类型 存储日期，例如，2018-06-22 TIME 时间类型 存储时间，例如，12:20:59 DATETIME 日期和时间类型 存储日期+时间，例如，2018-06-22 12:20:59 Table Constraints Constraint Description PRIMARY KEY This means that the values in this column are unique, and each value can be used to identify a single row in this table. AUTOINCREMENT For integer values, this means that the value is automatically filled in and incremented with each row insertion. Not supported in all databases. UNIQUE This means that the values in this column have to be unique, so you can’t insert another row with the same value in this column as another row in the table. Differs from the PRIMARY KEY in that it doesn’t have to be a key for a row in the table. NOT NULL This means that the inserted value can not be NULL. CHECK (expression) This allows you to run a more complex expression to test whether the values inserted are valid. For example, you can check that values are positive, or greater than a specific size, or start with a certain prefix, etc. FOREIGN KEY This is a consistency check which ensures that each value in this column corresponds to another value in a column in another table. For example, if there are two tables, one listing all Employees by ID, and another listing their payroll information, the FOREIGN KEY can ensure that every row in the payroll table corresponds to a valid employee in the master Employee list. Table Structure Foreign Key A foreign key is a field (or collection of fields) in one table, that refers to the primary key in another table. The foreign key constraint prevents invalid data from being inserted into the foreign key column, because it has to be one of the values contained in the parent table. 1234ALTER TABLE students ADD CONSTRAINT fk_class_id -- name the constraintFOREIGN KEY (class_id) -- use class_id column in students as FKREFERENCES classes (id); -- links to id in table classes Deleting the constraint won’t delete the column used as FK. 12ALTER TABLE studentsDROP FOREIGN KEY fk_class_id; Indexing We can index frequently accessed columns to speed up querying. Indexes are based on hash, so the more spread out the data in index columns are, the better indexing performs. 索引的效率取决于索引列的值是否散列，即该列的值如果越互不相同，那么索引效率越高。反过来，如果记录的列存在大量相同的值，例如gender列，大约一半的记录值是M，另一半是F，因此，对该列创建索引就没有意义。 12345ALTER TABLE studentsADD INDEX idx_score (score); -- indexing named as idx_score; it indexes column score ALTER TABLE studentsADD INDEX idx_name_score (name, score); -- create a two-column indexing of name and score Unique Add a UNIQUE constraint to make sure the uniqueness of student’s name (Assume no two students have the same name). 12ALTER TABLE studentsADD CONSTRAINT uni_name UNIQUE (name); Querying Table 注意字符串用的都是单引号 ' '. Conditionals =: equal &lt;&gt;: not equal LIKE: case insensitive exact string comparison; % is wildcard. 'ab%' matches ‘ab’，‘abc’，‘abcd’ _ is “appeared once”. LIKE \"ab_\" matches “abc”, but not “ab” or “abcd” BETWEEN … AND …: number is within range of two values (inclusive). e.g. col_name BETWEEN 1.5 AND 10.5 IN (…): number exists in a list. e.g. col_name IN (2, 4, 6) NOT …: to negate a predicate 12345678-- sometimes we don&#x27;t need FROM-- this is usually used to test connection to data baseSELECT 1SELECT 100 + 200SELECT * FROM students WHERE (score &lt; 80 OR score &gt; 90) AND gender = &#x27;M&#x27;;SELECT * FROM students WHERE (NOT class_id &lt;&gt; 2) AND score LIKE &#x27;8%&#x27; ; Projections 12-- rename column score as pointsSELECT id, score points, name FROM students; Orders Query results are usually ordered by PK. If we want to change the order, we can 1234567891011-- order by score (default in ascending order 正序)SELECT id, name, score FROM students ORDER BY score;-- order by score and gender (descending score and ascending id)SELECT id, name, score FROM students ORDER BY score DESC, id;-- together with WHERESELECT id, name, gender, scoreFROM studentsWHERE class_id = 1ORDER BY score DESC; Partial Results Query result is sometimes in huge amount. In this case, we only want to show part of the result. 1234567-- show only 3 resultSELECT id, name, gender, score FROM students ORDER BY score DESCLIMIT 3;-- show only 3 result, starting from the 7th.SELECT id, name, gender, score FROM students ORDER BY score DESCLIMIT 3 OFFSET 6; Groups 12345-- return #records in TABLE students, and name it numSELECT COUNT(*) num FROM students;-- return #records whose gender is &#x27;M&#x27;, and name the result &quot;boys&quot;SELECT COUNT(*) boys FROM students WHERE gender = &#x27;M&#x27;; 和 COUNT 类似的还有以下函数： 函数 说明 SUM 计算某一列的合计值，该列必须为数值类型 AVG 计算某一列的平均值，该列必须为数值类型 MAX 计算某一列的最大值，如果是字符串类型则返回排序最后的字符 MIN 计算某一列的最小值，如果是字符串类型则返回排序最前的字符 其中，如果 WHERE 条件没有匹配到任何行，COUNT()会返回0，而SUM()、AVG()、MAX()和MIN()会返回NULL。 分完组后，我们不能再用 WHERE 对组进行筛选，以组为级别进行筛选需要 HAVING. 12345678910111213-- 按照 class_id 分组(class_id=1, 2, 3, ... 各一组)， 分别返回每一组的总记录数SELECT class_id, COUNT(*) num FROM students GROUP BY class_id;-- 分完组后，SELECT class_id, COUNT(*) num FROM students GROUP BY class_id HAVING COUNT(*) &gt; 36;-- 对于像 name 这种在一个组内并不是相同的值，会返回 NULL / 报错-- 因此对于聚合查询，我们只能放入聚合查询的 col 名或者一些其他的聚合函数SELECT name, class_id, COUNT(*) num FROM students GROUP BY class_id;-- 查询每个班级男女分别的平均分SELECT class_id, gender, AVG(score) FROM students GROUP BY gender, class_id ORDER BY class_id, gender; Multiple Tables When you select from more than one table, database will return the Cartesian product of the results. 12345SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cnameFROM students s, classes cWHERE s.gender = &#x27;M&#x27; AND c.id = 1; Join 与前文的 Multiple Tables 不同的是，Multiple Tables 把所有结果先调出来再根据结果进行筛选，效率很慢；而我们的 JOIN 可以 INNER JOIN只返回同时存在于两张表的行数据。比如students表的class_id包含1，2，3，classes表的id包含1，2，3，4，所以，INNER JOIN根据条件s.class_id = c.id返回的结果集仅包含1，2，3。 RIGHT OUTER JOIN返回右表都存在的行。如果某一行仅在右表存在，那么结果集就会以NULL填充剩下的字段。 LEFT OUTER JOIN则返回左表都存在的行。如果我们给students表增加一行，并添加class_id=5，由于classes表并不存在id=5的行，所以，LEFT OUTER JOIN的结果会增加一行，对应的class_name是NULL： FULL OUTER JOIN，它会把两张表的所有记录全部选择出来，并且，自动把对方不存在的列填充为NULL： 12345678-- Join 指令模板SELECT ... FROM tableA ??? JOIN tableB ON tableA.column1 = tableB.column2;-- 上文使用的例子对应的指令SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.scoreFROM students sFULL OUTER JOIN classes cON s.class_id = c.id; Join in Graphs Null An alternative to NULL values in your database is to have data-type appropriate default values, like 0 for numerical data, empty strings for text data, etc. But if your database needs to store incomplete data, then NULL values can be appropriate if the default values will skew later analysis (for example, when taking averages of numerical data). 123-- select all non-null valuesSELECT column, another_column, … FROM mytableWHERE column_name IS/IS NOT NULL Operating on Rows Insert When we insert something into the table, we don’t have to specify value of the primary key column. Because the primary key is automatically calculated by the database. 12345678INSERT INTO table_name (col1, col2, ...) VALUES (v1, v2, ...) (v1, v2, ...);INSERT INTO students (class_id, name, gender, score) VALUES (1, &#x27;大宝&#x27;, &#x27;M&#x27;, 87), (2, &#x27;二宝&#x27;, &#x27;M&#x27;, 81); Update We can update a record in the table. 1234567891011UPDATE table_name SET col1=v1, col2=v2, ... WHERE ...;-- update a single recordUPDATE students SET name=&#x27;大牛&#x27;, score=66 WHERE id=1;-- update multiple recordsUPDATE students SET name=&#x27;小牛&#x27;, score=77 WHERE id&gt;=5 AND id&lt;=7;-- add 10 points to all scores below 80UPDATE students SET score=score+10 WHERE score&lt;80; Delete 1234DELETE FROM table_name WHERE ...;-- delete multiple records DELETE FROM students WHERE id&gt;=5 AND id&lt;=7; Operating on Table Create Table 1234567891011121314CREATE TABLE IF NOT EXISTS table_name ( column_name *DataType* *TableConstraint* DEFAULT *default_value*, another_column_name *DataType* *TableConstraint* DEFAULT *default_value*, … );Movies table schemaCREATE TABLE movies ( id INTEGER PRIMARY KEY, title TEXT, director TEXT, year INTEGER); Alter Table 1234567891011-- add a column to tableALTER TABLE mytable ADD column *DataType* *OptionalTableConstraint* DEFAULT default_value;-- remove a column from tableALTER TABLE mytableDROP column_to_be_deleted;-- renaming the tableALTER TABLE mytableRENAME TO new_table_name; Delete Table 1DROP TABLE IF EXISTS mytable; Others View 12345create view current_demographic_dim(current_demographic_key, current_salary_range, current_age_range)asselect demographic_key, salary_range, age_rangefrom SAMPLES.DEMOGRAPHIC_DIM; 实用SQL语句 Reference 廖雪峰的SQL教程 SQLBolt","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Java Quick Guide","slug":"2021-09-10-Java-Quick-Guide","date":"2021-09-10T04:00:00.000Z","updated":"2022-09-05T04:04:52.000Z","comments":true,"path":"2021-09-10-Java-Quick-Guide/","permalink":"https://yao-lirong.github.io/blog/2021-09-10-Java-Quick-Guide/","excerpt":"","text":"Basics Basic File Structure: 12345public class &lt;SameAsFileName&gt; &#123; public static void main(String args[])&#123; &#125;&#125; Typecasting: int a = (int) pow(2,5); Binary: reference to formatting, 1234567int i = 0b10101010; // give binary valueSystem.out.println(Integer.toBinaryString(x)); // print binaryInteger.parseUnsignedInt(&quot;10101010&quot;, 2); //input binary// print with paddingString.format(formatPattern, Integer.toBinaryString(data)).replace(&#x27; &#x27;, &#x27;0&#x27;); String formatPattern = &quot;%&quot; + maximumExpectedSize + &quot;s&quot;; good String Create a string of all character c: String 10Spaces = new String(new char[10]).replace('\\0', ' ');","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"C Manual","slug":"2021-08-31-Introduction-to-C","date":"2021-08-31T04:00:00.000Z","updated":"2022-06-08T19:34:38.000Z","comments":true,"path":"2021-08-31-Introduction-to-C/","permalink":"https://yao-lirong.github.io/blog/2021-08-31-Introduction-to-C/","excerpt":"大概是写这么多年 C(++) 以来第一次正式学 C (虽然其实在康奈尔学过一遍C++)","text":"大概是写这么多年 C(++) 以来第一次正式学 C (虽然其实在康奈尔学过一遍C++) Compiling C Program 12345# compile hello.c and name the executable as default (a.out)gcc hello.c# compile hello.c and name the output executable &quot;sayhello&quot;gcc hello.c -o sayhello We usually write return 0, this exit code 0 means EXIT_SUCCESS. Prototype Definition prototype - declare a function (write down its name) definition - define a function (write down its content) .h stand for “header” and it contains prototype of function .c stand for “code” and it contains definition of function Complex &amp; Custom Data Types struct 12345struct rect_t &#123; int left; ... &#125;; struct rect_t myRect; ### typedef The keyword typedef allows a programmer to create a new type. 12struct _rect_t &#123; ... &#125;;typedef struct _rect_t rect_t; Now we can create instance of the new type: (Note how this is different from a struct instance declaration) 123rect_t myRect;myRect.left = 1;... Strings char *strstr(const char *haystack, const char *needle): finds the first occurrence of the substring needle in the string haystack. sprintf(char *str, const char *format, ...): “prints out” formatted output to a string str, but instead of really printing them out, sprintf buffers the output to the string.(sprintf(str, \"Pi = %f\", 3.14); will set str to be Pi = 3.14) Dynamic Memory Allocation Consider the following program: 1234int * initArray(int howLarge) &#123; int myArray[howLarge]; for (int i = 0; i &lt; howLarge; i++) myArray[i] = i; return myArray; &#125; We cannot do this because the space allocated to myArray is only inside the scope of initArray and will be freed once we exit this function. So what we want is dynamic memory allocation so the memory will be allocated at a dynamic heap instead of the call stack. 123int *p = malloc(6 * sizeof(int)); // memory allocationp = realloc(p, 12 * sizeof(int)); // re-allocationfree(p); We can then rewrite the above function as: 1234int * initArray(int howLarge) &#123; int *myArray = malloc(howLarge * sizeof(int)); for (int i = 0; i &lt; howLarge; i++) myArray[i] = i; return myArray; &#125; Note the following when you use free: 12345678// you cannot free something on the stackint x = 3; int *p = &amp;x;free(p); // early termination// free can only be used to free address returned by mallocint *p = malloc(4*sizeof(int));p++;free(p); // early termination Debugging C in VSCode Install the extension “GDB Debugger - Beyond” Replace what’s in launch.json - configurations with the following codes: 123456789&quot;configurations&quot;: [ &#123; &quot;type&quot;: &quot;by-gdb&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;Launch(gdb)&quot;, &quot;program&quot;: &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;&quot;, &quot;cwd&quot;: &quot;$&#123;workspaceRoot&#125;&quot; &#125;]","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"更新archer主题 / 迁移Hexo博客","slug":"2021-08-29-更新archer主题--迁移Hexo博客","date":"2021-08-29T04:00:00.000Z","updated":"2022-04-23T07:35:52.000Z","comments":true,"path":"2021-08-29-更新archer主题--迁移Hexo博客/","permalink":"https://yao-lirong.github.io/blog/2021-08-29-%E6%9B%B4%E6%96%B0archer%E4%B8%BB%E9%A2%98--%E8%BF%81%E7%A7%BBHexo%E5%8D%9A%E5%AE%A2/","excerpt":"重要文件不多，全在下面列出来了，记得把它们迁移好就行","text":"重要文件不多，全在下面列出来了，记得把它们迁移好就行 123456789101112. # 「Hexo 根目录」├── source # 博客源文件├── themes│ └── archer # 「Archer 主题目录」│ ├── source # 渲染用源文件│ ├── assets│ └── favicon.ico # 网站缩略图标│ ├── avatar # 显示人物头像│ ├── intro # 网页头图│ └── _config.yml # Archer 主题配置文件│└── _config.yml # Hexo 配置文件 另外别忘记执行以下命令，安装必要插件 123npm install hexo-generator-json-content --savenpm install hexo-wordcount --savenpm install hexo-generator-feed --save","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Install and Configure Aria2 on WSL","slug":"2021-06-28-Install-and-Configure-Aria2-on-Linux","date":"2021-06-28T04:00:00.000Z","updated":"2022-06-08T19:34:24.000Z","comments":true,"path":"2021-06-28-Install-and-Configure-Aria2-on-Linux/","permalink":"https://yao-lirong.github.io/blog/2021-06-28-Install-and-Configure-Aria2-on-Linux/","excerpt":"","text":"Tutorial Install Aria2c: sudo apt install aria2c and a web-based GUI: AriaNg Create configuration files: 123mkdir ~/.aria2touch ~/.aria2/aria2.session #用于保存日志touch ~/.aria2/aria2.conf #创建配置文件 A template for “aria2.conf” can be downloaded from aria2c.com. You should change the dir field to be download path, change input-file and save-session to be the path of aria2.session. aria2.conf doesn’t support environment variable, so everything is at best written in absolute path. Run Aria2c with this configuration: aria2c --conf-path=/home/&lt;username&gt;/.aria2/aria2.conf. It will remember this as its configuration file and use it to start the service from now on. We can also add argument -D so Aria2c runs as daemon in the background. (Since the aria2c’s default configure file path is in ~/.aria2/, we don’t really need the --conf-path argument; but use it to specify a conf path if you put it somewhere else) 设置 aria2c 开机自动启动：编写脚本 myStartUp.sh 并放入 /etc/init.d/. (Remember to change its privilege to everyone) 12345#!/bin/bash#Short-Description: My Startup Servicesaria2c -D --conf-path=/home/&lt;username&gt;/.aria2/aria2.conf More on Start Service on WSL startup, refer to this answer Caution On WSL, when you start aria2 service the first time, note: (full command means the above command with --conf-path, but since our conf path is the same as default path, we can take ) use the full command aria2c --conf-path=/home/&lt;username&gt;/.aria2/aria2.conf and don’t start it in the background with -D. start it immediately after Windows is booted, before you open anything else or tweak anything Anything else than this full command or use this full command some time after Windows boot could cause problem. The problem is showing “Exception: [SocketCore.cc:312] errorCode=1 Failed to bind a socket, cause: Permission denied” even though no process is using port 6800. The reason for this problem is not clear. After staring aria2 once with the full command, we can shut it down and then start it in the background with aria2c -D --conf-path=/home/&lt;username&gt;/.config/aria2/aria2.conf. Now everything will work fine. Reference Linux中配置Aria2 RPC Server linux设置开机自启动 WSL 服务自动启动的正确方法","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"On Intelligence","slug":"2021-06-23-On-Intelligence","date":"2021-06-23T04:00:00.000Z","updated":"2022-06-08T19:33:42.000Z","comments":true,"path":"2021-06-23-On-Intelligence/","permalink":"https://yao-lirong.github.io/blog/2021-06-23-On-Intelligence/","excerpt":"Complexity is a symptom of confusion, not a cause.","text":"Complexity is a symptom of confusion, not a cause. 1 Artificial Intelligence 计算机学界的主流观点：不需要学习大脑 此观点的起始：Turing Test，即让人们认为它是智能，产生 intelligent behavior 更重要 the Chinese Room: 在中文屋中智能没有产生，作者认为 Understanding cannot be measured by external behavior; it is instead an internal metric of how the brain remembers things and uses its memories to make predictions. 但绝大多数的所谓”AI”和这里的中文屋和这一定义无任何相似之处 2 Neural Networks 一些可能已经过时的观点： Neural Network 没有考虑 feedback 和 time changing inputs Cognitive Scientist 虽然想记录大脑中的 feedback，但是迫于现有技术(fMRI)只能记录脑内活动的位置，无法记录连续的变化 3 The Human Brain Mind is the creation of the cells in the brain. The cortex is extremely flexible and that the inputs to the brain are just patterns. It doesn’t matter where the patterns come from; as long as they correlate over time in consistent ways, the brain can make sense of them. Function Hierarchy: 脑的每个功能部分都被划为 hierarchy，以输入的视觉为例 V1 (primary sensory areas): rawest, most basic level V2, V4, IT: concerned with more specialized or more abstract aspects association area: receive inputs from more than one sense 虽然是一个 hierarchy，但是实际上当我们从低层走向高层的过程中，information always flows in the opposite direction as well, and with more projections feeding back down the hierarchy than up. Uniformity of Cortex Parts: Mountcastle found that parts of cortex performing different function is very similar in appearance and structure. From there, he argues that all regions of the cortex are performing the same operation. The thing that makes the vision area visual and the motor area motoric is how the regions of cortex are connected to each other and to other parts of the central nervous system. Plasticity of Cortex: 我们发现如果大脑某个部分损坏，另一个部分可以接管它原先的人物，这佐证了 Mountcastle 的观点。另有一个 Thought Experiment：假设我们的大脑并不具有如此的可塑性，那么这就意味着我们的某个大脑部位是专门用来学习中文汉字的，但是对于生物进化来说，汉字进化地太快了，大脑根本不可能适应地这么快（或者外国人也可以迅速学中文亦能佐证这一观点） Similarity of Inputs into Brain: 不管视觉听觉还是什么输入，真正进了人体都是 Action Potentials. They are all the same - just patterns. 也用来佐证 Mountcastle 的观点。There are spatial and and temporal patterns: Spatial Patterns: coincident patterns in time; they are created when multiple receptors in the same sense organ are stimulated simultaneously Temporal Patterns: patterns entering your sensory organs are constantly changing over time 进一步给出了关于以上两点的例子：认为同时做出反应的假手是自己的手 / 镜头连舌头上的压感接收器，用舌头看东西 4 Memory 驳斥人脑比计算机更快，计算力更高 -&gt; 人脑能做到比计算机快是因为运行原理根本不同 -&gt; 引出本章主旨: the brain doesn’t “compute” the answers to problems; it retrieves the answers from memory. Four attributes of neocortical memory that are fundamentally different from computer memory: The neocortex stores sequences of patterns -&gt; predictions of future events The neocortex recalls patterns auto-associatively -&gt; recall memories appropriate for prediction The neocortex stores patterns in an invariant form -&gt; apply knowledge of past to new situations that are similar but not identical The neocortex stores patterns in a hierarchy. 接下来我们将详细介绍前三个特征并在第6章介绍最后一个特征 “阶层” Sequential Pattern: story is stored in your head in a sequential fashion and can only be recalled in the same sequence. You can’t remember the entire story at once. 一个有趣的观点: Truly random thoughts don’t exist. Memory recall almost always follows a pathway of association. Self-Associativity: The memory system can recall complete patterns when given only partial or distorted inputs. This is a result of Hebbian Learning: Firing together Wires together, so when only a part of the cell is activated, the whole group of cells will be activated. Invariant Representation: 人脑不是CD或硬盘，we don’t remember or recall things with complete fidelity. Instead, the brain remembers the important relationships in the world, independent of the details. 我们常用视觉来举例子：some set of the cells in the face recognition area remain active as long as your friend’s face is anywhere in your field of vision, regardless of its size, position, orientation, scale, and expression. This stability of cell firing is an invariant representation. 小引子导入下一章：下一章的主旨是人脑的主要功能就是 make predictions using memories，but given that the cortex stores invariant information, how can it make specific predictions? It combines knowledge of the invariant structure with the most recent details. 5 A New Framework of Intelligence Prediction is not just one of the things your brain does. It is the primary function of the neocortex, and the foundation of intelligence. The cortex is an organ of prediction. 这是作者本书中最基本的观点，也就是他所说的新的智能框架 (Memory-Prediction Framework of Intelligence) 。具体地来解释 Prediction 这个概念：Your brain makes low-level sensory predictions about what it expects to see, hear, and feel at every given moment, and it does so in parallel. All regions of your neocortex are simultaneously trying to predict what their next experience will be. “Prediction” means that the neurons involved in sensing your door become active in advance of them actually receiving sensory input. When the sensory input does arrive, it is compared with what was expected. Correct predictions result in understanding. Incorrect predictions result in confusion and prompt you to pay attention. 不局限于 sensory input，motor output 在我们的大脑中也是和 sensory input一样的 pattern, so neocortex can also remembers what behavior (pattern) leads to what sensory input (patter) and we can direct behavior to satisfy its predictions. 作者举了很多关于 prediction 的例子（预知乐曲的旋律，朋友的样子，你妈下一句话会说什么…）其中最有意思的例子应该是 “filling in”，即我们原来了解过的人脑的 “自动补全” 功能：人眼虽然有盲点但我们视觉没有盲点，自动将三个角补全成三角形，描绘出被树遮挡的大楼的样子，等等。Your visual cortex is drawing on memories of similar patterns and is making a continuous stream of predictions that fill in for any missing input. Behavior Cortex Intelligence 之间到底是个什么关系？ 从进化历程来看，cortex 起到什么作用？我们为什么要进化出 Cortex: in the beginning, the cortex served to make more efficient use of existing behaviors, not to create entirely new behaviors. 但是后来在进化过程中有了 new behavior？ Reptile: Keen senses and well-developed brains endowed them with complex behavior, but relatively rigid Mammals: Neocortex covering the old brain (reptile brain) Now sensory patterns are simultaneously fed into the neocortex and the old brain. The recalled memory is compared with the sensory input stream. It both “fills in” the current input and predicts what will be seen next. Humans: large front part of cortex for high-level planning and thought, so it could store more sophisticated types of memories and make predictions based on complex relationships motor cortex makes more connections with our muscles so cortex usurps motor control from other parts of the brain (old brain) and now the cortex can direct behavior to satisfy its predictions. 本部分也反驳了第一章中所谓的人工智能学者的 behavior determines intelligence 观点：早在 reptile 时期，动物就有了生存本能的 behavior，但是直到 cortex 出现，它们才有了 intelligence。而 cortex 的核心功能就是 prediction. To make predictions of future events, your neocortex has to store sequences of patterns. To recall the appropriate memories, it has to retrieve patterns by their similarity to past patterns (auto-associative recall). And, finally, memories have to be stored in an invariant form so that the knowledge of past events can be applied to new situations that are similar but not identical to the past. How the physical cortex accomplishes these tasks, plus a fuller exploration of its hierarchy, is the subject of the next chapter. 6 How the Cortex Works invariant representation: Light receptors in retina concentrate in fovea and sparse out in periphery, so retinal image relayed onto V1 is highly distorted. However, we don’t perceive any retinal pattern change at all. This is a result of invariant representation. In the course of spanning four cortical stages from retina to IT: cells in retina and V2 are rapidly changing, spatially specific, tiny-feature recognition cells. When we go to IT region, something magical happens and the cells become constantly firing, spatially nonspecific, object recognition cells. (They now fire when seeing a face, no matter it’s on the left or on the right) Integrating the Senses: 我们到现在为止都是讨论同一类型输入预测同一类型结果，实际上 association area 使得我们也可以预测其他类型的结果，比如视觉输入用来预测听觉，嗅觉等等的结果，亦可以用来指导动作 A New View of V1: 前文的模型有两个问题：仅当到了 IT 这一层时，我们奇迹般地获得了 invariant representation；大脑中大部分区域都是像 association area 一样得到多个输入，但我们的模型中好像 V2 只有 V1 一个输入，V4 只有 V2 一个。 To answer these questions, we propose a new model: V1, V2, V4 are not single cortical regions. Rather, each is a collection of many smaller subregions. V1 has largest number of little cortical areas. V2 has fewer, but larger subregions, each connecting to a number of V1’s subregions. Same for V4 and we have a single IT which has a bird’s eye view of the entire visual world. Now the job of any cortical region is to find out how its inputs are related, to memorize the sequence of correlations between them, and to use this memory to predict how the inputs will behave in the future. We can say each region of cortex forms invariant representation drawn from the input areas hierarchically below it. A Model of the World: 作者认为世界中 Every object is composed of a collection of smaller objects, and most objects are part of larger objects. In an analogous way, memories are stored in the hierarchical structure of the cortex. Time really matters and information flowing into the brain arrives as a sequence of patterns. 对于每个 cortical region，它识别出来这个 sequence，将其抽象成一个 name - a constant pattern of cell firing，并将这个名字发给他的上级。所以我们也可以说大脑存储的是 Sequence of Sequences. By collapsing predictable sequences into “named objects” at each region in our hierarchy, we achieve more and more stability the higher we go. This creates invariant representations. Sequences of Sequences: Two processes are at the essence of learning. Assume we are sorting out colored papers. bottom-up classification: deciding what color this paper is top-down sequence recognition: deciding which sequence are we reading in Notice these two processes help each other. 1. If you know the most likely sequence for this series of inputs, you will use this knowledge to decide how to classify the ambiguous input. 2. recognizing any sequence would be impossible if you hadn’t first classified each piece of paper. When we have finally recognized a color sequence, say “red red blue green”, we just pass this name to the next higher region; just like the colors to this region, the name is just a pattern to be combined with other inputs, classified, and then put into yet a higher-order sequence. The next higher up region doesn’t have to know what it means. What a Region of Cortex Looks Like: 我们说过每个 cortical region 有六层 (six layers 从上到下分别为 L1, L2, …, L6 不要跟视觉的 V1 V4 搞混) 但我们一般不把每一层看做人脑的基本单位，而是把 columns running perpendicular to the layer 看做 basic unit of computation in the cortex. 作者认为它是 basic unit of prediction. 我们接下来讨论 How cortical regions communicate with each other 共有三种方法： Upward Flow: Converge inputs from lower regions goes to the input layer of the next region through axons Downward Flow: Axons in layer 1 spread over long distances, so information flowing down the hierarchy from one column has the potential to activate many columns in the regions below it. Lateral Flow: L1 给 L4,5 发指令运动，L4,5 收到指令的同时，不仅向下给肌肉发放运动信号，也把这个消息告诉 thalamus，thalamus 过一会后会把这个消息重新传回给 L1。其中 thalamus 收到来自许多不同 L4, L5 的信息，然后再把这些信息一起返回给所有 L1 ，这样本 column 就知道知道周围其他人现在收到的信息。Column not only knows the sequence name (downward flow from above), but also where we are within the sequence (activity from other columns) How a Region of Cortex Works - The Details: How does a cortical region classifies inputs? It’s too complicated, we assume it does How does it learn sequences of patterns? Input from lower region -&gt; layer 4 fires -&gt; layers 2,3,5 fire -&gt; layer 1 fires to tell the region up some input has come. Fire together Wire together, so 2,3,5,1 wire together. 2,3,5 now can fire without a layer 4 input, so they learn to “anticipate” when they should fire based on firing of 1. Half of input to layer 1 comes from layer 5 in neighboring columns. This information represents what was happening moments before. It represents columns that were active prior to your column becoming active. The other half of the input to layer 1 comes from layer 6 cells in hierarchically higher regions. This information is more stationary. It represents the name of the sequence you are currently experiencing. Combining these two information, a prediction/sequence is formed. How does it form a constant “name” for a sequence? constant names = constant input to the next region during learned sequences = need to turn off the output of the layer 2 and layer 3 cells when a column predicts its activity, or, alternately, to make these cells active when the column can’t predict its activity. Layer 2 cell represent the name of the sequence and they stay on when we are within the sequence. Layer 3b cell represents don’t fire when our column successfully predicts its input but do fire when it doesn’t predict its activity. How does it make specific predictions? If you expect a fifth (prediction / invariant representation) and hear a D (specific input). In layer 2 we fire all intervals of fifth. In layer 4 we fire all intervals starting with D. The intersection between the two is our specific prediction. Flowing Up and Flowing Down: 上层给下层 prediction 当下层得到的输入与 prediction 不符 (unexpected)，我们将此特征传导给更上一层，直到 some higher region can interpret it as part of its normal sequence of events. That higher region generates a new prediction and propagates it down Can Feedback Really Do that? Feedback synapses are all far away from cell’s body, so it’s doubted whether the feedback currents can really make a difference. 但是新研究发现离得远的 synapse 可能有其他特殊的效果（并不确切证实） How the Cortex Learns: 比如我们有1,2,3层，一开始单个文字在第3层，随着我们持续学习和不断练习单个文字移到了第2层，相对的，我们在第3层习得短语这个 pattern。This ensures that we free up the top for learning more subtle, more complex relationships. 这也是我们变得更熟练的原因。 The Hippocampus: 我们常认为海马体是生成新记忆的中心，在作者的模型中，Hippocampus is the top region of neocortex. 我们刚刚说 unexpected input 被传输给上层，so if something gets to the top of the cortical pyramid, it is the information that can’t be understood by previous experience, the input that is truly new and unexpected. That’s what stored in Hippocampus, but it won’t be stored forever. It’s either transferred down to the cortex (长期记忆) or eventually lost (遗忘) 所谓人在壮中年时对”新事物”的记忆没有那么好实际上是因为这些”新”的东西实际上早已在以前的生活中出现过，所以人对第一次记忆特别深刻，对之后的类似事物就没那么好记性。（它竟然和 How the Cortex Learns 这很扯的一节联起来了） An Alternative Path up the Hierarchy: 这里要介绍的是从 Layer5 -&gt; thalamus 的路径。这条路径可开可关，它要么被上层激活打开，要么被下层的 unexpected input 激活。我们认为这条路径代表注意力，两种开启方式分别对应主动关注(pay attention)，以及因为奇怪的现象而被动关注 (attention is caught) Closing Thoughts: 分享了作者从零想结构写代码最后竟然能跑的例子，但是相对的如果别人只给你看一堆代码结构规划，你可能会怀疑这东西到底能不能跑，类比到脑结构中，怀疑的原因是 it is because our intuitive sense of the capacity of the cortex and the power of its hierarchical structure is inadequate. 7 Consciousness and Creativity Animals and Human Intelligence: Memory and Prediction are the core of “Intelligence” and they are used by all livings. There is just a continuum of methods and sophistication in how they do it. One-cell animal: They used DNA as the medium for memory. Individuals could not learn and adapt within their lifetimes. They could only pass on the DNA-based memory of the world to their offspring through their genes. Modifiable Nervous System: An individual could now learn about the structure of its world and adapt its behavior accordingly within its lifetime. But an individual still could not communicate this knowledge to its offspring other than by direct observation. Neocortex was also created at this time. Human Intelligence: It begins with the invention of language and the expansion of our large neocortex. The more important is language. We humans can learn a lot of the structure of the world within our lifetimes, and we can effectively communicate this to many other humans via language. What is Creativity? Recall that we make predictions by combining the invariant memory recall of what should happen next with the details pertaining to this moment in time. All cortical predictions are predictions by analogy. We are being creative when our memory-prediction system operates at a higher level of abstraction, when it makes uncommon predictions, using uncommon analogies. 注意 GEB 中也提到说 analogy 是智慧的核心 What is Consciousness? 有人认为 consciousness/mind 在身体之外，但是实际上它就在脑中。Your thoughts, which are located in the brain, are physically separate from the body and the rest of the world. Mind is independent of body, but not of brain. 8 The Future of Intelligence Because I have been immersed in the neuroscience and computer fields for over two decades, perhaps my brain has built a high-level model of how technological and scientific change occurs, and that model predicts rapid progress. Now is the turning point. General Direction of Intelligent Machine: Our intelligent machine may have a set of senses that differ from a human’s. attach to these senses a hierarchical memory system that works on the same principles as the cortex. We will then have to train the memory system much as we teach children. Over repetitive training sessions, our intelligent machine will build a model of its world as seen through its senses. The intelligent machine must learn via observation of its world. Once our intelligent machine has created a model of its world, it can then see analogies to past experiences, make predictions of future events. 这个智能机器的整体运作方法和大脑相同，但是它并不需要与大脑长得相似或得到和大脑相同的输入，它只需要复合结构的，能够用来作“预测”的输入即可。What makes it intelligent is that it can understand and interact with its world via a hierarchical memory model and can think about its world in a way analogous to how you and I think about our world. Ethical Problems? No. The strongest applications of intelligent machines will be where the human intellect has difficulty, areas in which our senses are inadequate, or in activities we find boring. In general, these activities have little emotional content. In the following areas, Intelligent Machines will exceed we humans: Speed: Transistor switch is much faster than human brain’s electrical signals. Capacity: we can add capacity to machine’s mind by doing the followings (these are also what we do in DL/ML) Adding depth to the hierarchy will lead to deeper understanding: the ability to see higher-order patterns. Enlarging the capacity within regions will allow the machine to remember more details, or perceive with greater acuity. Adding new senses and sensory hierarchies permits the device to construct better models of the world Replicability: we humans learn knowledge and form our own model of the world rather slowly. However, an intelligent machine need not undergo this long learning curve, since chips and other storage can be replicated endlessly and the contents transferred easily. Sensory Systems: Input patterns to the machine don’t have to be analogous to animal senses, or even to derive from the real world at all. In fact, the author suspects that out inability to tackle issue may be related to a mismatch between the human senses and the physical phenomena we want to understand. Intelligent machines can have custom senses more sensitive than our own, or senses that are distributed, or senses for very small phenomena. They might think in three, four, or more dimensions. Appendix: The Thousand Brain Theory Notes from Microsoft Research - The Thousand Brains Theory by Jeff Hawkins Local Cortical Circuit Inside a local cortical circuit, neurons are organized in layers. Most connections go vertically across the layers; limited connections go horizontally within layer. Recent find: all layers have a motor output. So it’s always sensorimotor input, no pure sensory input. Vernon Mountcastle: neocortex is remarkably uniform in appearance and structure because they are actually performing the same basic intrinsic function. A cortical column is the unit of replication. If you understand one of it, you understand the whole brain. Layer 2,3 - object Layer 4 - main input layer Layer 6 - location relative to the object L6 sends information to L4, L4 processes these information with its own other input. Over time it forms a representation of what the object itself is in layer L2,3. On top of that, if we have multiple cortical involved (imagine multiple fingers touching the cup instead of only one), we can instantly build a mental image of the cup by the connections across cortical units happened in L2,3. This is like a voting mechanism where each finger has a guess of its feeling and they settle what the object really is by talking to each other. Building a Reference Map A reference map is the sense of relative location as we are touching the cup Contrast to the classical view, the vast majority of connections between cortical regions are not hieratical at all. Hypothesis: the grid cells in entorhinal cortex also exist in every cortical column of every neocortex region. They don’t create reference frames for location but reference frames for the objects we interact (the cup). In the classical view, we have a hierarchy in our neocortex. The real structure is similar, 但我们并不是 杯柄 -&gt; 杯身 -&gt; 整个杯子 这种真正的阶梯式建模，而是每个“层级”都形成一个自己的杯子模型，这些模型并不相同. This model allows all models to “vote”. Everyone tries to guess what’s going on.","categories":[],"tags":[{"name":"Book","slug":"Book","permalink":"https://yao-lirong.github.io/blog/tags/Book/"}]},{"title":"TensorFlow 1.x Manual","slug":"2021-05-28-Introduction-to-TensorFlow-1.x","date":"2021-05-28T04:00:00.000Z","updated":"2022-06-08T19:53:42.000Z","comments":true,"path":"2021-05-28-Introduction-to-TensorFlow-1.x/","permalink":"https://yao-lirong.github.io/blog/2021-05-28-Introduction-to-TensorFlow-1.x/","excerpt":"海尔实习期间记录下的 TensorFlow 笔记","text":"海尔实习期间记录下的 TensorFlow 笔记 Basic Notion Graph: often refers to Computation Graph, which describes how to compute the output Eager execution: evaluates operations immediately, without building graphs Enabling eager execution changes how TensorFlow operations behave—now they immediately evaluate and return their values to Python. tf.Tensorobjects reference concrete values instead of symbolic handles to nodes in a computational graph. Since there isn’t a computational graph to build and run later in a session, it’s easy to inspect results using print() or a debugger. Evaluating, printing, and checking tensor values does not break the flow for computing gradients. Operation: 图中的节点, takes Tensor object as input, and produces Tensor objects as output Tensor: multi-dimensional arrays with a uniform type (called dtype), 包含一个 n 维的数组或列表. 一个静态类型 rank, 和 一个 shape. It does not hold the values of that operation’s output, but instead provides a means of computing those values. It is a symbolic handle of input/output of Operation. 图上操作间传递的数据都是 Tensor: A Tensor can be passed as an input to another Operation. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire Graph that represents a large, multi-step computation. Session: launch the computation of a graph InteractiveSession: a better graph runner that allows you to compute each operation step by step instead of only giving out the final result, as in Session 12345678910# Build a dataflow graph.a = tf.constant([[1.0, 2.0], [3.0, 4.0]])b = tf.constant([[1.0, 1.0], [0.0, 1.0]])c = tf.matmul(a, b)# Construct a `Session` to execute the graph.sess = tf.compat.v1.Session()# Execute the graph and store the value that `e` represents in `result`.result = sess.run(e) a, b, c are Tensor here. c = tf.matmul(a, b) creates an Operation of type “MatMul” (Matrix Multiplication) that takes tensors a and b as input, and produces c as output. Variable: represent shared, persistent state your program manipulates (parameters of the model) it is a tf.Tensor whose value can be changed by running ops on it Placeholder: a tensor whose value will later be fed. Operations on Tensors tf.reduce_xxx(t, axis=i): If we have a tensor t of dimension $d_1 d_2 … d_n $, apply r = reduce_xxx(t, axis = i), Each entry along axis i will be collapsed into a single entry, so r will have dimension $d_1 d_2 … d_{i-1} d_{i+1} … d_n $: 1234567891011121314151617a=np.random.randint(1,10,(2,3,4))&#x27;&#x27;&#x27;2 arrays of dimension 3 X 4[[[8 5 7 1] [9 7 2 2] [7 7 4 6]] [[7 7 8 4] [7 4 3 6] [5 3 2 8]]]&#x27;&#x27;&#x27;sess = tf.Session()with sess.as_default(): r = (tf.reduce_sum(a, axis=1)).eval() # reduce along axis of length 3&#x27;&#x27;&#x27;[[24 19 13 9] [19 14 13 18]]&#x27;&#x27;&#x27; tf.reshape(t, list): Reorder all the elements in t so that we have a new dimension in r: d1′ = list[0], d2′ = list[1], ... If we have d′i = −1 as one of the dimension, $d_i’ = $, so 1r = tf.reshape(a, [-1,2,2]).eval() # r will havee shape (6, 2, 2) tf.concat([t1, t2, ...], axis = i): pile all the arrays along axis i. These arrays must have the same length along the other axis. In the result, only the length along axis i will increase, the length of other axis remain the same. tf.tile(t, [m1,m2,...]): multiple axis i with mi, so the result tensor dimension is (d1 × m1, d2 × m2, ...) Debug with Tensorboard tf.summary: Follow the official guide tf.estimator: Specify model_dir when initializing your estimator. Everything about the trained model will be stored in this directory, including event files logging training process. Reference Utility Sometimes we encounter module 'tensorflow' has no attribute ... because TensorFlow changed/refactored its function name. We can use this list to manually update all changed names or directly use this script.","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Look Back on Cornell 21SP","slug":"2021-05-24-Look-Back-on-Cornell-21SP","date":"2021-05-24T04:00:00.000Z","updated":"2023-02-23T04:15:40.000Z","comments":true,"path":"2021-05-24-Look-Back-on-Cornell-21SP/","permalink":"https://yao-lirong.github.io/blog/2021-05-24-Look-Back-on-Cornell-21SP/","excerpt":"生年不满百 常怀千岁忧 昼短苦夜长 何不秉烛游 为乐当及时 何能待来兹 愚者爱惜费 但为后世嗤 仙人王子乔 难可与等期","text":"生年不满百 常怀千岁忧 昼短苦夜长 何不秉烛游 为乐当及时 何能待来兹 愚者爱惜费 但为后世嗤 仙人王子乔 难可与等期 这学期是放假的学期，突如其来的COVID-19让我意识到人生苦短，为未来奋斗不如及时享乐，活在当下。和家里人一起过个年，多看看动漫看看电影，自己爱干点啥干点啥吧。 没选任何CS专业课，不过TA了4820。唯一一节类似于专业课的是 MATH3360，其他都是随便上上，主要时间花费在自己锻炼锻炼身体和学日语学乐理上。想起来锻炼身体的原因是去年去清华之前体检医生说我心率过缓，让我回忆起体检之前还一个人在上海很烦躁，每天熬夜到两三点打游戏早上九点起，到了晚上就开始心慌。这两件事加起来我突然觉得自己会不会过两年突然猝死了，为了防止猝死这学期运动运动，结果学期末再去体检的时候发现心率还是那样，估计天生心脏就这个样，和我运不运动没啥卵关系。 JAPAN2202 Intermediate Japanese II もしもこのメールを読めたなら、この瞬間、あなたの幸せは2倍にも３倍にもなります。 なぜならあなたには、あなたのことを思ってこれを送ってくれた誰かがいるだけでなく、文字も読めるからです。 けれどなによりあなたは生きているからです。 ​ ーー「世界がもし１００人の村だったら」 因为本人经典眼高手低，托上学期在清华认识的Leo帮我做了 placement exam 帮我考到了 2000 级日语课，以为自己作为一个中国人，开学前恶补一下就能赶上课程进度。于是开课前自己速度看完了《大家的日本语》初级两册，然后把《上級へのとびら》他们讲过的前两课也自己看了。本来开学前还是比较有信心的，结果真要去上课输出能力还是太差了，而且老师一点到我名口吃地说不出话来，真的本来大一这毛病都快好了，zoom一年给我干回原型甚至加重了。最后就是这门课要求时间太长了，一天我可能要花6h以上在它上面，曲线有点陡峭，最终选择了drop。可能从2201开始跟会更好吗？但是2201的那些知识点又太基础了，自己就是在一个上不去下不来的尴尬地方，其实最好就是我当时20FA之前把《大家的日本语》两册都看完，说实话都怪我妈逼我去的那个实习，把我计划也打乱了，身体也搞坏了。 退课以后自己学得也挺起劲的，一个多月把《上級へのとびら》这本书自己看完了，知识点也算大体掌握，不明白的找了个一对一上了两节课也都搞明白了。词汇量不咋地但是觉得自己听力阅读都还凑合。自己看佐贺第二季和葡萄原来的生肉都算是能看的地步，总得来说这学期日语还是很有收获的。 其他课程 以下几门课除数学外我全部摆烂SU，最终成绩都在尴尬的 A- 左右，我说实话，早知道我就直接不交作业了，摆了但又没完全摆，学了但又没完全学。 BIONB2220 Intro to Neuroscience: 算是这几门里我唯一一门用心学的课，挺不错的神经科学入门，每一章都是由对应领域的研究人员来上的 ARKEO2661 Ancient Ships and Seafaring: 折磨，我本以为能挺有意思的，但是这种你只是稍微感兴趣的东西被当成课和考试的东西来上真的很折磨，考试都是选择题，但是第一次考得不好让我决定SU这门课了。结果第二次近乎考了满分，主要是第二次学聪明了，直接把所有的slide合并成一个大的pdf，搜一下答案基本就有了，唉要是第一次就知道的话，这门课也能算个GPA booster PSYCH2150 Psychology of Language: 折磨，同上，真的不能选这种只是稍微感兴趣的课上。我第一次发觉语言学这么无聊这么扯淡。 另外 MATH3360，纯粹是为了凑一个 Algebra Requirement 上的课，上学期前我还考虑 double major math，让4710给我干沉默以后我真的对数学院说拜拜了，但是上完这门课再随便选一门我就能拿个 Math Minor，但是转念一想，我拿个minor有啥用呢？好蠢，这学期真的不该上这门课。 在家的这个学期也自学了一些乐理，通过 Functional Ear Trainer 这个软件掌握了大调的通过根音找音，总得来说对于小学时候被强逼着照谱子弹琴快乐不少。","categories":[],"tags":[{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"}]},{"title":"Setting up a Server","slug":"2021-05-15-Setting-up-a-Server","date":"2021-05-15T04:00:00.000Z","updated":"2021-08-28T21:33:50.000Z","comments":true,"path":"2021-05-15-Setting-up-a-Server/","permalink":"https://yao-lirong.github.io/blog/2021-05-15-Setting-up-a-Server/","excerpt":"","text":"Initial Server Setup Logging In We chose “use ssh keys to log in” when creating the server, so we need to first get our root password by “reset root password”. Next time when you log in, you will be prompted to change password. log into server ssh root@your_server_ip. Passphrase set when creating ssh keys are needed. Adding User Check currently available users with cat /etc/passwd Add a new user with adduser &lt;username&gt; If a wrong user is added accidentally, delete it with deluser &lt;username&gt; Grant this newly added user sudo privilege by “appending” it to sudo “Group” usermod -aG sudo &lt;username&gt; Logging in as New User We can log in with the following two ways: Enabling ssh password login: go to /etc/ssh/sshd_config and change PasswordAuthentication no to PasswordAuthentication yes. Restart the service after editing sudo service ssh restart. Continue use SSH Authentication: We want to copy the keys with the correct ownership and permissions, so use rsync --archive --chown=sammy:sammy ~/.ssh /home/sammy (Replace “sammy” with your username) explains what –archive does --chown=USER:GROUP forces all files to be owned by USER with group GROUP be sure that the source directory (~/.ssh) does not include a trailing slash (check to make sure you are not using ~/.ssh/) If you accidentally add a trailing slash to the command, rsync will copy the contents of the root account’s ~/.ssh directory to the sudo user’s home directory instead of copying the entire ~/.ssh directory structure. We can now log in as the newly added user &lt;username&gt;@your_server_ip Setting up Firewall Before everything, you should check IPV6 is enabled by going to nano /etc/default/ufw and check IPV6=yes. Set up a default profile to deny all incoming and allow all outgoing. 12ufw default deny incomingufw default allow outgoing This is enough for a PC but not enough for a server. We would need to allow ssh, HTTP, and HTTPS. 123ufw allow sshufw allow httpufw allow https The Firewall will then allow traffic from the default ports specified by these applications. For example, ssh uses port 22, so ufw allow ssh is equivalent to ufw allow 22. Enable and check firewall’s status: 12ufw enableufw status verbose For more commands related to UFW, check UFW Essentials. Install PHP 安装php，可用想要安装的版本替换 “7.4”: apt install php7.4-cli 安装所需要的插件，可以通过 aptitude search php7.4 |grep -i mysql 来寻找对应的插件（可用自己需要的 mbstring, GD, 等替换 mysql） conf.d - individual site configuration stored here","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Tsinghua DSA 作业总结 (3)","slug":"2021-02-11-Tsinghua-DSA-作业总结-(3)","date":"2021-02-11T05:00:00.000Z","updated":"2022-06-08T19:33:26.000Z","comments":true,"path":"2021-02-11-Tsinghua-DSA-作业总结-(3)/","permalink":"https://yao-lirong.github.io/blog/2021-02-11-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(3)/","excerpt":"CST数据结构（2020秋）PA3","text":"CST数据结构（2020秋）PA3 3.1 Not Found 算法 要找二进制字符串 A 中最短的未出现过的子串 B，我们先考虑一个比较长的子串，其长度为 24。 注意到 A 的长度最长为 16777216 = 2^24。因为还要掐头去尾，所以 A 中长度为 24 的子串的总数必定小于 2^24 个，而长度为 24 的字符串总共有 2^24 种，所以 A 中必定有某个长度为 24 的字符串是不存在的。 我们用 bitmap 边读入，边记录下所有出现过的长为 24 的子串。这个 bitmap 只存长度为 24 的子串，我们叫它 bitmap24。读入完成后，注意到任何一个在 A 中出现的长为 23 的子串必定是某一 24 子串掐头或去尾得到的，于是我们遍历所有在 24 子串，对他们掐头去尾，将得到的两个结果存入 bitmap23 中，如此做直到 bitmap1 存完。 最后我们从长度 24 开始遍历，找到第一个长度 n 使得所有长度为 n 的子串都在 A 中出现了，那么所要找的“最短未出现子串” B 必然有长度 n+1，我们只需要再遍历一遍 bitmap(n+1) 找到第一个不存在的字符串即可 细节 读入字符串的时候当总长度达到 24 以后，我们就要读一个新的弃一个旧的，因为根据题目分析 B 最长也就是 24 一个 int 是 4 byte = 32 bit = 2^5 bit，所以 bitmap24 需要 224/25 = 219 个 int，bitmap1 … bitmap 5 各自仅需 1个 int 因为我们是将二进制字符串用 int 方式存在 bitmap 中，如果这个字符串有 leading 0s, 它们在输出时会被忽略掉，所以我们需要根据 bitmap-n 这个长度 n 来补全 leading 0s 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#include&lt;cstdio&gt;#include&lt;iostream&gt;using namespace std;// int_size[i] is the number of ints needed to store all strings of length iconst int int_size[25] = &#123; 1, // there should be no bitmap for string of length 0, // but we give it 1 to make the whole program consistent 1, 1, 1, 1, 1, // 2^1 2^2 2^3 2^4 2^5 each only needs one int 2, 4, 8, 16, 32, // 2^6, ... 10 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288 &#125;;// ones[i] is 2^i - 1const int ones[25] = &#123; 0, 1, 3, 7, 15, 31, 63, 127, 255, 511, //10 1023, 2047, 4095, 8191, 16383, 32767, 65535, 131071, 262143, 524287, //20 1048575, 2097151, 4194303, 8388607, 16777215&#125;;// bitmap[i] is the bitmap for binary strings of length iunsigned int *bitmap[25];// make our bitmap contain a binary string x of length nvoid setbit(int n, unsigned int x);// returns true if our bitmap contains a binary string x of length nbool checkbit(int n, unsigned int x);// print a binary string x of length nvoid print_binary(int n, unsigned int x);int main()&#123; for (int i = 0; i &lt;= 24; i++) &#123; bitmap[i] = new unsigned int[int_size[i]]; for (int j = 0; j &lt; int_size[i]; j++) bitmap[i][j] = 0; &#125; // n is the total number of characters we read in // s is the string at our sliding window // c is the character we just read in // input is 0 if c is &#x27;0&#x27;, is 1 if c is &#x27;1&#x27; unsigned int n = 0, s = 0, input = 0; char c = getchar(); n = 1; // read till nothing more to read or the string is 24 char long for (; c!=&#x27;\\n&#x27; &amp;&amp; n&lt;24; c = getchar()) &#123; input = c - &#x27;0&#x27;; s = (s &lt;&lt; 1) | input; n += 1; &#125; // n is the number of characters read in, including the line feed // n-1 is the actual length of s setbit(n-1, s); // we probably halted because n==24, so we read in 24 valid 0 1 characters // If so, there can be more to be read, so we try to read more but keep the string at 24 characters long // skip this loop if the string is finished with a space for (; c!=&#x27;\\n&#x27;; c = getchar()) &#123; input = c - &#x27;0&#x27;; s = (s &lt;&lt; 1) | input; s = s &amp; 0xFFFFFF; // keeps only the first 24 characters setbit(24, s); n += 1; &#125; n -= 1; // n is the number of characters read in, including the line feed // delete 1 to obtain the actual string length // len is the length of answer string // ans is the binary string in int representation // full is true if all the strings of length i is in our bitmap unsigned int len = 0, ans = 0; bool full = false; for (int i = n&gt;24 ? 24 : n; i&gt;0 &amp;&amp; !full; i--) &#123; full = true; // we assume this level is full for (int j = 0; j &lt; ones[i] + 1; j++) &#123; // iterate all strings 0 ~ 2^i if (checkbit(i, j)) &#123; // percolate down to its substring setbit(i - 1, j &gt;&gt; 1); setbit(i - 1, j &amp; ones[i - 1]); &#125; else if (full) &#123; // current substring doesn&#x27;t exist, and all the previous substrings do exist // so this is the FIRST substring that doesn&#x27;t exist ans = j; len = i; full = false; &#125; &#125; &#125; print_binary(len, ans);&#125;void setbit(int n, unsigned int x) &#123; // x%32 就是存储 x 的 bit，即从左向右 x%32 个位置的那个 bit // 但由于计算机中存储数是从右向左存的，我们需要让 1 从右端开始移动 ( 31- x%32 ) 个位置才可以 // 这样我们得到一个第 x%32 为1，其他位为 0 的二进制数，通过 or 与原 bitmap 储值合并 // bitmap[x/32] |= (1&lt;&lt;(31 - x%32)); bitmap[n][x&gt;&gt;5] |= (1&lt;&lt;(31 - x&amp;31));&#125;;bool checkbit(int n, unsigned int x) &#123; // bitmap[x/32] &amp; (1&lt;&lt;(31 - x%32)) is determined solely by the x%32 bit of this int chunk // If that bit is 0, the whole expression is 0 // If that bit is 1, the whole expression is greater than 1 and thus evaluate to true // return bitmap[x/32] &amp; (1&lt;&lt;(31 - x%32)); return bitmap[n][x&gt;&gt;5] &amp; (1&lt;&lt;(31 - x&amp;31));&#125;void print_binary(int n, unsigned int x) &#123; // int 是从右往左存的，且我们只能访问最右边的 least-significant digit // 我们要从左往右打印，只能将从右向左的每个 bit 顺序存起来再倒序打印 int ans[25]; int m = 0; while (x != 0) &#123; ans[m] = x &amp; 1; x = x &gt;&gt; 1; m++; &#125; // 补全 leading 0s for (int i = m; i &lt; n; i++) &#123; ans[i] = 0; &#125; for (int i = n - 1; i &gt;= 0; i--) &#123; printf(&quot;%d&quot;,ans[i]); &#125; printf(&quot;\\n&quot;);&#125; 复杂度分析 读入长度为 n 的字符串，耗时 O(n) 如果 n &gt;24 则从 bitmap24 开始遍历，如果 n&lt;=24 则从 bitmap(n) 开始遍历，耗时 O(2min(24, n)) 当 n 达到 2^24 级别时，整体复杂度还是 O(n) Reference 用C++实现bitmap 3.3 Kth 算法 题目要求找出 a,b,c 三个数组对应的三元数对中和为第 k 大的那个三元数对，观察到如果 a,b,c 是有序数对，那么必有 a[i]+b[j]+c[k] &lt; a[i+1]+b[j]+c[k], a[i]+b[j]+c[k] &lt; a[i]+b[j+1]+c[k], a[i]+b[j]+c[k] &lt; a[i]+b[j]+c[k+1]. 于是，我们维护一个优先队列，每次出队 (i,j,k) 就入队 (i+1,j,k) (i,j+1,k) (i,j,k+1)。如此做 k 次，出队的就是我们要找的三元对。我们现在将“找第 k 大”转变成了一个三维图的遍历问题。 实现中要注意的是不能让同一个点多次入队，我们可以开一个 vis 数组，但是每个数组最多有 500000 个元素，三维 vis 数组空间绝对不够。于是我们想一种遍历顺序，使得每个点只被遍历一次。首先考虑最简单的一维，单个的 x 轴，就是不停地遍历下一个而已 i, i+1, i+2, ... ；扩展到二维其实就是多个一维情况，我们通过 (0,j), (1,j), ... (i-1,j) 到达 (i,j) 那我们如何到达 (0,j) 呢？通过 (0,0) 的一维扩张，也就是说，当 x 轴为 0 时，我们既向 x 方向扩张，也向 y 方向扩张，而当 x 轴不为 0 时，我们只向 x 方向扩张。 对于三维情况，想象 x,y,z 正方向为右，前，下。则在任意时刻，我们都向 x 扩张；仅当 x=0 时，我们向 y 方向扩张；仅当 x=0 且 y=0 时，我们向 z 方向扩张。并且由于我们根据优先级选取每一次的扩张边界，我们一定也是优先级最高的先被找到。 细节 Heap 的实现：sink 时首先判断孩子存不存在（孩子坐标与元素总数比较）如果左孩子存在且“右孩子不存在，或左孩子优先级比右孩子高”，则与左孩子互换；如果右孩子存在且右孩子优先级更高，则与右孩子互换 三维的遍历顺序：尝试向 y 方向扩张时，如果 x!=0，跳过此次扩张；尝试向 z 方向扩张时，如果 x!=0 || y!=0，跳过此次扩张 数组的排序：在本题提供接口中，我们无法直接访问数组 a,b,c 中的元素，所以我们自己开另外三个数组 s,u,t 其中 s[i] 表示 a 中第 i 大的元素所对应在 a 中的位置。即 s,u,t 存 1…n， 代表 a,b,c 中的下标。为取得 s，我们使用 sort(s,n) 但比较器用的却是 a 的比较器 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146#include &quot;kth.h&quot;#define _CRT_SECURE_NO_WARNINGS#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;const int N = 500007, K = 2000003;int dir[3][3] = &#123; &#123;1,0,0&#125;, &#123;0,1,0&#125;, &#123;0,0,1&#125; &#125;;// sort x-axis by only comparing sums along x-axisint sortx_cmp(const void* a, const void* b) &#123; if (compare(*(int*)a, 1, 1, *(int*)b, 1, 1) == 1) return -1; else if (compare(*(int*)b, 1, 1, *(int*)a, 1, 1) == 1) return 1; else return 0;&#125;int sorty_cmp(const void* a, const void* b) &#123; if (compare(1, *(int*)a, 1, 1, *(int*)b, 1) == 1) return -1; else if (compare(1, *(int*)b, 1, 1, *(int*)a, 1) == 1) return 1; else return 0;&#125;int sortz_cmp(const void* a, const void* b) &#123; if (compare(1, 1, *(int*)a, 1, 1, *(int*)b) == 1) return -1; else if (compare(1, 1, *(int*)b, 1, 1, *(int*)a) == 1) return 1; else return 0;&#125;struct triple &#123; int x, y, z; triple() &#123; x = 0; y = 0; z = 0; &#125;; triple(int a, int b, int c) &#123; x = a; y = b; z = c; &#125; triple(const triple&amp; from) &#123; this-&gt;x = from.x; this-&gt;y = from.y; this-&gt;z = from.z; &#125;&#125;;// myPQ is my priority queuetriple myPQ[K*2];const triple INF = triple(10e7, 10e7, 10e7);// a, b, c is the array given in problem int a[N], b[N], c[N];inline bool operator&lt;(const triple&amp; t1, const triple&amp; t2) &#123; return compare(a[t1.x], b[t1.y], c[t1.z], a[t2.x], b[t2.y], c[t2.z]);&#125;inline bool operator&gt;(const triple&amp; t1, const triple&amp; t2) &#123; return compare(a[t2.x], b[t2.y], c[t2.z], a[t1.x], b[t1.y], c[t1.z]);&#125;// refactorred PQ that only uses strictly greater/lesser to be consistent with compare functionclass PriorityQueue &#123; int n = 0; triple* a = myPQ;public: void add(triple x) &#123; a[++n] = x; swim(n); &#125; triple extract() &#123; if (n == 0) throw &quot;Nothing to extract&quot;; triple result = a[1]; swap(a[1], a[n]); a[n--] = INF; sink(1); return result; &#125; bool isEmpty() &#123; return n == 0; &#125; void print() &#123; for (int i = 1; i &lt;= n; i++) &#123; printf(&quot;%d in heap: (%d, %d, %d)\\n&quot;, i, ::a[a[i].x], b[a[i].y], c[a[i].z]); &#125; &#125;private: void swim(int i) &#123; while (i &gt; 1 &amp;&amp; !(a[i / 2] &lt; a[i])) &#123; swap(a[i / 2], a[i]); i = i / 2; &#125; &#125; void sink(int i) &#123; int l = i * 2, r = i * 2 + 1; while ((l &lt;= n &amp;&amp; !(a[i] &lt; a[l])) || (r &lt;= n &amp;&amp; !(a[i] &lt; a[r]))) &#123; if (l &lt;= n &amp;&amp; (r &gt; n || !(a[l] &gt; a[r]))) &#123; // l is in the heap and (r is not in the heap, or l is the better choice compared to r) swap(a[i], a[l]); i = l; l = i * 2; r = i * 2 + 1; continue; &#125; else if (a[l] &gt; a[r] &amp;&amp; r &lt;= n) &#123; swap(a[i], a[r]); i = r; l = i * 2; r = i * 2 + 1; continue; &#125; else return; &#125; &#125;&#125;;void get_kth(int n, int k, int *x, int *y, int *z) &#123; for (int i = 0; i &lt;= n; i++) &#123; a[i] = b[i] = c[i] = i; &#125; qsort(a+1, n, sizeof(int), sortx_cmp); qsort(b+1, n, sizeof(int), sorty_cmp); qsort(c+1, n, sizeof(int), sortz_cmp); PriorityQueue q; q.add(triple(1, 1, 1)); for (int i = 1; i &lt; k; i++) &#123; // extract k-1 triples triple now = q.extract(); int nowx = now.x, nowy = now.y, nowz = now.z; for (int j = 0; j &lt; 3; j++) &#123; int nextx = nowx + dir[j][0], nexty = nowy + dir[j][1], nextz = nowz + dir[j][2]; if (nextx &gt; n || nexty &gt; n || nextz &gt; n) continue; if ((j == 1 &amp;&amp; nowx != 1) || (j == 2 &amp;&amp; (nowx != 1 || nowy != 1))) continue; q.add(triple(nextx, nexty, nextz)); &#125; &#125; triple result = q.extract(); *x = a[result.x]; *y = b[result.y]; *z = c[result.z];&#125; 复杂度分析 共有三个数组，一个数组中有 n 个元素，找大小为 k 对的三元数对。首先对三个数组进行排序 O(nlogn)，每有一个数对出优先队列，就有最多三个入优先队列，共操作 k 次，每次操作 O(logk)，总共 O(klogk)。总时间 O(nlogn + klogk) 3.4 Component 算法 堆的合并 左偏树 题目的询问永远是某一联通分量中第 k 大的点的权值，k 是一个常数。第 k 大又可以看做前 k 个最大元素中最小的元素，即如果我们维护一个小根堆，使它恒有 k 个元素（n&lt;k 时输出 -1，n&gt;k 时弹出 n-k 次最小的元素）那么这 k 个元素必然是连通块中前 k 大的元素，堆顶元素就是我们的询问。 当加入的新边 (u,v) 联通两个不曾联通的连通块时，对应的两个堆必须合并。支持快速合并操作的优先队列，我们选择左式堆。(u,v) 将块联通，实际上是将其所在的堆合并起来，我们必须能够高效找到 (u,v) 所属哪个堆，即其所属堆的根是谁，使用并查集存储这个信息。 细节 每个节点都有编号，我们不用传统的 class 建优先级队列，而直接用数组存每个点对应的信息，速度更快，访问更方便 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#define _CRT_SECURE_NO_WARNINGS#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;iomanip&gt;using namespace std;const bool DEBUG = false;const int N = 1000007;int n, m, k, q;// We use a min-heap 小根堆 to store the points// delMax getMax refers to the &quot;max priority&quot; element, which has the smallest value// value[i] is the value of the point i// father[i] is the root of the heap i belongs to// lchild[i], rchild[i] is the left and right child of the point i in heap// npl[i] is the null-path-length of heap i// sze[i] is the size of heap iint value[N], father[N], lchild[N], rchild[N], npl[N], sze[N];// find(i) returns the root of the heap i belongs toinline int find(int x) &#123; return x == father[x] ? x : (father[x] = find(father[x]));&#125;// merge heap b into heap aint merge(int a, int b) &#123; if (a == 0) return b; if (b == 0) return a; if (value[a] &gt; value[b]) swap(a,b); rchild[a] = merge(rchild[a], b); father[rchild[a]] = find(a); if (lchild[a] == 0 || npl[lchild[a]] &lt; npl[rchild[a]]) &#123; int temp = lchild[a]; lchild[a] = rchild[a]; rchild[a] = temp; &#125; npl[a] = rchild[a] == 0 ? 1 : npl[rchild[a]] + 1; sze[a] = sze[lchild[a]] + sze[rchild[a]] + 1; return a;&#125;// getMax(x) returns the value of root of the heap x represents// Requires: x is the root of a heapint getMax(int x) &#123; // x is the root, root is the max, so we just return the value of x return value[x];&#125;// delMax(x) returns the new root after deleting root in heap x// Requires: x is the root of a heapint delMax(int x) &#123; int ans = value[x]; sze[x] -= 1; int new_root = merge(lchild[x], rchild[x]); father[new_root] = new_root; // new root is now a root, so its father is itself father[x] = new_root; // this deleted node, and all the nodes pointing to the deleted node should now point to the new root return ans;&#125;// deletes the Max element until this heap has no more than k elementsvoid prune(int x) &#123; if (sze[x] &lt;= k) return; delMax(find(x)); // needs to find(x) because delMax requires a root prune(find(x)); // after being deleted, x becomes a stranded point // prune must take in the new root of the heap&#125;// print all points and their informationvoid print() &#123; if (!DEBUG) return; cout &lt;&lt; &quot;# value Parent Lchild Rchild npl size &quot; &lt;&lt; endl; for (int i = 1; i &lt;= n; i++) &#123; cout &lt;&lt; setw(2) &lt;&lt; i; cout &lt;&lt; setw(6) &lt;&lt; value[i]; cout &lt;&lt; setw(6) &lt;&lt; father[i]; cout &lt;&lt; setw(8) &lt;&lt; lchild[i]; cout &lt;&lt; setw(8) &lt;&lt; rchild[i]; cout &lt;&lt; setw(6) &lt;&lt; npl[i]; cout &lt;&lt; setw(6) &lt;&lt; sze[i]; cout &lt;&lt; endl; &#125;&#125;int main() &#123; scanf(&quot;%d%d%d%d&quot;, &amp;n, &amp;m, &amp;k, &amp;q); for (int i = 1; i &lt;= n; i++) &#123; scanf(&quot;%d&quot;, value + i); father[i] = i; lchild[i] = rchild[i] = npl[i] = 0; // points to null sze[i] = 1; &#125; sze[0] = 0; npl[0] = 0; father[0] = lchild[0] = rchild[0] = 10e9; for (int i = 1; i &lt;= m; i++) &#123; int a, b; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); if (find(a) == find(b)) continue; // already connected, another edge doesn&#x27;t make a difference int merged = merge(find(a), find(b)); prune(merged); &#125; for (int i = 1; i &lt;= q; i++) &#123; int op, a, b; scanf(&quot;%d&quot;, &amp;op); if (op == 1) &#123; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); if (find(a) == find(b)) continue; int merged = merge(find(a), find(b)); prune(merged); &#125; else if (op == 2) &#123; scanf(&quot;%d&quot;, &amp;a); if (sze[find(a)] &lt; k) printf(&quot;-1\\n&quot;); else printf(&quot;%d\\n&quot;, getMax(find(a))); &#125; &#125;&#125; 复杂度分析 初始化后，每个点最多被入堆一次（所在连通块与他人联通），出堆一次（因为不属于前 k 大而被弹出堆）每次出入堆操作是两个左式堆的 merge，复杂度 O(logn)。共 n 个点，所以总体复杂度是 O(n logn) Reference 题解 P3377 【模板】左偏树(可并堆) 课程代码","categories":[],"tags":[{"name":"Tsinghua","slug":"Tsinghua","permalink":"https://yao-lirong.github.io/blog/tags/Tsinghua/"}]},{"title":"Tsinghua DSA 作业总结 (2)","slug":"2021-02-10-Tsinghua-DSA-作业总结-(2)","date":"2021-02-10T05:00:00.000Z","updated":"2022-06-08T19:33:28.000Z","comments":true,"path":"2021-02-10-Tsinghua-DSA-作业总结-(2)/","permalink":"https://yao-lirong.github.io/blog/2021-02-10-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(2)/","excerpt":"CST数据结构（2020秋）PA2a","text":"CST数据结构（2020秋）PA2a 2-1 Build 心得 List -&gt; 编号存树 我用的是自己写的 List 存树，对于一个节点，它有一个指针指向包含它所有孩子的 List，并有 height size 存储该点的高度及子树规模，当发生节点移动时，递归地向上更新。但是这样的话每次更新时，必须遍历当前节点的所有孩子，才能确认是否需要更新该点的高度或子树规模，这样不符合题目中 “复杂度与 cost 成线性” 的要求。所以会TLE，解决方法是在每一个点都存一个它向后看能看到的最大子树高度以及它后面所有点的子树规模和，这样每次删除某一点时，只需要更新它前面兄弟的这两个值就好了，符合我们对 cost 的定义。 List 存还会发生 MLE 的问题。既然题目中已经给出每个店的序号，其实我们不需要用 List 存，只需要用多个数组存储相对应的信息（前后节点，父子节点，本书规模及高度，其向后看所有兄弟的最大高度和字数规模和）即可，这样也解决了我们一开始读入时需要自建邻接表的问题 2-4-2 Kidd 算法 线段树，离散化。 线段树的每个节点所代表的区间必须一闭一开（我的实现中是左闭右开的），如果是闭区间会使同一个点被储存在相邻的区间中两次。 线段树中每个节点要存两个东西：1. 本节点对应的区间被翻转的次数 2. 本节点所包含的上所有子区间（包括它自己）被翻转的次数。其中 2 通过 本节点被翻转的次数 * 本节点代表的区间的大小 + 两个孩子区间的所有子区间被翻转的次数 得来。 所以在每次查询时，如果只是单纯的相交，相交部分也在这个区间被当做一个整体翻转时所翻转了，所以我们计算出相交范围的大小，乘上此区间被翻转的次数；如果查询区间包含在当前区间里面（恰好是当前区间），我们只需要加上当前区间及其所有子区间被反转的次数就好了 代码 离散化部分有严重错误，既然第一步就有错所以剩下的对不对咱其实也不知道。但是思路大概就这么个思路（ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165#include&lt;cstdio&gt;#include&lt;iostream&gt;using namespace std;int sort_cmp (const void * a, const void * b) &#123; return ( *(int*)a - *(int*)b );&#125;const int N = 200003;// isQuery[i] is true if the ith operation is a query &#x27;Q&#x27;, is false if the ith operation is a flip &#x27;H&#x27;bool isQuery[N]; int interval[N][2];// a stores the unique discretized intervalint a[N*2], unique_num = 0;struct treeNode&#123; bool isNode = false; // true if this is a leaf in segment tree int l, r; // this node represents the interval [l,r) int v; // this node stores value v; this interval *only* has been flipped v times long long total; // the points in this interval and all its subintervals have been flipped total times&#125;st[(N*2)&lt;&lt;1];void build(int li, int ri, int x);void update(int li, int ri, int x);long long query(int li, int ri, int x);int bisearch(int li, int ri, int g);int original_to_discrete(int x);int discrete_to_original(int y);int temp[N*4];int main()&#123; int n, m; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); cin.ignore(100,&#x27;\\n&#x27;); char o; int l, r; for(int i=0; i&lt;m; i++)&#123; scanf(&quot;%c%d%d&quot;, &amp;o, &amp;l, &amp;r); isQuery[i] = (o==&#x27;Q&#x27;); interval[i][0] = l; interval[i][1] = r; temp[i*4 + 0] = l; temp[i*4 + 1] = l+1; temp[i*4 + 2] = r; temp[i*4 + 3] = r+1; cin.ignore(100,&#x27;\\n&#x27;); &#125; qsort(temp, 4*m, sizeof(int), sort_cmp); a[unique_num++] = temp[0]; for(int i=1; i&lt;4*m; i++) &#123; if(temp[i]!=temp[i-1]) a[unique_num++] = temp[i]; &#125; build(0, unique_num, 0); for(int i=0; i&lt;m; i++)&#123; if(isQuery[i])&#123; cout&lt;&lt;query(original_to_discrete(interval[i][0]), original_to_discrete(interval[i][1]), 0)&lt;&lt;endl; &#125; else &#123; update(original_to_discrete(interval[i][0]), original_to_discrete(interval[i][1]), 0); &#125; &#125; return 0;&#125;// node x in s-tree represents the interval [li,ri)void build(int li, int ri, int x)&#123; st[x].isNode = true; st[x].l = li; st[x].r = ri; st[x].v = 0; //cout&lt;&lt;&quot;building node &quot;&lt;&lt;x&lt;&lt;&quot;represents [&quot;&lt;&lt;st[x].l&lt;&lt;&quot;, &quot;&lt;&lt;st[x].r&lt;&lt;&quot;)&quot;&lt;&lt;endl; if(li+1 != ri) &#123; int mid = (li+ri)/2; build(li,mid,(x&lt;&lt;1) + 1); build(mid,ri,(x&lt;&lt;1) + 2); &#125; return;&#125;// currently at s-tree node x, updating interval [li,ri]void update(int li, int ri, int x)&#123; int dis = discrete_to_original(st[x].r-1) - discrete_to_original(st[x].l) + 1; if(li&lt;=st[x].l &amp;&amp; ri&gt;=st[x].r-1)&#123; // interval(x) \\subseteq [li,ri] st[x].v += 1; st[x].total += dis; return; // we should immediately stop updating any children of this node, because that will do a duplicate update &#125; if(!st[(x&lt;&lt;1)+1].isNode) return; // if this is a leaf node, return if(st[(x&lt;&lt;1)+1].isNode &amp;&amp; li &lt;= st[(x&lt;&lt;1)+1].r-1)&#123; // intersects left child update(li, ri, (x&lt;&lt;1)+1); &#125; if(st[(x&lt;&lt;1)+2].isNode &amp;&amp; ri &gt;= st[(x&lt;&lt;1)+2].l)&#123; // intersects right child update(li, ri, (x&lt;&lt;1)+2); &#125; // &#x27;st[x].v * dis&#x27; is the number of flips caused by &quot;this&quot; interval being flipped // st[(x&lt;&lt;1)+1].total is the total number of flips this node&#x27;s left child has // st[(x&lt;&lt;1)+2].total is the total number of flips this node&#x27;s right child has st[x].total = st[x].v * dis + st[(x&lt;&lt;1)+1].total + st[(x&lt;&lt;1)+2].total; return;&#125;long long query(int li, int ri, int x)&#123; long long res = 0; if(li&lt;=st[x].l &amp;&amp; ri&gt;=st[x].r-1)&#123; // interval(x) \\subseteq [li,ri] res += st[x].total; return res; &#125; int dis = discrete_to_original(min(st[x].r-1, ri)) - discrete_to_original(max(st[x].l, li)) + 1; res += st[x].v * dis; if(st[(x&lt;&lt;1)+1].isNode &amp;&amp; li &lt;= st[(x&lt;&lt;1)+1].r-1)&#123; res += query(li, ri, (x&lt;&lt;1)+1); &#125; if(st[(x&lt;&lt;1)+2].isNode &amp;&amp; ri &gt;= st[(x&lt;&lt;1)+2].l)&#123; res += query(li, ri, (x&lt;&lt;1)+2); &#125; return res;&#125;int bisearch(int li, int ri, int g)&#123; int mid = 0; while(ri &gt; li+1)&#123; mid = (li+ri)&gt;&gt;1; if(a[mid]&lt;=g) li = mid; else ri = mid; &#125; return li;&#125;int discrete_to_original(int y)&#123; return a[y];&#125;int original_to_discrete(int x)&#123; return bisearch(0, unique_num, x);&#125; 2.7 Virus 心得 堆的 sink 的边界条件应该是 1234int l=i*2, r=i*2+1;while((l&lt;=n &amp;&amp; a[i]&gt;=a[l]) || (r&lt;=n &amp;&amp; a[i]&gt;=a[r]))&#123; ...&#125; 而不是 12345678int height(int x)&#123; if(x==0) return 1; int digit=0; while(x&gt;0) &#123;x=x&gt;&gt;1; digit++;&#125; return digit;&#125;while((a[i]&gt;=a[l]||a[i]&gt;=a[r])&amp;&amp;i&lt;pow(2,height(n)))&#123; ...&#125; 堆是一个完全二叉树 (Complete Binary Tree) 而不是一个完美二叉树 (Perfect Binary Tree) 代码 仅展示了堆的实现部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788const int M=1000007;const int INF=10e7;const int N = 1007;struct point &#123; int id; // id = x*N + y int t; point(int x, int y, int ti) &#123; id = x*N + y; t = ti; &#125; point() &#123; id = 0; t = 0; &#125; point(int n) : point() &#123;&#125; point(const point &amp;from) &#123; this-&gt;id = from.id; this-&gt;t = from.t; &#125;&#125;;point myPQ[M];inline bool operator&lt;(const point &amp;p1, const point &amp;p2) &#123; return p1.t &lt; p2.t;&#125;inline bool operator&gt;(const point &amp;p1, const point &amp;p2) &#123; return p1.t &gt; p2.t;&#125;inline bool operator&lt;=(const point &amp;p1, const point &amp;p2) &#123; return p1.t &lt;= p2.t;&#125;inline bool operator&gt;=(const point &amp;p1, const point &amp;p2) &#123; return p1.t &gt;= p2.t;&#125;class PriorityQueue &#123; int n = 0; point *a = myPQ;public: void add(point x) &#123; a[++n] = x; swim(n); &#125; point extract() &#123; point result = a[1]; swap(a[1],a[n]); a[n--]=INF; sink(1); return result; &#125; bool isEmpty() &#123;return n == 0;&#125;private: void swim(int i) &#123; while(i&gt;1 &amp;&amp; a[i/2]&gt;=a[i])&#123; swap(a[i/2],a[i]); i = i/2; &#125; &#125; void sink(int i) &#123; int l=i*2, r=i*2+1; while((l&lt;=n &amp;&amp; a[i]&gt;=a[l]) || (r&lt;=n &amp;&amp; a[i]&gt;=a[r]))&#123; if(a[l]&lt;=a[r]&amp;&amp;l&lt;=n)&#123; swap(a[i],a[l]); i = l; l=i*2; r=i*2+1; continue; &#125; else if (a[l]&gt;a[r]&amp;&amp;r&lt;=n)&#123; swap(a[i],a[r]); i = r; l=i*2; r=i*2+1; continue; &#125; else return; &#125; &#125;&#125;;","categories":[],"tags":[{"name":"Tsinghua","slug":"Tsinghua","permalink":"https://yao-lirong.github.io/blog/tags/Tsinghua/"}]},{"title":"Tsinghua DSA 作业总结 (1)","slug":"2021-02-09-Tsinghua-DSA-作业总结-(1)","date":"2021-02-09T05:00:00.000Z","updated":"2022-06-08T19:33:28.000Z","comments":true,"path":"2021-02-09-Tsinghua-DSA-作业总结-(1)/","permalink":"https://yao-lirong.github.io/blog/2021-02-09-Tsinghua-DSA-%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93-(1)/","excerpt":"CST数据结构（2020秋）PA1a","text":"CST数据结构（2020秋）PA1a 1-1 A*B Problem 心得 每个数组存一位的话速度太慢过不了，必须压位 10e5 是 10 * 10^5 所以是 10^6 … 我们把一个大整数分成几块存在数组里的时候，如果这个数头上有0的话，0就会被忽略了（不压位的话没有这个问题，因为0也就是1位）比如4位4位存，100046000303025会变成[3025, 30, 460, 100]，直接输出会变成100460303025，明显不对，所以我们输出的时候要记得补全0 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;using namespace std;// N is the maximum number of digits of the input, M is the maximum number of digits of the productconst int N = 5007, M = 10023;// we multiply each 10000 together and store them in a single entry, 10e3 has 4 0s .const int ten = 10e3, d_ten = 4;// a is input A in e4 base, b is input B in e4 base, c is their product in e4 base.int a[N], b[N], c[M];// a_len is the number of entires in a needed to store input A, so it&#x27;s the digit needed to store A in e4 base; b_len is that for Bint a_len, b_len;// digit is a helper array we will need during multiplicationint digit[] = &#123;1,10,100,1000,10000&#125;;// inputA and inputB is A and B read in from streamchar inputA[N], inputB[N];// multiplies e4 base A and B together and store the result in Mvoid multiply();// returns the length of product in e4 baseint get_clen();// print padding zero in each entry of array C when outputting the resultvoid print_padding_zero(int,int);int main()&#123; int n = 0; scanf(&quot;%d&quot;,&amp;n); for (int i=0; i&lt;n; i++)&#123; memset(a,0,sizeof(a)); memset(b,0,sizeof(b)); memset(c,0,sizeof(c)); memset(inputA,0,sizeof(inputA)); memset(inputB,0,sizeof(inputB)); scanf(&quot;%s&quot;,inputA); int ina_len = strlen(inputA); a_len = (ina_len-1)/d_ten + 1; scanf(&quot;%s&quot;,inputB); int inb_len = strlen(inputB); b_len = (inb_len-1)/d_ten + 1; // store the number in reverse order and e4 baes in array a,b for (int i=ina_len-1; i&gt;=0;)&#123; int tostore = 0; for (int j=0; j&lt;d_ten &amp;&amp; i-j&gt;=0 ; j++)&#123; int ASCII = inputA[i-j] - &#x27;0&#x27;; tostore += ASCII*digit[j]; &#125; a[a_len-1 - i/d_ten] = tostore; i = i-d_ten; &#125; for (int i=inb_len-1; i&gt;=0;)&#123; int tostore = 0; for (int j=0; j&lt;d_ten &amp;&amp; i-j&gt;=0 ; j++)&#123; int ASCII = inputB[i-j] - &#x27;0&#x27;; tostore += ASCII*digit[j]; &#125; b[b_len-1 - i/d_ten] = tostore; i = i-d_ten; &#125; multiply(); int c_index = get_clen(); printf(&quot;%d&quot;, c[c_index]); for (int i=c_index-1; i&gt;=0; i--)&#123; print_padding_zero(c[i], ten/10); printf(&quot;%d&quot;, c[i]); &#125; printf(&quot;\\n&quot;); &#125; return 0;&#125;void multiply()&#123; for(int i=0; i&lt;a_len; i++)&#123; for(int j=0; j&lt;b_len; j++)&#123; int product = a[i] * b[j]; c[i+j] += product; c[i+j+1] += c[i+j] / ten; // carry over digit c[i+j] %= ten; // only stores e4 base number &#125; &#125;&#125;inline int get_clen()&#123; int n = a_len + b_len + 3; while (c[n]==0 &amp;&amp; n&gt;0) n--; return n;&#125;/** * Each entry in array C should store an e4 base number, but sometimes it stores a number smaller than that. * That&#x27;s because it ignores the leading 0s (leading 0s in reversely stored C) when in this case 00XX. * Example: 1004 0030 57 stored in C has form [57, 30, 1004], this function helps print out the first 00 in 0030 **/inline void print_padding_zero(int n, int digit)&#123; if (n&lt;digit) printf(&quot;0&quot;); if (digit == 10) return; print_padding_zero(n,digit/10);&#125; Reference 高精度乘法 高精度乘法的压位 1-3 Filename 心得 编辑距离，移动窗口节省空间 字符串的读入：一开始以为是 getline 的问题，读不进来字符串，实际上是因为仅用 scanf 读入3个整数后会留下一个换行符在 buffer 中。使用 cin.ignore(100,'\\n') 删除换行符（忽略 100 个字符，或者忽略1个 '\\n'；忽略所有读入，直到总共忽略了 100 个字符，或者忽略了 1 个换行符） dp数组开到 10012 会爆炸，改用滚动窗口，dp[0][j] 表示 $ x_1x_2…x_{i-1}$ 到 y1y2...yj （前一步）所需要的操作，dp[1][j] 表示 $ x_1x_2…x_{i-1}$ 到 y1y2...yj （这一步）所需要的操作。几个实现细节在注释中已标出 TLE: 第一感觉想的是在每次计算 i, s.t.x1x2...xi 变成 y 所需要的操作后（即计算完成 dp[i][0...m]后），扫一遍数组，如果所有值都大于 k 的话，就停止查找。但是这样不会对数据规模有任何可观的缩减，因为如果说我们有长为 105 的两个字符串，并且他们可以在 k 操作内互相转换（极端情况两个相同的字符串），我们仍然需要进行 O(mn) = 105 × 105 次操作。 根据习题课的解决方法，其实当两个数组间的长度差超过 k 时就绝对不可能从一个转换成另一个了。所以，对于每个 x 的子串 x1x2...xi，我们只需要看 yi − kyi − k + 1...yi + k 就可以了。当然还要注意 i − k, i + k别越界，所以实际上是看 ymin(1, i − k)...ymax(m, i + k) 这个子序列 这个改动会造成一些 WA，直觉一下子想到是有可能在最后退出循环时，我们因为 k 的限制压根就没扫到 dp[n][m](= dp[1][m])。实际上问题差不多，是因为 dp[i][j] = min(dp[i-1][j], dp[i][(j-1)]) + 1; 这句话中 dp[0][j] 我们一开始全初始化为0，对于 dp[i][i+k] 这个位置，它的一种方案 dp[i-1][i+k] 永远不会被上一步更新到，因为上一步只更新 dp[i-1][(i-1)-k] ~ dp[i-1][(i-1)+k] 即 dp[i-1][i+k] 恒等于0，即 dp[i][i+k] 永远会采取 dp[i-1][i+k] 这一方案。将 dp[0][j] 初始化为 infinity 解决 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;const int N = 501007, Inf = 501; // Inf is &quot;effective infinite&quot; : k&lt;=500char a[N], b[N]; // a, b store the string x, yint dp[2][N]; // For each iteration i, dp[0] is equivalent to dp[i-1], dp[1] is equivalent to dp[i]int main()&#123; int n,m,k; //length of x, length of y, max number of operations scanf(&quot;%d%d%d&quot;, &amp;n, &amp;m, &amp;k); cin.ignore(100,&#x27;\\n&#x27;); cin.getline(a+1, n+1); cin.getline(b+1, m+1); // initialize dp[0][j] to be the operations needed to edit an empty string to y1y2...yj // initialize the rest of the array to be infinite for(int j=0;j&lt;=m;j++) dp[0][j] = j, dp[1][j] = Inf; for(int i=1; i&lt;=n; i++) &#123; // start from the 1st character // dp[0][0] represents dp[i-1][0] in an ordinary dp array // dp[i-1][0] represents the distance between x1x2...x_&#123;i-1&#125; to the empty string dp[0][0] = i-1; for(int j=max(1, i-k);j&lt;=min(m,i+k);j++)&#123; // only looks at y[i-k] to y[i+k] if(a[i]==b[j]) dp[1][j] = dp[0][(j-1)]; else dp[1][j] = min(dp[0][j], dp[1][(j-1)]) + 1; &#125; // &quot;previous&quot; of next iteration i+1 is current value from this iteration i for(int j=max(1, i-k);j&lt;=min(m,i+k);j++) dp[0][j] = dp[1][j]; &#125; printf(&quot;%d\\n&quot;, dp[1][m]&lt;=k ? dp[1][m] : -1); return 0;&#125; Reference C++ cin.ignore()的用法详解 C++ cin&gt;&gt; cin.get() cin.getline() 1.4 Risk 心得 Queap, 二分搜索 每次询问的时候，假设我们要看前 m 天，现在的 Queap 中存了 qsize 天，如果 qsize&gt;m 的话，我们存了一些没必要看的天，那么我们就需要把这些没必要的天给推出去，所以看出来我们需要推出 qsize-m 个没必要的天。然而我的实现在 dequeap 和 enqueap 时会实时更新 qsize 所以实际上 Queap 只会弹出大概一半的元素，会造成很大的问题。所以我们必须先记录 qsize-m 然后再更新 最后的T次询问是对已经有的数据，询问有多少在相应的区间内。我们这里可以使用排序后二分查找区间分界点的位置，而不是对于每个元素都看是在哪个区间内。这样可以大大缩短时间 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168#define _CRT_SECURE_NO_WARNINGS#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;// elements in queue/queap are the number of infections of previous days// size of queue/queap is the number of days we need to keep track of// Queue Nodestruct qNode &#123; long long value; qNode* next, * prev;&#125;;qNode *qHead = new qNode, * qTail = new qNode;// Queap Node; Invariant: node n in a queap [h...n...t] is the max element in interval in [h...n]struct pNode &#123; long long value, num; pNode* next, * prev;&#125;;pNode *pHead = new pNode, * pTail = new pNode;long long qsize = 0;// enqueue value v into Q;// update &quot;max value available&quot; in queap Pvoid enqueap(long long v) &#123; qNode* q = new qNode; pNode* p = new pNode; qsize += 1; q-&gt;value = v, q-&gt;next = qHead-&gt;next; q-&gt;prev = qHead; qHead-&gt;next-&gt;prev = q; qHead-&gt;next = q; pNode* i = pHead-&gt;next; int num = 0; while (i != pTail) &#123; if (v &gt;= i-&gt;value) &#123; num += i-&gt;num; i = i-&gt;next; delete i-&gt;prev;&#125; else break; &#125; // i is the first interval max greater than inserted value // replace everything between pHead and i with the newly inserted value if (num &gt; 0) &#123; p-&gt;value = v; p-&gt;num = num+1; p-&gt;next = i; p-&gt;prev = pHead; i-&gt;prev = p; pHead-&gt;next = p; &#125; else &#123; // num==0 says v &lt; pHead-&gt;next, insert p in between Head and Head-&gt;next p-&gt;value = v; p-&gt;num = 1; p-&gt;next = i; p-&gt;prev = pHead; i-&gt;prev = p; pHead-&gt;next = p; &#125; return;&#125;// dequeue one element from Q and from Pvoid dequeap() &#123; if (qsize &lt;=0) return; qsize -= 1; qNode *qDel = qTail-&gt;prev; qTail-&gt;prev = qTail-&gt;prev-&gt;prev; qTail-&gt;prev-&gt;next = qTail; delete qDel; pNode *pDel = pTail-&gt;prev; if (pDel-&gt;num &gt; 1) pDel-&gt;num--; //there is still element after deletion else if (pDel-&gt;num == 1)&#123; //only one element left, there will be 0 elements after this deletion, so let&#x27;s just delete this node altogether pTail-&gt;prev = pDel-&gt;prev; pDel-&gt;prev-&gt;next = pTail; delete pDel; &#125; else throw &quot;0 element in pNode error&quot;; // there is no element in the top node, which is not supposed to happen return;&#125;// returns the Max value in the Queap; returns 0 if Queap is emptyinline long long getMax() &#123; return qsize&gt;0 ? pTail-&gt;prev-&gt;value : 0;&#125;// print both queue and queap for debugging purposesvoid printQueue() &#123; qNode* q = new qNode; q = qHead-&gt;next; pNode* p = new pNode; p = pHead-&gt;next; cout&lt;&lt;&quot;printing out queue Q:&quot;&lt;&lt;endl; while (q != qTail) &#123; cout &lt;&lt; q-&gt;value &lt;&lt; &quot; &quot;; q = q-&gt;next; &#125; cout &lt;&lt; endl; cout&lt;&lt;&quot;printing out queap P:&quot;&lt;&lt;endl; while (p != pTail) &#123; long long v = p-&gt;value; for (int i = 0; i &lt; p-&gt;num; i++) &#123; cout &lt;&lt; v &lt;&lt; &quot; &quot;; &#125; p = p-&gt;next; &#125; cout &lt;&lt; endl;&#125;// cmp function for qsortinline int sort_cmp (const void * a, const void * b)&#123; return ( *(long long*)a - *(long long*)b );&#125;//returns the last element &lt;=g in [l,r]int bisearch(const long long a[], int l, int r, long long g) &#123; int mid = -1; while(l&lt;r) &#123; mid = (l+r)&gt;&gt;1; a[mid] &lt;= g ? l=mid+1 : r=mid; &#125; return l-1;&#125;long long observed[1000007]; int input[1000007];int main()&#123; // initializes Q and P qHead-&gt;next = qTail, qTail-&gt;prev = qHead; pHead-&gt;next = pTail, pTail-&gt;prev = pHead; int n; scanf(&quot;%d&quot;, &amp;n); for (int i = 0; i &lt; n; i++) &#123; scanf(&quot;%d&quot;, &amp;input[i]); &#125; // For each day, we first check which day is the earliest day we have to keep track of // If there are some days we no longer have to keep track of, we dequeue them from the queap and queue // Then record the maximum infection number maintained by queap and enqueue the infection number of today for (int i = 0; i &lt; n; i++) &#123; long long m; scanf(&quot;%lld&quot;, &amp;m); long long num = qsize-m; for(long long j=0; j &lt; num; j++)&#123; dequeap(); &#125; observed[i] = getMax(); enqueap(input[i]); &#125; qsort(observed, n, sizeof(long long), sort_cmp); // There are T queries, always on already observed infection number // We only care about number of days in a certain range, not the date or any other information // Therefore, we can use binary search to get the number of days in this given range. int T; scanf(&quot;%d&quot;, &amp;T); for (int i=0; i&lt;T; i++)&#123; long long p,q; scanf(&quot;%lld%lld&quot;, &amp;p, &amp;q); int pnum = 0, qnum = 0; pnum = bisearch(observed,0,n,p-1) + 1; // &lt;p \\equiv &lt;=p-1; returns the index of the last element &lt;p, so there are index+1 elements qnum = bisearch(observed,0,n,q-1) - pnum + 1; // similarly, &lt;q \\equiv &lt;=q-1; bisearch(observed,0,n,q-1) - bisearch(observed,0,n,p-1) gives the number of elements in range [p,q) printf(&quot;%d %d\\n&quot;, pnum, qnum); &#125; return 0;&#125; Reference 双向链表(结构体+指针) 定义结构体变量及初始化; 结构体定义变量的三种方法 unsigned long long int scanf","categories":[],"tags":[{"name":"Tsinghua","slug":"Tsinghua","permalink":"https://yao-lirong.github.io/blog/tags/Tsinghua/"}]},{"title":"Look Back on Cornell/Tsinghua 20FA","slug":"2021-01-11-CornellTsinghua-20FA-总结","date":"2021-01-11T05:00:00.000Z","updated":"2022-06-08T19:56:44.000Z","comments":true,"path":"2021-01-11-CornellTsinghua-20FA-总结/","permalink":"https://yao-lirong.github.io/blog/2021-01-11-CornellTsinghua-20FA-%E6%80%BB%E7%BB%93/","excerpt":"CS4820 Intro Analysis of Algorithms I got to do all the stuff I want. … I’m actually one of the Ithaca’s firefighters now and on average we have a really big and nice fire each year so I got to work when that comes. – Dexter Kozen, 2020/12/16","text":"CS4820 Intro Analysis of Algorithms I got to do all the stuff I want. … I’m actually one of the Ithaca’s firefighters now and on average we have a really big and nice fire each year so I got to work when that comes. – Dexter Kozen, 2020/12/16 因为网课的原因不想上写代码的课，4820是我本学期上的唯一一节CS课，也是拿的第一个CS A+。但是总体来说 4820 和 3110 上到最后都没有 2112 和 2802 有一种我真的成长了的成就感，可能是因为 4820 覆盖的东西太多，导致知识间比较割裂，用来证明一个算法的技巧在另一个模块就用不太上了。 他们说什么上完 4820 对面试有帮助，我觉得这完全就是扯淡的，可能确实刷题的时候你更容易看出来这应该用什么算法了，但是这门课所有的编程训练只能说是非常初级（到连我都觉得不难的水平）课程重点还是在算法的分析和证明上，和证明题一对比，给的编程题真就跟过家家一样。最后一次作业复刻了 sxy 的壮举交成了 release，幸亏还有个编程题的 10 分保底，这次作业就拿了 10/30 分。但是由于前面作业考试都不错，而且去不了跨时差 Office Hour 导致我所有问题都在 Piazza 上问了，participation grade 特别高。xzy 在课程中问 participation grade 怎么算时发现了以后我才去看的，我的 Piazza 贡献是整个班的第二，人家第一是成天回答问题，我是成天什么都没搞懂在上面问问题。不过用 Kozen 的话说，这也是一种 “contribute to the intellectual content of the course” 方式。所以大概是因为极高的 participation grade 以及对 Kozen 的诚恳请求，老人家最后高抬贵手给我了个 A+ （不过要是没误交作业也是我应得的啦） 对于 Dexter Kozen，说实话感觉他的教学水平并不如 Myers 和 Halpern，有的时候基本上是完全照着课本来的，如果他是完全按照课本来的，那么一般看课本甚至比他讲得好；但是当他按照自己的证明方法讲的时候，他的方法又比书上的好理解很多…不过比下肯定是有余的，比其他几个我上过课但是没在这提名的叫兽好多了。而且 Dexter Kozen 还会在最后一节课给你弹吉他听，就这还要啥自行车？ 课程内容分布方面（可能不是 Kozen 而是书本的问题）分治和动态规划两章，我并不认为第一次接触这些概念的同学能听懂任何东西，分治直接给的是 FFT 和找平面中最近点的两个很变态的例子，动规直接是从多维动规开始讲的（书上是按维度顺序来的，这确实是Kozen一时兴起）网络流这一节 作为 Jon Kleinberg 和 Eva Tardos 的主场，讲得例子都非常有意思。 MATH4710 Basic Probability 傻逼中的傻逼课，我不认为在上完这节课以后我对概率和统计的理解对比2802之后有任何进步。当时是为了打算春季上机器学习，所以即使知道这法国人讲得屎但也是顶着头上落屎的风险上的。没想到人家法国人这么实诚，你觉得我讲得屎，我就真就用实际行动证明老子讲得就是屎，你还拿他没办法，你说气不气？上这个法国人的课上到学期末，我甚至都不知道 Poisson 或者其他概率分布的图像长什么样，我对每一个分布的认知完全是割裂的，直到期末考试我还要一个个查提到的分布的 distribution function 到底是什么。每一次的作业和考试都在非常离谱的地方给我扣分，最后成绩也贼难看，完完全全地打击了我学习数学的信心，不如说整个康奈尔数学院的存在，除了少数几个教授以外，就是为了打击你学习数学的信心。整个院里面，除了研究做不出来所以只能折磨学生取乐的教授，还有研究做不出来而且本身实力就不行所以在读博期间必须兼职助教于是就也跟着虐待学生的变态博士生。不化简扣1分，你说100分满分，中位数 7,80 你扣也就算了，60满分中位数58的考试你因为不化简给我扣2分，而且让我化简的还是一个带了四项组合数的超恶心式子，你怎么不直接让我默写 π 的后1000位呢？ EAS1540 Introductory Oceanography 大多数人强推的科学课，说实在话没什么意思。个人感觉不如 DEA1500（虽然是不一样的 distribution requirement）Gary Evans在我心里还是暂时的选修课之神的地位。大家都说这门科学课讲得东西简单（毕竟是连美国人都推荐的科学课）实际也确实如此，但是简单的部分他讲的我都会，顶多也就是高中地理高一水平；难的部分呢他说实话又没讲明白，而且 Bruce Monger 在整个学期都在不停的讲环保环保，所以来的没有 Gary 最后一课画龙点睛那样令人印象深刻，不过 Bruce Monger 的目标也达到了，现在我确实清楚地意识到如果 2030 和 2050 控温目标没达到的话，地球真的会灭亡。 INFO1998 Freshmen Team Projects (Intro to Machine Learning) 没意思，教你怎么用 sklearn 库的课 CS2024 C++ Programming 还是挺不错的，五六年以后，我终于第一次正式学习了C++这门语言，就是作业有些无聊。 CS4320 Introduction to Database Systems 我drop了这门课 一门教数据库的课，开课两个礼拜竟然仍然没教学生们如何在你的电脑上安装数据库来使用基本的 SQL 指令；老师操着一口谁也听不懂的德国口音，让我梦回我托福水平只有80分的时候元素听托福听力的那个秋天；录像上传 youtube 公开，就好像除了你的学生以外谁还会闲着没事不去上CMU的数据库，来上您的课练习德语一样（但还是要赞扬一下这个老师公开上传，他实际上也将近几年的所有课程录像上传到了 Cornell VOD，只是我不清楚到底谁会去看讲得这么烂的课） 在清华认识的同学 Leo 竟然跟我说他最后这门课得了一个 A+，而他得 A+ 的诀窍就是自己原来接触过数据库，不去上课，作业发下来以后自己查找相关资料进行学习，也是非常离谱，不知道德国人发现原来他觉得做得不错的学生都是通过这种方式“做的不错”的会作何感想。 两年以来，我首次感觉到我的学费花得值，就是在这个动荡的2020年，Cornell 对它的中国学生说，你可以去清北上交中的一所学校进行你的线下秋季学期，其他的所谓 Ivy 和 Ivy+ 们，大气都不敢出一个，更别说去清北上交了。康奈尔牛逼！ 其实一开始我是录的上海交通大学，后来好像被补录的清华，有了TOP2，谁还会去上海一个不知名的小学校呢？ 30240184 数据结构 现在我们要来证明一下它的正确性……………为什么要证明呢？就好像你不能说自己是世界一流大学你就是了，你肯定得证明一下自己确实有匹配的实力才行 – 邓俊辉，于为什么要证明二分查找的正确性 这是一门神奇的课程，无论讲得概念是简单的链表还是难的线段树，都能让学生受益，邓俊辉老师是伟大的老师，他让我一个原来觉得线段树或kd树这种东西离我很远的人，感受到了原来我也能听明白这么复杂的数据结构。他也让我认识到，我校的 Nate Foster 真是个 cjb，ocaml 的红黑树实现不是因为 pattern matching 所以代码才那么少，是因为它使用的是 3-4 重构而不是传统的旋转，要是 C++ 写重构代码量也会大幅减少… 邓俊辉老师也鼓励了我，说不定我和清华的同学们实力差距没有我原来想象的那么巨大，我差的就是一堂这么好的课，一个全是学CS同学的宿舍，一个耐心认真的老师而已。清北和MIT是我心目中的圣地，我以为里面的人都是愿意穷其一生为全人类服务的人，我也以为我这次来之后，这个想法要么破碎要么印证，因为这门课，因为邓俊辉老师，我现在更倾向于说我的这个想法被印证了。 实际上上完这门课以后我对自己的认知更迷茫了，虽然 PA1 做的不是那么好（和清华同学比，自己的预期还是达到了的）PA2 因为康奈尔期末考，摔伤了腿，生病等等几乎没做，但咱 PA3 拿了满分啊，而且 PA3 是唯一一个真正有 TA 指导的的 Programming Assignment，其他的都是别的同学可以直接问他们班里宿舍里的信竞大佬，我只能闭门造车。四舍五入，我是不是要是有一定程度的帮助，和清华同学比一点也不差呢？说实话我来之前确实心里面有点这样想，毕竟清华的人虽然聪明，但是大部分人高中三年没任何编程经历，我虽然蠢，但是对自己CS还是比较有信心的。直到上完这门课，期末考试几乎都不会的情况下，我的心里还是抱有那么一丝丝希望：说不定咱和清华人比一点不差呢。说到这个，我和其他人说起清华这门课很难的时候，他们表现得竟然是惊讶而不是理所当然，他们竟然真的觉得康奈尔提供的教育足以让我们可以和清华人抗衡。看来在有自知之明这点上我还是比其他同学高一点的（ 选这门课还要感谢 cz，是他跟我说了这门课评价很好我才会去选，不然“数据结构”这种课我绝对觉得我都学得会了，懒得上。实际上我一开始不是在邓老师门下学习，一开始给国际交换生的名额只有另一个讲师的班了，但我第一节课去听了以后，她讲尾递归竟然说用到的空间会是 O(n)，其中 n 是调用次数。别的我不确定，但是 Myers 曾经明确地说过尾递归的好处就是可以重复地使用调用栈，如此一来不会有溢出（当然了说不定 Java 和 C++ 并不一样）于是下课以后我去询问为什么是 O(n) 和她讲了我的想法，她表现得挺不耐烦，然后我追着她出了教室，她一边开自行车的锁一边听我说话，最后就撂下一个 ”恩，那可能是这样吧“ 就走了。这个表现让我回忆起了我的高中班主任也是这样，说自己很喜欢学生，很喜欢教学，最后你去找她的时候她根本是漠不关心，虚伪的一B，幸亏我后来认识 Leo，知道了他（并不知道自己应该跟我一个班）一直在听邓老师的课，去听了以后真是一个天上一个地下。清华的同学们啊，你们都在清华了，为什么要折磨自己，不跟着邓老师学呢？ 上完这门课我也在想，是不是只有差的老师，没有差的学生？如果每个人都听邓老师的课，我很怀疑他们会听不懂（毕竟我都听懂了）可是其他大学的同学们并无法享受到这等待遇，只能逼着自己听讲那个讲得烂的。就好像 qsq 在北师大，或者我自己在康奈尔的经历完全一样，一个差的老师不仅帮不到学生，还会严重地打击一个人的自信心，让他深深地怀疑我到底喜不喜欢这个学科，这个学科到底适不适合我等等此类。 Logic, Computing, Games 清华大学特聘教授，斯坦福大学名誉教授，什么什么很厉害的研究所的创始人 Johan van Benthem！来给你 上网课！ 而且这网课还是如果你想要问问题必须等老师一节课下课以后，你才能和助教举手示意说我有个问题，然后他会给你一个麦克风你才能问。学生没有任何直接联系教授的方法，必须通过助教，助教又一副爱答不理的样子，课程推荐阅读也没有，必要先修知识也不写，甚至连个 syllabus 都没得。教授的课讲得其实还不错，第一节课我感觉自己听到了前二十分钟，当我后面完全听不懂想要自己阅读材料寻求帮助时，发现这门课屁都不提供，也不知道是这个教授不上心，还是助教什么都不管只管向教授报告“一切安好”，或者是全清华的课都是这个德行，我反正上不下去这课了，也是很遗憾的，毕竟对方看起来真的是很有名的教授，比康奈尔的哲学院估计好不少呢… 话说后来我还去看了几节，发现人是越来越少，看到清华的同学们也退了我是很开心的，不过他们可能是因为听不懂英语退的吧？我也不知道 Introduction to Artificial Intelligence 圆了一个自己“上叉院的课”的这一装逼梦，第一节课是个老头子，英语讲得也不很溜道但就是要说英语，第一节课啥也没讲就讲了讲AI的历史什么的，很是没意思。后来发现这个英语讲得不溜道，说话也不溜道的老头子是姚期智老先生…嘶，对不起 第二节课就开始有意思了，一上来就 reinforcement learning 的什么很厉害的人来讲很先进的 reinforcement learning，有意思是有意思，不过是局限于叉院同学们的有意思，我已经听不懂了，早早拜拜，听了一节课就行了 课堂上还有来自 Princeton 的同学们（看到她们 Gmail 右上角的校标确定的）我第二节课就不去了，不知道我们 Princeton 的小同学是什么时候不去的，还是她们都能听懂。不过你说她们有叉院同学的实力，我是绝对不信的。","categories":[],"tags":[{"name":"Cornell","slug":"Cornell","permalink":"https://yao-lirong.github.io/blog/tags/Cornell/"},{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"},{"name":"Tsinghua","slug":"Tsinghua","permalink":"https://yao-lirong.github.io/blog/tags/Tsinghua/"}]},{"title":"2020 Web Journal","slug":"2020-12-31-2020-网络日志","date":"2020-12-31T05:00:00.000Z","updated":"2022-06-08T19:31:52.000Z","comments":true,"path":"2020-12-31-2020-网络日志/","permalink":"https://yao-lirong.github.io/blog/2020-12-31-2020-%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97/","excerpt":"A cartoon intro to DNS over HTTPS: HTTP, DNS, DNS over HTTPS 简介 (中文版 1 2) VS Code上也能玩转Jupyter Notebook 什么是生成函数？: 有自带数学公式的dpf版，来源为豆丁网。其中第三页结尾式子应为 $g(x)+xg(x) = \\frac{g(x)}{x} - 1$, 此段结尾（在第四页开头）的式子应为 $g(x) = \\frac{-x}{x^2+x-1}$. IDM使用技巧","text":"A cartoon intro to DNS over HTTPS: HTTP, DNS, DNS over HTTPS 简介 (中文版 1 2) VS Code上也能玩转Jupyter Notebook 什么是生成函数？: 有自带数学公式的dpf版，来源为豆丁网。其中第三页结尾式子应为 $g(x)+xg(x) = \\frac{g(x)}{x} - 1$, 此段结尾（在第四页开头）的式子应为 $g(x) = \\frac{-x}{x^2+x-1}$. IDM使用技巧","categories":[],"tags":[{"name":"Journal","slug":"Journal","permalink":"https://yao-lirong.github.io/blog/tags/Journal/"}]},{"title":"C++ Manual","slug":"2020-11-29-C++-常见问题","date":"2020-11-29T05:00:00.000Z","updated":"2023-05-15T01:05:08.000Z","comments":true,"path":"2020-11-29-C++-常见问题/","permalink":"https://yao-lirong.github.io/blog/2020-11-29-C++-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"char ：1个字节 char*(即指针变量): 8个字节 short int : 2个字节 int： 4个字节 unsigned int : 4个字节 float: 4个字节 double: 8个字节 long: 8个字节 long long: 8个字节 unsigned long: 8个字节 I/O 读入字符串可以用 scanf(\"%s\") 或 getline() 读入字符且忽略空格可以用 scanf(\" %c\")，注意 %c 前面的空格 C++ cin.ignore()的用法详解 C++ cin&gt;&gt; cin.get() cin.getline() unsigned long long int scanf 文件的读取及写入: 12freopen(&quot;myfile.txt&quot;,&quot;r&quot;,stdin);freopen (&quot;myfile.txt&quot;,&quot;w&quot;,stdout); Class and Header It is not possible to create an instance of a class without invoking the constructor STL的使用 为什么 std::vector 不支持 push_front？ 指针的使用 指针变量的传值和传址 C++ delete 和 delete []的区别 NULL和nullptr的区别 类与结构体的使用 C++中结构体与类的区别（struct与class的区别） 定义结构体变量及初始化; 结构体定义变量的三种方法 C++构造函数什么时候会被调用 C++中如何声明两个递归调用的类 String Split a String: strtok: Reference 1, Reference 2 123456789char str[] =&quot;- This, a sample string.&quot;;char * pch;pch = strtok (str,&quot; ,.-&quot;);while (pch != NULL)&#123; printf (&quot;%s\\n&quot;,pch); // Note to use NULL the next time you call strtok pch = strtok (NULL, &quot; ,.-&quot;);&#125; Convert a char array to integer: sscanf(s, \"%i\", &amp;imm) automatically detects whether s is an decimal 142 or a hex 0xa2c (Note the 0x before a hex number is necessary) 123char myarray[5] = &#123;&#x27;-&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;\\0&#x27;&#125;;int i;sscanf(myarray, &quot;%d&quot;, &amp;i); Read in both hex and dec number: scanf(\"%i\", ) Strings in C (char arrays) end with a terminating null-character '\\0' 其它问题 switch statement gives “a label can only be part of a statement…”: switch 后要加分号 ; 12345switch (option) &#123; case &#x27;a&#x27;: ; ... case &#x27;b&#x27;: ; ... &#125;","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Python Manual","slug":"2020-11-29-Python-常见问题","date":"2020-11-29T05:00:00.000Z","updated":"2023-04-11T20:38:42.000Z","comments":true,"path":"2020-11-29-Python-常见问题/","permalink":"https://yao-lirong.github.io/blog/2020-11-29-Python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"python 整型与二进制数值的相互转换 pyhton 的异或与 Hamming Distance Python之正则表达式——查找 Python类型强制转换 multi-line statements: 123total = item_one + \\ item_two + \\ item_three List Find Median of List in Python: statistics.median(list) Union of two lists: to remove all repetitions, use res = list(set().union(lst1, lst2, lst3, ...)) concatenate two lists: lst1 + lst2 Hash a list: you cannot hash a list, because list is mutable. You can only hash immutable objects. Therefore, to hash a list, you first convert it to a tuple: hash(tuple([1,2,3])). Take only the elements indexed at xth multiple to form a new list: lst[::x] so the third argument of : means step, just as in range. Generate range with floats: use np.arange(start, stop, step), or np.linspace(start, stop, num_wanted) Unzip a zipped list: list(zip(*test_list)) note * is the unpacking operator to unpack the iterable into separate elements that can then be passed as arguments to the zip() function. Dict Sort a dictionary: dct= dict(sorted(dct.items(), key=lambda item:item[0])) to sort by keys; change to item[1] to sort by values. Remove an item from dict by key: dct.pop(your_key) Function Access &amp; Change global variable in local functions: you can access global variable in local functions without any other keywords. However, if you want to change the global variable in your local function. You will have to use the global keyword. By using a global keyword, you can either create a global variable in a local function, or link back to a global variable already created. 1234567x = &quot;h&quot;def myfunc(): global x x = &quot;fantastic&quot;myfunc() Change Variable in an Outer Scope: Similar to the global keyword, we have a nonlocal keyword for this purpose. 1234567def foo(): a = 1 def bar(): nonlocal a a = 2 bar() print(a) # Output: 2 Multiple number of arguments to a function: 123def foo(a, b, c, *others): print(a, b, c) print(&quot;And all the rest are &quot;, list(others)) Import a Custom Module: the same as import, but now the module name can be a variable instead of a static string 123package_name = &quot;numpy&quot;package = __import__(package_name) package.array() Class print a class like Java’s toString : 12345class Test: def __repr__(self): # what to display when looked at in an interactive prompt return &quot;Test()&quot; def __str__(self): # what to print when called print(Test) return &quot;member of Test&quot; Self-defined comparator: 12345678910111213class CustomNumber: def __init__(self, value): self.value = value def __lt__(self, obj): &quot;&quot;&quot;self &lt; obj&quot;&quot;&quot; return self.value &lt; obj.value def __le__(self, obj): &quot;&quot;&quot;self &lt;= obj&quot;&quot;&quot; def __eq__(self, obj): &quot;&quot;&quot;self == obj&quot;&quot;&quot; def __ne__(self, obj): &quot;&quot;&quot;self != obj&quot;&quot;&quot; def __gt__(self, obj): &quot;&quot;&quot;self &gt; obj&quot;&quot;&quot; def __ge__(self, obj): &quot;&quot;&quot;self &gt;= obj&quot;&quot;&quot; hash on a custom object: 12345678class Emp: def __init__(self, emp_name, id): self.emp_name = emp_name self.id = id def __hash__(self): # when you want to get the hash, use hash(instance_of_custom_object) return hash((self.emp_name, self.id)) Local variable in a class: Elements outside the __init__ method are static elements; they belong to the class. Elements inside the __init__ method are elements of the object (self); they don’t belong to the class. 1234class MyClass: static_elem = 123 # static def __init__(self): self.object_elem = 456 # specific to eacy instance Exception Self-specified exception: 123456789101112class MyCustomError(Exception): def __init__(self, *args): if args: self.message = args[0] else: self.message = None def __str__(self): if self.message: return &#x27;MyCustomError, &#123;0&#125; &#x27;.format(self.message) else: return &#x27;MyCustomError has been raised&#x27; try catch clause in python: 1234try: print(x)except: print(&quot;Exception thrown. x does not exist.&quot;) String convert string to int: int(s) How to remove the leading and trailing spaces in Python: my_string.strip() 合并一个 String List: \"\".join(str_lst) Join with seperator: \",\".join(str_lst) advanced split with re : re.split(\"split_on_what_in_regex\", str) Extract characters from a string: \"\".join(re.findall(\"[a-zA-Z]+\", str)) Convert String of Digits into a List of Digits: and just to characters 123456num = 2019# If you want a list of integersres = [int(x) for x in str(num)]# If you are good with a list of charactersres = list(str(num)) Format float to scientific computing: print(\"a = %.2e\" %(num)) String Format in General: f-string is a new feature since python 3.6 and you should use it as string formatting convention f\"iter: &#123;i&#125;\" to align signs: '+' indicates that a sign should be used for both positive as well as negative numbers. '-' indicates that a sign should be used only for negative numbers (this is the default behavior). ' ' indicates that a leading space should be used on positive numbers, and a minus sign on negative numbers (most used) 12345678# scientific format with f-stringf&#x27;&#123;num:.5e&#125;&#x27; # float number, use space to also align negative signf&#x27;&#123;num: .3f&#125;&#x27; # align integers to have a fixed lengthf&#x27;&#123;num:3d&#125;# f-string braced evaluation also supports everything (including functions)f&quot;&#123;&quot;Eric Idle&quot;.lower()&#125; is funny.&quot; Data Structures Queue: Python 用的不是 enqueue dequeue，而是 put get 1234import queueq = queue.Queue()q.put(s)v = q.get() Priority Queue: 12from queue import priorityQueueq = PriorityQueue() 函数式编程 python的filter基本用法: lst = list(filter(func, lst)), dct = dict(filter(func, dct)) python3中map()函数用法: map(func, list) Python reduce() 函数: similar to fold_left reduce(lambda acc x : ..., list, init) IO 读入多行文件: lines = file1.readlines() 123456789file_path = os.getcwd() + &quot;\\\\&quot; + file_namef = open(file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;)lines = f.readlines()f = open(file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)f.write(string)f.writelines(lst_of_strs)f.close() 写入csv文件的几种方法 写入csv文件的几种方法总结 获取当前文件夹下所有文件夹名 1filenames = os.listdir(path) Get Parent Directory Name: os.path.dirname(os.getcwd()) Profiling Creating Profiling Data 123456import cProfileprofiler = cProfile.Profile()profiler.enable()# Code goes here profiler.disable()profiler.dump_stats(&quot;execution.stats&quot;) Inspecting Profiling Data 123import pstatsstats = pstats.Stats(&quot;example.stats&quot;)stats.print_stats() The following columns will be shown: ncalls: number of times function was called tottime: amount of time spent in the function (not counting any time spent in subfunctions) percall: tottime / ncalls cumtime: all the time spent in the function and subfunctions percall: cumtime / ncalls filename:lineno(function): name of function that was called and where it is defined A fairly common practice is to sort by one of the above attributes. Or to look at its callees to see where that function wound up spending time. You can also perform the inverse, and look up a function’s callers. This can be helpful if you have a function that is taking a lot of time, but you don’t know who is calling it. 123stats.sort_stats(&quot;cumtime&quot;).print_stats(2) #print first 2 functions that spent highest cumulative timestats.print_callers(&quot;cprofile_example.py:7&quot;) # 7 is line7stats.print_callees(&quot;cprofile_example.py:3&quot;) You can also use the visualization tool snakeviz. Reference: Profiling Python Code with cProfile Profile Memory 123456import tracemalloctracemalloc.start()# Code goes here print(&quot;maximum memory usage is &quot; + str(tracemalloc.get_traced_memory()[1] / 1024 / 1024 / 1024) + &quot; Gb&quot;)tracemalloc.stop() Pip pip freeze to show all installed packages pip show &lt;package_name&gt; to show a specific package NumPy Difference between max and maximum: numpy.maximum(A,B) returns the element-wise bigger one of the two numpy.max(A) returns the maximum value inside A Matrix/Vector Multiplication: np.matmul(A, B): Returns matrix product of A and B np.multiply(A, B): Returns element-wise multiplication of A and B np.dot(A, B): Returns dot product of A and B numpy.diagonal(M): Returns the diagonal of a 2-D matrix M numpy.tile(A, reps): repeats A reps times numpy.where(cond, A, B): condition on array. Really useful function, so is just A if cond else B Solve TypeError: only integer scalar arrays can be converted to a scalar index when you execute a[a == b]: this happens because a is not an np array. It is a list and the message above comes from the list type. reference Convert sclacr to array or to any shape: np.reshape(scalar, (1,1)) When your matrix operations involve inverses A−1, it is always better to use the inverse indirectly than to manifest it explicitly because manifesting it often involves intricate computation that may harm numerical stability. That is, use np.linalg.solve() instead of np.linalg.inv reference np.frompyfunc to more efficiently apply function on numpy arrays: This function is internally called when you apply a function to a np.array, but if the otuput doesn’t meet your expectation, you can use this function to specify what it should do. 1234double = lambda x = 2xnpfunction = np.frompyfunc(f, &lt;input_number&gt;, &lt;output_number&gt;)npf = np.frompyfunc(double, 1, 1)# npf(arr) &lt;==&gt; f(arr) in this particular case For each row, extract the corresponding column: Qs = network(states)[np.arange(actions.shape[0]), actions] network(states) is B × dimA representing for each sample, the value of taking a specific action. actions is vector of B storing which action we actually took. Using this command, we extract the value of taking a specific action at a specific state. Note There are a total B (state, action) pairs. Pandas 1234567891011121314# 直接循环 df 循环的是 col 名for col in df: print(col)# 想要循环每一行的数据应使用 iterrows()# row = (row_index: int, data: pd.Series)for row in df.iterrows(): print(row)# 想要读取某一行的数据使用 loc[i]，返回 pd.Seriesrow0 = df.loc[0]# loc 用来过滤时如果有两个以上条件：只能用&amp;，用and会报错，此外也要用圆括号括起来 df.loc[ (df[&quot;att1&quot;] == &quot;012&quot;) &amp; (df[&quot;code&quot;] == &quot;2A&quot;) ] AttributeError: ‘float’ object has no attribute ‘split’ Mathplotlib import matplotlib.pyplot as plt Change where y range starts in matplotlib: plt.ylim(bottom = x) Rotate the labels in x-axis by 90 degrees: this trick helps you when you have too long x-axis labels. plt.xticks(rotation = 90 ) Output/Save Plot: plt.savefig('filename.png') Change labels, ticks, … Change ticks are applicable when your x-axis is discrete, like [1, 2, 5, 10] and you want any neighboring two only has unit distance instead of, say between 2 and 5 have 3 unit distance. 1234567plt.xlabel(&#x27;X axis&#x27;, fontsize=15)plt.ylabel(&#x27;Y axis&#x27;, fontsize=15) plt.xticks(lst_of_tick_position, labels, color=&#x27;blue&#x27;, rotation=60) # disabling yticks by setting yticks to an empty listplt.yticks([]) Different Kinds of Plot: scatter plot: plt.scatter(x,y) histogram: plt.hist(x,y) 普通折线图: 123x = np.arange(-10,10,0.1)y = 2*xplt.plot(x,y) reset plot: plt.clf() Plot lines w/ custom line label: 123456#plot individual lines with custom colors, styles, and widthsplt.plot(df[&#x27;leads&#x27;], label=&#x27;Leads&#x27;, color=&#x27;green&#x27;)plt.plot(df[&#x27;prospects&#x27;], label=&#x27;Prospects&#x27;, color=&#x27;steelblue&#x27;, linewidth=4)plt.plot(df[&#x27;sales&#x27;], label=&#x27;Sales&#x27;, color=&#x27;purple&#x27;, linestyle=&#x27;dashed&#x27;)plt.legend() Json Json doesn’t dump UTF-8: When you have json output like \\u2019, it may not be your fault. Note the json standard is to escape non-ascii characters even if it’s not needed. You can override this with the following command: 12with open(&#x27;output.json&#x27;, &#x27;w&#x27;) as f: json.dump(posts, f, indent=4, ensure_ascii=False)","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"LaTeX Manual","slug":"2020-11-23-Latex-实用技巧手册","date":"2020-11-23T05:00:00.000Z","updated":"2024-01-23T15:55:07.523Z","comments":true,"path":"2020-11-23-Latex-实用技巧手册/","permalink":"https://yao-lirong.github.io/blog/2020-11-23-Latex-%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E6%89%8B%E5%86%8C/","excerpt":"","text":"Commands 排版大括号 $f(x)=\\left\\{ \\begin{aligned} x &amp; = &amp; \\cos(t) \\\\ y &amp; = &amp; \\sin(t) \\\\ z &amp; = &amp; \\frac {x}{y} \\end{aligned} \\right.$ 1234567f(x)=\\left\\&#123;\\begin&#123;aligned&#125;x &amp; = &amp; \\cos(t) \\\\y &amp; = &amp; \\sin(t) \\\\z &amp; = &amp; \\frac xy\\end&#123;aligned&#125;\\right $f(x)= \\begin{cases} 0&amp; \\text{x=0}\\\\ 1&amp; \\text{x!=0} \\end{cases}$ 1234567\\[f(x)=\\begin&#123;cases&#125;0&amp; \\text&#123;x=0&#125;\\\\1&amp; \\text&#123;x!=0&#125;\\end&#123;cases&#125;\\]% note cases only support one align operator &amp;% and they need to stay in a math block Sections and Chapters: Usually, \\section is the top-level document command in most documents. However, in reports or books, and similar long documents, this would be \\chapter or \\part. To get an unnumbered chapter / section add an asterisk (*) at the end of the command, like \\section*. 大写字母 双写体: ℝ - \\mathbb&#123;R&#125; 粗体: R - \\mathbf&#123;R&#125; 花体: ℛ - \\mathcal&#123;R&#125; Text: Roman Font: \\textrm&#123;&#125; Typewriter Font: \\texttt&#123;&#125; Spacing: \\quad - equal to the current font size (= 18 math unit): between a mathematical symbol and text in displayed expressions 1\\[ E_n(t) \\to e^&#123;-t&#125;\\quad\\text&#123;as &#125;t\\to\\infty \\] \\qquad - double “quad” (= 36 math unit) : between two separate equations 1\\[ x^2 + y^2 = a^2,\\qquad x-y=b \\] \\, (= 3 math unit) is the most commonly seen one with the thinnest space. \\; (= 5 math unit) is the spacing I always use. specify how many inches/cm - \\hspace&#123;0.1in&#125;; you can use \\vspace&#123;1cm&#125; for vertical space ~ inserts a non-breaking space, where you tell LaTeX not to insert a linebreak here Vector: p⃗ - \\vec&#123;p&#125; Self-defined operators: \\operatorname*&#123;argmin&#125; Dot: horizontal dots on the line: … - \\ldots horizontal dots above the line: ⋯ - \\cdots single horizontal dot on the line: ⋅ \\cdot vertical dots: ⋮ - \\vdots diagonal dots: ⋱ - \\ddots Matrix: note this needs to be placed inside a math block $$ \\begin{bmatrix} \\sigma_{11} &amp; \\cdots &amp; \\sigma_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{n1} &amp; \\cdots &amp; \\sigma_{nn} \\end{bmatrix} $$ 12345\\begin&#123;bmatrix&#125; \\sigma_&#123;11&#125; &amp; \\cdots &amp; \\sigma_&#123;1n&#125; \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_&#123;n1&#125; &amp; \\cdots &amp; \\sigma_&#123;nn&#125;\\end&#123;bmatrix&#125; proportional to: ∝ \\propto tilt / similar to: ∼ \\sim Wrap formula with a box $$ \\boxed{h(\\mathbf{x}) = \\operatorname*{argmax}_y \\; \\hat\\pi_y \\prod_{\\alpha=1}^{d} P(x_\\alpha | y)} $$ 1\\boxed&#123; h(\\mathbf&#123;x&#125;) = \\operatorname*&#123;argmax&#125;_y \\; \\hat\\pi_y \\prod_&#123;\\alpha=1&#125;^&#123;d&#125; P(x_\\alpha | y) &#125; Comment in latex: use % Under or above any notation (also can do multiline): $$ \\underset{k\\in N}{E} \\overset{wow}{E} \\underset{\\substack{i \\in N \\\\ j \\in N}}{E} $$ 1234\\underset&#123;k\\in N&#125;&#123;E&#125;\\overset&#123;wow&#125;&#123;E&#125; % to write multi-line, use \\substack\\underset&#123;\\substack&#123;i \\in N \\\\ j \\in N&#125;&#125;&#123;E&#125; Under or above with braces: (For how to overlap multi braces, see the original answer) $$ \\underbrace{(x + 2)^3}_\\text{text 1} \\quad \\overbrace{(x - 3)}^\\text{text 2} $$ 12\\underbrace&#123;(x + 2)^3&#125;_\\text&#123;text 1&#125;\\overbrace&#123;(x - 3)&#125;^\\text&#123;text 2&#125; Multiple lines of subscript / under (There is no known multiple lines of superscript) $$ \\sum_{\\substack{a=b \\\\ b=c}} $$ 1\\sum_&#123;\\substack&#123;a=b \\\\ b=c&#125;&#125; Ceiling and floor: ⌊x⌋ ⌈x⌉ 12\\lfloor x \\rfloor \\lceil x \\rceil Make something to be centered even though its component has a non-zero width $$ \\int_{-\\infty}^{\\infty} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\, dx \\\\ \\int_{-\\infty}^{\\infty} \\mathclap{e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}} \\, dx $$ 12\\int_&#123;-\\infty&#125;^&#123;\\infty&#125; e^&#123;-\\frac&#123;(x-\\mu)^2&#125;&#123;2\\sigma^2&#125;&#125; \\, dx \\\\\\int_&#123;-\\infty&#125;^&#123;\\infty&#125; \\mathclap&#123;e^&#123;-\\frac&#123;(x-\\mu)^2&#125;&#123;2\\sigma^2&#125;&#125;&#125; \\, dx Different types of alignment: align: numbers the equation (you can use \\nonumber after a specific line to suppress numbering) align*: doesn’t number the equation alignat: doesn’t add space between the columns (rl pairs), but you have to tell it how many columns you have (It works if you have fewer columns than you specified but doesn’t if otherwise. So in theory you can just set a super big number to it) 123456\\begin&#123;alignat*&#125;&#123;3&#125;&amp; m \\quad &amp;&amp; \\text&#123;módulo&#125; \\quad &amp;&amp; m&gt;0\\\\&amp; a \\quad &amp;&amp; \\text&#123;multiplicador&#125; \\quad &amp;&amp; 0&lt;a&lt;m\\\\&amp; c \\quad &amp;&amp; \\text&#123;constante aditiva&#125; \\quad &amp;&amp; 0\\leq c&lt;m\\\\&amp; x_0 \\quad &amp;&amp; \\text&#123;valor inicial&#125; \\quad &amp;&amp; 0\\leq x_0 &lt;m\\end&#123;alignat*&#125; $$ \\begin{alignat*}{3} &amp; m \\quad &amp;&amp; \\text{módulo} \\quad &amp;&amp; m&gt;0\\\\ &amp; a \\quad &amp;&amp; \\text{multiplicador} \\quad &amp;&amp; 0&lt;a&lt;m\\\\ &amp; c \\quad &amp;&amp; \\text{constante aditiva} \\quad &amp;&amp; 0\\leq c&lt;m\\\\ &amp; x_0 \\quad &amp;&amp; \\text{valor inicial} \\quad &amp;&amp; 0\\leq x_0 &lt;m \\end{alignat*} $$ Basic Knowledge align with &amp; An align is a table-like structure, and &amp; is a column separator. The thing is that the columns in an align are rlrlrlrlrl..., that is, every other column is right aligned and left aligned. So, below the a is in a right aligned column, while =b is left aligned 1a &amp;= b In following, text will be in a right aligned column, 1a &amp;= b &amp; text We should note that &amp;&amp; is just &amp; &lt;no code here&gt; &amp;, Therefore, if you do the following, text will be in a left aligned column, as you basically just add an empty column between =b and text 1a &amp;= b &amp;&amp; text reference Best Practices \\[ \\] over $$ $$ for display math: $$ is TeX primitive syntax and it’s pretty much deprecated. You should always use \\[ \\]. more detailed reasons \\( \\) over $ $ for inline math: similarly, $ is old TeX syntax whereas \\( \\) is LaTex syntax. People suggest choose at your own preference for this inline math one since the old TeX syntax does provide much more readability. more discussion here Helper TikzEdt: A “what you see is what you get” Tikz editor. Tikz is a very powerful graph drawing package for LaTeX Reference Overleaf - Spacing in Math Mode Detailed Spacing Explanation Referring to Mathematics into Type","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Python网络爬虫与信息提取","slug":"2020-11-17-Python网络爬虫与信息提取","date":"2020-11-17T05:00:00.000Z","updated":"2025-04-10T04:23:04.996Z","comments":true,"path":"2020-11-17-Python网络爬虫与信息提取/","permalink":"https://yao-lirong.github.io/blog/2020-11-17-Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/","excerpt":"Python网络爬虫与信息提取 - 嵩天","text":"Python网络爬虫与信息提取 - 嵩天 规则 requests.get() r = requests.get(string url, timeout = n) r 是一个 response 类，它包含如下属性 r.status_code: 返回码 r.text: HTTP 相应内容的字符串形式 r.encoding: 从 header 中猜测的网页编码方式 r.apparent_encoding: 从内容中分析出来的编码方式 r.content:HTTP 相应内容的二进制形式 如果过 n 秒后还没有成功得到服务器的相应/从服务器读取数据，程序会 throw TimeOutError requests 库的异常 在 request 网络连接时，我们一定要用 r.raise_for_status() 来在返回码不是200正常时，raise exception，然后用 try ... catch ... 语句保证异常被有效处理 requests.ConnectionError: 网络连接错误异常，如DNS查询失败、拒绝连接等 requests.HTTPError: HTTP错误异常 requests.URLRequired: URL缺失异常 requests.TooManyRedirects: 超过最大重定向次数，产生重定向异 requests.ConnectTimeout: 连接远程服务器超时异常 requests.Timeout: 请求URL超时，从整个发起URL请求开始产生超时异常 HTTP 协议对资源的操作 GET: 请求获取URL位置的资源 HEAD: 请求获取URL位置资源的响应消息报告，即获得该资源的头部信息，节省带宽 POST: 请求向URL位置的资源后附加新的数据，不改变现有内容，只是增加 PUT: 请求向URL位置存储一个资源，覆盖原URL位置的资源 PATCH: 请求局部更新URL位置的资源，即改写该处资源的部分内容 （与 PUT 相比节省带宽） DELETE: 请求删除URL位置存储的资源 requests.request() r = requests.request(string method, string url, **kwargs) method 对应上文六种方式 Optional Arguments: params=kv: 其中 kv 是一个 dict 值，在 url 后代入几个参数，形如 ?k1=v1&amp;key2=v2&amp;... headers = hd: hd 是一个 dict 值，定制访问服务器时的 header 字段，有的时候网站会保护自己的数据不允许爬虫访问，我们改变 headers 以后就可以伪装成浏览器 data = dt, json = js: 在 post或put时将 dt 或 js 传给服务器 Robots 协议 通过这个协议对网络爬虫能爬取的东西进行限制，一般就在网络的根目录url/robots.txt 下，一个例子是京东的协议 如果你的爬虫速度非常慢，访问次数很少，访问量不大（近似人类行为），原则上可以不遵守 robots 协议 提取 BeautifulSoup 的基本元素 from bs4 import BeautifulSoup s = BeautifulSoup(r.text, \"html.parser\") returns the r.text restructured as html type. s.tag: 标签，最基本的信息组成单元，对应 html 中的一对尖括号，下文中 t = s.tag t.name: 标签的名字，比如 &lt;p&gt;&lt;/p&gt; 的名字是 p t.attrs: 标签的属性，以字典形式存储。&lt;p class=\"...\" id=\"...\"&gt;&lt;/p&gt; t.string: 两个尖括号间的字符内容，即&lt;p&gt;...&lt;/p&gt; 中的 ... 使用 BeautifulSoup 遍历 HTML h = s.head 返回 soup 中的第一个 head 标签 BautifulSoup 的每个类型都带有 pretify() 函数，优化打印效果: h.prettify(), s.prettify(), … 下行遍历 h.contents: 返回一个列表，包含此 tag 下的所有儿子节点 h.children: 返回一个所有儿子节点的 iterator, 即可以使用 for c in h.children: ... 遍历儿子节点 h.descendant: 返回所有子孙节点的 iterator 上行遍历 h.parent: 返回 h 的父亲标签 h.parents: 返回 h 的 ascendant 的 iterator，即一直到根节点的所有节点，用来循环遍历 ascendant 平行遍历 注意树中可能存在 NavigableString, Tag, Comment 种种类型，我们需要进行判断，不能假设平行遍历的下一个绝对是 tag next_sibling: 返回按照HTML文本顺序的下一个平行节点标签 previous_sibling: 返回按照HTML文本顺序的上一个平行节点标签 next_siblings: 返回按照HTML文本顺序的后续所有平行节点标签的 iterator previous_siblings: 返回按照HTML文本顺序的前续所有平行节点标签的 iterator 信息标记的形式 XML (Extensible Markup Language): 通过标签表达所有信息；可扩展性好，但比较繁琐；用于互联网上的信息交互和表达 用一对标签表达其中的信息&lt;tag&gt; ... &lt;/tag&gt;，如果没有内容，则只用一个标签（即一对尖括号）来表达&lt;tag/&gt;. JSON (JavaScript Object Notation): 有类型的键值对；适合程序处理直接使用；用于云端和客户端的通信，一般在接口处使用 YAML (YAML Ain’t Markup Language): 无类型的键值对；文本信息比例最高，可读性高；用于系统配置文件中 使用缩进表达所属关系 123name : oldName: 延安自然科学院 newName: 北京理工大学 - 表示并列关系 123name: -延安自然科学院-北京理工大学 | 表达整块数据（可以跨越多行），# 表达注释 12text: | #学校介绍北京理工大学天下第一bulabulabula 使用 BeautifulSoup 提取信息 我们使用find_all 来提取HTML页面中的信息： s.find_all(name, attrs, recursive, string, ...) 返回一个存储着查询结果的列表 name: 对标签名称的检索字符串 可以是一个字符串： 寻找所有 a 标签 s.find_all(\"a\") 也可以是一个包含字符串的列表：寻找所有 a 标签和 b 标签 s.find_all([\"a\",\"b\"]) 可以是一个正则表达式：寻找所有名字中包含 b 的标签s.find_all(re.compile(\"b\")) 会返回 b 标签和 body 标签 attrs: 对标签属性值的检索字符串 查找包含某一属性的标签：s.find_all(\"p\", \"course\") 返回所有带有 course 这个 attribute 的 p tag 查找包含某一属性等于指定值的标签：s.find_all(id = \"link1\") 返回 [&lt;a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\"&gt;Basic Python&lt;/a&gt;] recursive: 表示是否对子孙全部检索的布尔值，默认True string: 对标签中的字符串进行搜索：s.find_all(string = re.compile(\"python\")) 返回所有包含有 python 的标签内容 简易表示方法：&lt;tag&gt;(..)等价于 &lt;tag&gt;.find_all(..); soup(..) 等价于 soup.find_all(..) 实战 如果我们不需要通过分析页面架构，只是通过在 html 文件中搜索就可以得到想要的信息的话，这无疑是最简单的。这种情况可能发生于信息存在键值对中，如此搜索变得很方便 raw string: r'[1-9]\\d&#123;5&#125; 转义符 escape character 不转义，表达其在字符串中的原意 string: '[1-9]\\\\d&#123;5&#125;' 转义符 escape character 有 Re 库的基本使用 re.search(pattern, string): 在 string 中搜索匹配 pattern 的第一个位置，返回 match 类型 re.match(p, s): 从 s 的开始位置起匹配 p，返回 match 类型 re.findall(p, s) : 搜索 s ，以列表类型返回全部与 p 匹配的子串 re.split(p, s, maxsplit=n): 将 p 按照 s 匹配结果进行分割，将匹配的部分去掉，其他部分分割成子串存在列表中；可选参数 maxsplit 最多分为 n 个部分 re.sub(p, repl, s, count=n): 将 n 个 s 中与 p 匹配的子串替换成 repl，n如果未定义则默认替换所有 针对对于同一正则表达式的多次操作，我们可以将一个正则表达式编译为一个 pattern 类型，之后就可以多次使用针对这个类型的以上函数操作 regex = re.compile(pattern) regex.search(s), regex.match(s), ... Re 库的 match 对象 match 对象就是一次匹配的结果，包含许多关于这次匹配的信息 .string: 待匹配的文本 .re: 匹配时使用的patter对象（正则表达式） .pos: 正则表达式搜索文本的开始位置 （即在 string 中搜索的范围） .endpos: 正则表达式搜索文本的结束位置 .group(0): 获得匹配后的字符串 .start(): 匹配结果在原始字符串的开始位置 .end(): 匹配结果在原始字符串的结束位置 .span(): 返回(.start(), .end()) 其他技巧 中文的打印与对齐 我们一般使用 string.format() 函数格式我们的输出，用法：theFormat.format(parameter1, parameter2, ...) formatter string 中有几个参数：（注意必须严格遵守这个顺序） &#123;n&#125; 表示使用第 n 个参数填充这个部分，不指定则按照与参数的对应顺序来 &#123;:p&#125; 使用参数 p 填充，不指定则默认为空格 &#123;:&lt;n&#125; &#123;:^n&#125; &#123;:&gt;n&#125; 左对齐，居中对齐，右对齐，宽度为 n &#123;:,&#125; 在千位用逗号分隔数字 &#123;:.n&#125; 浮点数保留小数点后 n 位 &#123;:&lt;type&gt;&#125; 可以是 d 代表整形，f 代表浮点型，e 代表科学计数法浮点型，% 代表百分号浮点型 \"&#123;0:^10&#125;\\t&#123;1:&#123;2&#125;^10&#125;\".format(\"排名\",\"学校名称\",chr(12288)) 打印 第零个参数(居中对齐宽度为10)，打印 tab，打印第一个参数(用第2个参数进行居中填充宽度为10) 第二个参数是中文 \"&#123;1:x&lt;20,.3f&#125;\".format(0,21031295.21413) 打印 ‘21,031,295.214xxxxxx’ 在 UTF-8 编码中，对应的中文空格字符是 chr(12288).","categories":[],"tags":[]},{"title":"CS4820 及 Algorithm Design 一般性内容总结","slug":"2020-10-13-Algorithm-Design-及-CS4820-一般性内容总结","date":"2020-10-13T04:00:00.000Z","updated":"2022-06-08T20:13:42.000Z","comments":true,"path":"2020-10-13-Algorithm-Design-及-CS4820-一般性内容总结/","permalink":"https://yao-lirong.github.io/blog/2020-10-13-Algorithm-Design-%E5%8F%8A-CS4820-%E4%B8%80%E8%88%AC%E6%80%A7%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93/","excerpt":"CS 4820 develops techniques used in the design and analysis of algorithms, with an emphasis on problems arising in computing applications. Example applications are drawn from systems and networks, artificial intelligence, computer vision, data mining, and computational biology. This course covers four major algorithm design techniques (greedy algorithms, divide and conquer, dynamic programming, and network flow), computability theory focusing on undecidability, computational complexity focusing on NP-completeness, and algorithmic techniques for intractable problems, including identification of structured special cases, approximation algorithms, and randomization.","text":"CS 4820 develops techniques used in the design and analysis of algorithms, with an emphasis on problems arising in computing applications. Example applications are drawn from systems and networks, artificial intelligence, computer vision, data mining, and computational biology. This course covers four major algorithm design techniques (greedy algorithms, divide and conquer, dynamic programming, and network flow), computability theory focusing on undecidability, computational complexity focusing on NP-completeness, and algorithmic techniques for intractable problems, including identification of structured special cases, approximation algorithms, and randomization. Greedy Algorithm Greedy Stays Ahead Greedy is at least as good as the optimal solution in each step Exchange Argument Take any optimal solution, we can make it exactly the same as our greedy solution without having the optimal solution produce a worse result. There is some “structure” unique to this problem. All solutions have this “structure” give the same number of lateness. Our greedy solution has this “structure” We can exchange any optimal solution to have this “structure” without making this solution worse Divide and Conquer Master theorem says that for an algorithm with running time $T(n) = aT(\\frac{n}{b}) + f(n)$. f(n) is some polynomial of n, so we have $T(n) = aT(\\frac{n}{b}) + O(n^c)$. a = bc: T(n) = O(nc logn) - A balance between constant work at each level and number of subproblems at each level. a &lt; bc: T(n) = O(nc) - Time dominated by the constant work we do at upper levels: take a = 1 as an extreme example, all of the time will be spent on top level. a &gt; bc: T(n) = O(nlogba) - Time dominated by each subproblems we have as the recursion go deeper. A lot of branches of subproblems will be generated. Network Flow Max flow 问题转换为 Min Cut 问题，Min Cut 问题永远可以给自己不想要的边 infinite capacity 来将它排除在 min cut 之外。 effectively infinite: 任何一个无法达到的数，都可以视作 infinite，比如 infinite capacity 可以是一个已知的 cut 值+1 (max flow 必然小于任意一个 cut，所以没有任何一个 flow 可以达到 cut + 1) NP Proving Reduction Show that your reduction σ takes polynomial time. Show that x is a solution to the problem you are reducing from if and only if σ(x) is a solution to the problem you are trying to show is NP-hard. You need to show the implication in both directions. Proving NP, NP-Hard, NP-Completeness NP: prove you can verify a solution in polynomial time NP-hard: prove some known NP-Hard (or NP-complete) problem can be reduced to A in polynomial time (注意是别的已知问题可以被转换成我们要证明的问题) NP-completeness: it is NP-hard and it is NP Important NP-Complete Problem satisfiability problems: Boolean satisfiability, CNFSAT (conjunctive normal form satisfiability) , 3CNFSAT (aka 3SAT) graph problems: Clique, Independent Set, Vertex Cover, Dominating Set, Colorability, Planar 3-colorability covering problems: Set Cover, 3-dimensional matching (3DM) tour problems: directed and undirected Hamiltonian circuit (HC), Traveling Salesperson (TSP) numerical problems: Subset Sum (SS), Partition, Knapsack, Bin Packing Tips If a problem asks you to decide if there exists a set of at least k objects satisfying some property, try reducing from another problem that involves picking at least k objects, e.g. Independent Set or Clique. Similarly, if a problem asks you to decide if there exists a set of at most k objects satisfying some property, try reducing from another problem that involves picking at most k objects, e.g. Vertex Cover or Set Cover. When reducing Independent Set / Vertex Cover to another graph-like problem. We find out adding a node representing edges is very useful. (Dominating Set, practicefsol 4, fakesol7 3) When a problem does not easily fit into either of the general categories listed above, usually the best thing to try first is 3CNFSAT. When do the reduction from A to B, try to reduce A to a special case of B. (hw5 P3 Clique -&gt; Submatrix Domination, hw6 P2 Vertex Cover -&gt; Dominating Set) Turing Machine Decidability Give a total Turing Machine (one that always halts) to accept any “yes” instance and reject any “no” instance Undecidability Prove by Diagonalization Prove by Reduction: we usually reduce our problem to Halting Problem or the complement of it (Non-Halting Problem aka. Looping Problem). Note: σ in this case has to be computable instead of polynomial-time To prove some problem is undecidable within a certain time bound, use clocked diagonalization. Crucial Facts Minimum Spanning Tree cut property: Let A and B partitions vertices V, if e is the minimum edge connecting A and B, e must be in every minimum spanning tree. cycle property: Let C be any cycle in G, e be the maximum cost edge on that cycle, e is not in any minimum spanning tree Proof Techniques Loop Invariant and Recursion: 一个很好的例子是 T7.42 的证明 recursion = induction loop invariant = induction hypothesis termination condition = basis computation = logic – Dexter Kozen","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"INFO1998 Intro to Machine Learning (sklearn, pandas)","slug":"2020-10-02-INFO1998-Intro-to-Machine-Learning","date":"2020-10-02T04:00:00.000Z","updated":"2022-06-08T19:30:54.000Z","comments":true,"path":"2020-10-02-INFO1998-Intro-to-Machine-Learning/","permalink":"https://yao-lirong.github.io/blog/2020-10-02-INFO1998-Intro-to-Machine-Learning/","excerpt":"The goal of this course is to provide you with a high-level exposure to a wide range of Data Science techniques and Machine Learning models. From the basics of getting your Jupyter environment setup, to manipulating and visualizing data, to building supervised and unsupervised models, this class aims to give you the base intuition and skillset to continue developing and working on ML projects. We hope you exit the course with an understanding of how models and optimization techniques work, as well as have the confidence and tools to solve future problems on your own.","text":"The goal of this course is to provide you with a high-level exposure to a wide range of Data Science techniques and Machine Learning models. From the basics of getting your Jupyter environment setup, to manipulating and visualizing data, to building supervised and unsupervised models, this class aims to give you the base intuition and skillset to continue developing and working on ML projects. We hope you exit the course with an understanding of how models and optimization techniques work, as well as have the confidence and tools to solve future problems on your own. Lec2 Data Manipulation Introduction to Pandas Series: one dimensional array DataFrame: 2-D table Filtering DataFrames: loc Cleaning-Up DataFrames: df.dropna(), df[df['Open'].notnull()] (These two methods both return a new DataFrame instead of modifying the existed one) View DataFrames: head, tail, … Summary Statistics: mean, median, … describe Dealing with missing data Fill in some random info of our choice: 12#if we there is no record about which cabin he is in, we assume he is on the Top Deckdf[&#x27;Cabin&#x27;]=df[&#x27;Cabin&#x27;].fillna(&#x27;Top Deck&#x27;) Using summary statistics: fill missing entries with median or mean works well with small set Use regression and clustering: will be covered later Lec3 Data Visualization Types of Graphs Heatmap Correlation Plots Coloring Graphs plt.scatter(Longitude, Latitude, c=Temp.values.ravel(),cmap=plt.cm.OrRd) color a scattered plot based on values of Temp with color scheme cm.OrRd. Find more color schemes from matplotlib manual. Lec4 Linear Regression Preparing Data 12345678910from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_split# X must be a table (in case there are multiple x in y = a1*x1 + a2*x2 + ... + k)X = data[[&#x27;cost&#x27;,&#x27;compl_4&#x27;]] # Y must be one columnY = data[&#x27;median_earnings&#x27;] from sklearn.model_selection import train_test_split# test is 20% of all datax_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) Predicting and Fitting 12345678# creates Linear Regression model LR = LinearRegression()# note LR is an object by calling fit, we set all of its coefficientsLR.fit(x_train, y_train)# predict() returns the predicted valuey_predicted = LR.predict(x_test)# score(x,y&#x27;) first computes the predicted value y based on x and our model, then compare it with y&#x27;score = LR.score(x_test,y_test) Describing the Model 12345678# Gives a comprehensive view of Y = a1*x1 + a2*x2 + ... + kLR?# coefficients of x (a1, a2, ...)LR.coef_# intercept kLR.intercept_ Lec5 Measuring Model’s Accuracy When determining accuracy, usually want to compare our model to a baseline. Therefore, instead of comparing our model’s prediction to each specific y value, we compare it with the mean y value. 123456from sklearn.metrics import mean_squared_errorcelcius_MSE = mean_squared_error(y_test, celcius_predictions)test_goal_mean = y_test.mean()baseline = np.full((len(celcius_predictions),), test_goal_mean)baseline_MSE = mean_squared_error(baseline, celcius_predictions) overfitting: too specific to the data given, doesn’t predict any other data underfitting: no matter what data you use to train this model, it gives the same curve, so it doesn’t have prediction power either because it doesn’t show any pattern of the data. Lec6 Classifiers Linear regression is used to predict the value of a continuous variable. Classifiers are used to predict categorical or binary variables. KNN 12345678from sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)k = 10model = KNeighborsClassifier(k) # specify k nearest elementsmodel.fit(x_train,y_train)predictions = model.predict(x_test) Lec7 Other Supervised Learning Models Decision Trees 123from sklearn import treefrom sklearn.tree import DecisionTreeClassifiermodel = tree.DecisionTreeClassifier(max_depth=5) How to reduce overfitting? Reduce levels of trees Train multiple decision trees (maybe one for each training data) and take its average as final result Logistic Regression Value always between 0 and 1. Accept if value higher than threshold, reject if lower. K-fold Cross Validation Rather than doing test-train split only once, we do it k times: First separate our sample into k pieces and each time we take one of them as test set, the others as training set. Use from sklearn.model_selection import KFold to achieve this. Calculate a score for each of the split and take its average as the final score. This score is usually closer to real errors. 12345678910111213141516171819202122from sklearn.model_selection import KFoldfrom sklearn.metrics import mean_squared_error, accuracy_scoreincX = inc_data[[&#x27;education.num&#x27;]]incY = inc_data[&#x27;income&#x27;]kf = KFold(n_splits = 5)accuracy = 0for train_index, test_index in kf.split(incX): X_train = incX.iloc[train_index] Y_train = incY.iloc[train_index] X_test = incX.iloc[test_index] Y_test = incY.iloc[test_index] # best_depth 是我们前一题找到的使分最高的 depth level of decision tree model = tree.DecisionTreeClassifier (max_depth = best_depth) model.fit(X_train, Y_train) pred_test = model.predict(X_test) accuracy += accuracy_score(Y_test, pred_test) accuracy /= 5print(accuracy) Lec9 Unsupervised Learning Supervised Learning: The desired solution (target) is also included in the dataset Unsupervised Learning: The training data is unlabeled and algorithm tries to learn by itself Hierarchical Clustering Hierarchical clustering groups observations into multiple levels of sets; the top-level set includes all of the data, and the bottom-level sets contain individual observations. The levels in between contain sets of observations with similar features. 1234567891011from sklearn.preprocessing import StandardScalerfrom scipy.cluster.hierarchy import dendrogram, linkagefrom matplotlib import pyplot as plt# Standardize features by removing the mean and scaling to unit variancedata = StandardScaler().fit_transform(data)# build our model from dataclust = linkage(data) # draw the dendrogram visulizationdendrogram(clust)plt.show() K-Means Clustering We want to cluster the data into k groups. We first randomly choose k points in this dataset. Then we assign other data points to the group they are closest to. After assigning all data points to some group, we recompute the center of each group by taking the means of all points in that group. Repeat this process until no points change group assignment after one iteration. 1234from sklearn import clusterk = 3kmeans = cluster.KMeans(n_clusters = k) #cluster into k groupskmeans.fit(data)","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Add \"Open with Windows Terminal\" to Right-Click Menu","slug":"2020-09-29-Add-Open-with-Windows-Terminalto-Right-Click-Menu","date":"2020-09-29T04:00:00.000Z","updated":"2022-06-08T19:34:22.000Z","comments":true,"path":"2020-09-29-Add-Open-with-Windows-Terminalto-Right-Click-Menu/","permalink":"https://yao-lirong.github.io/blog/2020-09-29-Add-Open-with-Windows-Terminalto-Right-Click-Menu/","excerpt":"Windows Terminal Preview now provides native support of this feature.","text":"Windows Terminal Preview now provides native support of this feature. Download the icon here Move the icon to directory C:\\Users\\&lt;your username&gt;\\AppData\\Local\\WindowsTerminal Create a .reg file and run it. 123456789Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\wt]@=&quot;Open with Windows Terminal&quot;&quot;Icon&quot;=&quot;C:\\\\Users\\\\&lt;your username&gt;\\\\AppData\\\\Local\\\\WindowsTerminal\\\\terminal.ico&quot;[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\wt\\command]@=&quot;C:\\\\Users\\\\harmo\\\\&lt;your username&gt;\\\\Local\\\\Microsoft\\\\WindowsApps\\\\wt.exe&quot; In Windows Terminal’s settings, Add \"startingDirectory\" : \".\" into defaults list: 1234&quot;defaults&quot;: &#123; &quot;startingDirectory&quot; : &quot;.&quot; &#125;, Reference How to Add Open Windows Terminal Here Option to Right-click Menu Add “open Windows terminal here” to right-click context menu 将”在此处启动Windows Terminal”添加到右键菜单","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"CS2024 C++ Programming","slug":"2020-09-07-CS2024-C++-Programming","date":"2020-09-07T04:00:00.000Z","updated":"2022-06-08T19:30:10.000Z","comments":true,"path":"2020-09-07-CS2024-C++-Programming/","permalink":"https://yao-lirong.github.io/blog/2020-09-07-CS2024-C++-Programming/","excerpt":"The goal of CS2024 is to teach as much of the C++ language as possible with an eye towards your being able to use it effectively in future classes that may depend on it and/or in a professional setting. C++ is ever changing with new standards released every three years. We look to strike a balance between making sure you thoroughly understand “historic” C++ as well as introducing you to new features enabled in the language in the past decade.","text":"The goal of CS2024 is to teach as much of the C++ language as possible with an eye towards your being able to use it effectively in future classes that may depend on it and/or in a professional setting. C++ is ever changing with new standards released every three years. We look to strike a balance between making sure you thoroughly understand “historic” C++ as well as introducing you to new features enabled in the language in the past decade. Lec01 Introduction Explaining our First Program #include &lt;iostream&gt; Tells the compiler that we would like to load definitions from a header file named “iostream”. The # (pound sign) indicates this is a preprocessor directive, it gets dealt with BEFORE your code is compiled std::cout &lt;&lt; “Hello World!” &lt;&lt; std::endl; &lt;&lt; is an operator that directs content from the right to the left. In this case, we direct the string “Hello World” to std::cout, which is the console Compiling C++ Windows: use Visual Studio Linux: g++ -std=c++11 -lstdc++ -o demo1 demo1.cpp: -o specifies the name of the compiled file Compiler takes the text of the source code and converts it into a binary object so that it can execute it a bit more efficiently. Lec02 Input/Output and Operators Input and Output &gt;&gt; stream extraction operator std::cin &gt;&gt; k take a value from cin, which is the input stream keyboard, and assign it to k getline(cin,str): cin uses space as delimiter so it won’t read in a whole line. Use this to read a full line Using using is similar to import in java, so that you don’t have to use the full name of a function when calling it. 1234567using std::cout;using std::endl;int main(int argc, char *argv[]) &#123; // No longer need to use the std:: prefix cout &lt;&lt; “Hello World” &lt;&lt; endl;&#125; Lec03 Introduction to Classes Struct C-Style structure definition: (Define a structure called Course, which has three fields ) 12345typedef struct &#123; string name; string instructor; int numStudents;&#125; Course; Classes Variables defined inside that class are called member variables. Functions defined inside the class are called member functions Public vs. Private public and private keywords can appear as many times as you want in the class definition. 1234567891011class Course &#123;public: // These can be seen outside the class // Define member functions int getStudentCount() &#123; return numStudents; &#125;private: // These can be seen inside the class only // Define member variables string name; string instructor; int numStudents;&#125; Declaration and Definition of Member Functions You don’t have to define the functions where they are declared. Instead, you can define them outside of the class declaration. When you define them outside of the class declaration, you can still access the member variables inside that class. That’s because you are telling the compiler that this is a member function. 12345678910111213141516class Course &#123;public: // These can be seen outside the class // Define member functions int getStudentCount(); void setStudentCount(int count); private: ...&#125;string Course::getCourseName()&#123;return name;&#125;int Course::getStudentCount()&#123;return numStudents;&#125; You usually want to define your getter and setter functions inside class definition. When other functions you are trying to define are too big, we usually define them outside the class definition and usually in a separate file. So we declare the functions in header file **.h and define them in another file **.cpp 123456789101112/* &lt;Courses.h&gt; */class Course &#123;private: void complexLogic();&#125;/* &lt;Courses.cpp&gt; */#incldue &quot;Courses.h&quot;void Courses::complexLogic()&#123; ...&#125;; Constructors Constructors have to have the same name as the class. Constructors have no return type. You can define Constructors outside of class definition too. Constructors are called when you declare an instance of that type: MyClass instance. Note defining a pointer of that class without allocating memory to that pointer MyClass *p will not call the constructor, but declaring a pointer and allocating memory will call the constructor, because that’s the real time an instance is created MyClass *p = new MyClass(). Lec5 Functions I Enum If you don’t assign values to the ones following the first, they will all have value of previous increment 1. 12345678// Define error codesenum RonsError &#123; cNoError = 0, // Values are optional, default is 0 cBadArg, // If a value is not present, cBadResult, // assign previous value + 1 cUnknownErr&#125;; In C++11, we can use the class keyword to define sort of a “namespace” for the enum. 12345678enum class Months &#123; JAN = 1, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC&#125;if ((month == Months::DEC) || (month &lt; Months::MAR)) ...Months get_march()&#123; return Months::MAR;&#125; Function Declaration and Definition Revisited 123456789101112// mymath.h -- header file for math functionslong squareIt(long);// mymath.cpp -- implementation of math functionslong squareIt(long x)&#123; return x * x;&#125;// main.cpp#include “mymath.h”void main()&#123; cout &lt;&lt; “5 squared is “ &lt;&lt; squareIt(5) &lt;&lt; endl;&#125; You should never include a “.c++” file in another c++ file. Lec6 Function II Inline Functions 12345inline int performAddition(int x,int y) &#123; return x+y;&#125; Wherever this function is called the compiler has the option of replacing the call with the body of the actual function, instead of creating a memory stack for that function call and etc. The compiler may not do that when it’s a recursive call or that function is really long. Pass By Reference Why and when do you want to use pass by reference? You need to return multiple values. C++ only allows you to return one value. So you send those values as pass by reference parameters You are passing a large structure/class. When passing values, the compiler will make a copy of those structure/class and pass them, which takes up a lot of stack space. In the second case, maybe you don’t want to change anything in the structure, but passing by reference makes such a mistake likely to happen. To fix this, you can declare this passed by value as const, so when you accidentally modify it, you will get a compile-time error. 12345bool isBusy(const BIGDataType &amp;arg1)&#123; if (arg1.busyField = 0) return true; return false;&#125; Default Argument When we declare a function, we can set a default value to its argument. (Don’t set a default value in function definition) 12345678910class Counter &#123; … void increment(int incrementBy=1); … &#125;;void Counter::increment(int incrementBy);&#123; mycount += incrementBy;&#125; x.increment(); // increment x by 1 y.increment(2); // increment y by 2 Unary Scope Operator When you have 3 variables with the same name defined in global scope, local scope, and a nested scope inside local scope and you want to access the variable in the global scope inside some scope, you can use the :: before calling this variable. There is no way for you in the nested scope to access the variable with a same name in local scope (parent scope). 123456789int x=1; // in the global scopeint main(int argc, char *argv[]) &#123; int x = 6; // local variable to main() // cannot be accessed in the following nested scope &#123; int x = 5; // local variable in a sub-scope of main() cout &lt;&lt; “x is : “ &lt;&lt; ::x &lt;&lt; endl; // &quot;x is : 1&quot; &#125;&#125; Lec7 Function III Function Templates 1template &lt;typename a,typename b,…&gt; return_type function_name (formal args) At compilation time the compiler will look at your code and generate a separate function for each type used throughout your code when calling template functions. For example, for this maximum below, when the compiler sees the call to maximum(3,5,8), it uses the function template to automatically generate an overloaded version of maximum() that takes three variables of type int as its arguments. 12345678910template &lt;class T&gt; T maximum(T val1, T val2, T val3)&#123; T maxValue = val1; if (val2 &gt; maxValue) maxValue = val2; if (val3 &gt; maxValue) maxValue = val3; return maxValue;&#125;return maximum(3,5,8); Lec8 Arrays and Vectors Arrays Arrays don’t have boundary checking. 12345678910111213#include &lt;array&gt;// initializationconst int size = 5;array&lt;int,size&gt; myArray;// range based for-loopfor (int item : myArray) cout &lt;&lt; “Next item is: “ &lt;&lt; item;// sorting and searchingsort(myArray.begin(),myArray.end()); //ascending orderbool found = binary_search(myArray.begin(),myArray.end(),2); Vectors 123456#include&lt;vectors&gt;vector&lt;int&gt; primeVector&#123;2,3,5,7,11,13&#125;; primeVector[6] = 17; //valid syntax but can crash the programprimeVector.at(6) = 17; //involves boundary checking and throw an error Lec9 Pointers Dynamic Allocation 123int *iPtr; // declares a pointer to intiPtr = new int; // &quot;new int&quot; gives a dynamically allocated instance of int // then we assign this space to iPtr Note: in the above example, a memory in the heap is allocated to this pointer iPtr contains one of the following: A pointer to the newly allocated data type (in this case, an int) NULL (if the pointer could not be allocated due to insufficient memory) We should always check whether it is NULL before using a dynamically allocated pointer. We can use delete iPtr to dispose a dynamically allocated pointer. 12345int *iPtr; // iPtr points to some random memoryiPtr = new int; // iPtr points to some memory allocated to it in heap*iPtr = 5; // write 5 to the memory iPtr is allocated todelete iPtr; // release the memory assigned to iPtr / iPtr now no longer points to that memoryreturn 0; Pointers to Already Existing Values Existing values are in stack frame, so when our pointers point to something already existed, they point to something in the stack frame, but remember variables in stack frame can disappear when out of scope. 12345678int main()&#123; int *iPtr; if (true) &#123; int p = 5; iPtr = &amp;p; &#125; cout &lt;&lt; “*iPtr is “ &lt;&lt; *iPtr &lt;&lt; endl;&#125; So the danger is you will have to know how long this stack frame will live, or you will lose track of what you are pointing to and end up pointing something totally irrelevant. Common Confusion with * int *p - declaring a pointer: The star is part of the type name, and says that we want a pointer to some other type (in our example, int * is the type of p). r = *p - dereferencing a pointer (RHS): The star is the dereference operator. This assignment gives the variable ra new value, namely the value inside the box that the pointer p points to. *p = r - dereferencing a pointer (LHS): The star is the dereference operator. This assignment changes the value inside the box that p points to be a new value, namely the value of the variable r. Pointer Chaos 12345678910111213141516int *a = 5, *b = 7;// dereference a, get the value stored in the memory a is pointing to,// and write a same value to the memory b is pointing to *b = *a; // let b point to the same address as a is pointing tob = a; // release the memory allocated to a, // also doing that for b since they are pointing at the same thingdelete a; // throw &quot;pointer being freed is not allocated&quot; error// since we already deleted it when we did that for adelete b; Pointers to User-Defined Types When we want to use member member (functions/ variables), we can use one of the following: 123Course *aCourse = new Course;(*aCourse).setStudentCount(45);aCourse-&gt;setStudentCount(45); Passing Pointers as Arguments 1234567int *a = new int;int x = 5;// store 0 in the memory location pointed at by intPtrvoid setToZero(int *intPtr) &#123; *intPtr = 0; &#125; setToZero(a); // pass to it a pointer whose value is some addresssetToZero(&amp;x); // pass the address of some variable to it Const with Pointers Principle of Least Privilege: Any operation you do should only be given the opptunity to happen if it absolutely needs to. Following this principle, we don’t want to give writing privilege to functions doing reading. There are four possibilities between constant/non-constant pointers pointing to constant/non-constant data: 12345678910111213141516171819202122232425262728293031// Non-Constant Pointer, Non-Constant Data// Free for the pointer to point to something else,// Free for the data it is pointing to be written as something elseint *intPtr = new int;// Non-Constnat Pointer, Constant Data// We can’t modify the data pointed at by coursePtr// We CAN set coursePtr to a different valuevoid printAllCourseData(const Course *coursePtr, const int size)&#123; // For this function, maybe we will direct pointer to some other course // once one course&#x27;s info has been printed, // while we don&#x27;t want to change that info // because this is just a reading function&#125;// Constant Pointer, Non-Consant Data// Pointer can only point to a specific memory// The data it is pointing to can be changedvoid setupCourse(Course *const coursePtr)&#123; // For this function, we only want to change information of this course passed in.&#125;// Constant Pointer, Constant Data// We can’t modify the data pointed at by coursePtr// We can’t set coursePtr to a different value eithervoid printCourseData(const Course *const coursePtr)&#123; // We only want to print out the info of this course passed in and do nothing else&#125; Lec10 Classic Arrays and Pointer Arithmetic Classic Array 12int *j[4]; == (int *) j[4]// array of 4 pointersint (*p)[4]; // a pointer to an array of 4 integers Arrays are somewhat pointers. For example, if we have int b[10], b always points to the first element in this array: b == &amp;b[0] Pointer Arithmetic For any array p[n] == *(p+n). In particular, *(p+n) gives the contents of we have after advancing n steps from p. In fact, we also have p[n] == n[p], because our a[m] is just a syntactic sugar for *(a+m) Dynamic Allocation of Arrays 123int a1[8] = new int; // WRONGint *a = new int[8]; // RIGHTdelete [] a; // Must use this, ”delete a” is undefined There are more scope issues when you use arrays as pointers. For example, the following code returns a pointer to something inside current call stack frame. It will disappear when out of the scope. Therefore, the returned pointer from function MakeArray() actually points to something undefined. 123int *MakeArray() &#123; int iArray[50]; return iArray; &#125; The following code behaves differently. Instead of returning a pointer to something in the call stack, it returns something in the heap, which will not disappear after the function finishes execution. 123int *MakeArray(int size) &#123; int *anArray = new int[size]; return anArray; &#125; Passing Arrays as Parameters Since arrays are pointers, you can only pass the real array to a function. There is no concept of passing a copy of that array. These are standard ways of declaring a function taking in arrays as its parameters. 12void swap(int *A, int j, int k);void swap(int A[], int j, int k); Memory Allocation with malloc and sizeof malloc is a function for dynamic memory allocation and it only takes in byte. sizeof(SomeDataType) returns the number of bytes this data type needs. Say we want to declare an array of 6 Courses in heap here. 12Course *courseArray = malloc(sizeof(Course) * 6); // Old C way to initialize array in heapCourse *courseArray = new Course[6]; // The C++ way to do it Lec11 Classes – A Deeper Look 1clang -std=c+11 -lstdc++ -c MyString.cpp Implicit Inline When you define a function right in the class definition, you make this function implicitly inline. Therefore, there’s no actual method/function created; the code of the method is substituted through the rest of the code wherever that method is called. Multiple Constructors You can use a delegate constructors to save yourself from writing duplicate code. It will just call that constructor, if the delegate constructors take in arguments, you can just pass in those arguments there. 123456789101112131415// older c++ styleMyString::MyString(string initValue) : MyString() &#123; if (growStorage(initValue.length())) &#123; strcpy(storagePtr, initValue.c_str()) stringLength = initValue.length(); &#125;&#125;// c++ 11 styleMyString::MyString(string initValue) : MyString&#123;&#125; &#123; ... &#125;&#125;// Another ExampleMenu::Menu(MenuItem* list[], int n, char prom, string title) : MenuItem(prom, title)&#123; for (int i = 0; i &lt; n; i++) items.push_back(list[i]);&#125;; Destructor The destructor is a special method (similar to constructor) that is called just before an object is destroyed. There is only one destructor per class (can’t overload). It takes no arguments. A destructor should be used to clean up any dynamically allocated resources (memory, OS objects). You call the destructor when using delete keyword. Passing and Returning Reference Passing Reference If you modified a parameter passed by reference in a function, the change would persist in the calling function. Note that the way we call this function has not changed. We still pass in two strings instead of pointers. You don’t have to do anything differently to specify that the string arguments are being passed “pass-by-reference” when I call the function; I only need to specify that I want to use pass-by-reference when I declare the getTimeAndTemp function. 12345678void getTimeAndTemp(string &amp;time,string &amp;temp)&#123; time = getTheREALTime(); temp = getTheREALTemp();&#125;int main() &#123; string theTime,theTemp; getTimeAndTemp(theTime,theTemp); // theTime and theTemp will be changed.&#125; Returning Reference When we add a &amp; before the function name, the function still returns whatever type it returns, but now the function call can appear on left side of assignment operator and we can write a new value to the memory address the returned value is stored in. For the following example, charAt still returns a char type. The only difference is that we can now directly change the returned value stored in the object by using the assignment operator. 123456789101112char &amp;MyString::charAt(int index) &#123; // boundary checking is omitted for clarity return storagePtr[index];&#125;int main() &#123; MyString str(“Hello World!”); char c =str.charAt(11); cout &lt;&lt; c; // &#x27;!&#x27; str.charAt(11) = ‘?’; // legal because we are returning reference cout &lt;&lt; str.charAt(11); // &#x27;?&#x27; cout &lt;&lt; “str is now: “ &lt;&lt; str.MakeString() &lt;&lt; endl; // Hello World?&#125; const in class As a qualifier to a member variable. It means that the member variable cannot be changed As a qualifier to a member function. It means that the member function cannot change anything in the class: 1string getName() const &#123; return mName; &#125; static in class There is ever only one copy of that variable that is shared among all the instances of the class. The storage for this variable must be declared in the global scope using the fully qualified name of the variable (classname::static_variable_name) The shared copy of the variable can be accessed either as a field of any instance or using the fully qualified name of the variable 123456789101112// &quot;Person.h&quot;class Person &#123; static int number_of_persons;&#125;// &quot;Person.cpp&quot;int Person::number_of_person = 0; // &quot;main.cpp&quot;cout &lt;&lt; Person::number_of_person; // 0Person p(&quot;Harmony&quot;); // increment number_of_person by 1 in the constructorcout &lt;&lt; p.number_of_person; // 1 this in class Its “type” is pointer to class type. So, if we have a Person class, Person has an implicitly defined member variable named this that is of type Person * Any of the member variable and functions in the class can be referenced from this Lec12 Operator Overloads Unary Operator Overloads we just have to use the operator keyword. 1234567891011// &quot;MyString.h&quot;int operator~();std::string operator+();// &quot;MyString.cpp&quot;int MyString::operator~()&#123; return stringLength;&#125;string MyString::operator+()&#123; return MakeString(); // returns a std::string from our MyString instance&#125; Binary Operator Overloads We define most binary operator overloads globally when it doesn’t make “sense” which of the two instances of the operands should “host” the overload. (Expressions on both ends are to some extent equal to the other) We use inline to allow us to place this in the header file without causing multiple definition errors, so we are never really “defining” it, but just replace the code whenever it is called. 12345678inline MyString operator+(const MyString &amp;str1, const MyString &amp;str2) &#123; // use the overloaded unary + sign to return a std::string // then use the std::string overloaded binary + sign to concatenate two strings MyString temp( (+str1) + (+str2) ); return temp;&#125; Here we have an instance of binary overload not done globally. It is a “binary operator” but only takes one argument. 123T &amp;operator[](int i) &#123; return *(mStoragePtr + i); // equivalent to return mStoragePtr[i];&#125; Copy Constructors Whenever we use the assignment operator to initialize a variable when it is declared, the compiler actually looks for a constructor that takes in a single argument that matches the type of the value you are assigning to the newly declared instance. If we have MyString str2 = 1;, the compiler would look for a constructor for MyString that took a single integer: MyString::MyString(int arg). If you copy constructors take in some object, it must be pass-by-reference! 123456789Point::Point(Point &amp;anotherPoint) &#123; // ...&#125;int main()&#123; Point p1(4,5); // will use our custom constructor Point p2(p1); // will use the copy constructor (just as a constructor function) Point p3 = p1; // will use the copy constructor&#125; Overloading Assignment = Rather than initialize some variable, we want now to assign a new value to an existing variable. Rather than a global function, we will define it as a member function in our class. 1234567MyString &amp;MyString::operator=(const MyString &amp;sourceStr)&#123; // convert sourceStr to a std::String with our predefined unary + // setValue takes a C++ string // return the address of this object setValue(+sourceStr); return *this;&#125; Overloading Stream Direction &lt;&lt; and &gt;&gt; 123456789101112131415inline ostream&amp; operator&lt;&lt;(ostream &amp;os, MyString &amp;str) &#123; os &lt;&lt; +str; // we must always return the stream that was passed in. That allows &quot;chaining&quot;(cout&lt;&lt;a&lt;&lt;&quot;good&quot;&lt;&lt;endl;) to work return os;&#125;inline istream&amp; operator&gt;&gt;(istream &amp;is, MyString &amp;str) &#123; int allocatedSpace = str.getAllocatedSpace(); char *tempBuf = new char[allocatedSpace]; // allocate temp is.get(tempBuf,allocatedSpace-1); // read from instream into location of tempBuf [tempBuf] up to [tempBuf + allocatedSpace - 1] string tempStr = tempBuf; // convert tempBuf to a std::string str.setValue(tempStr); // set str of MyString class to be tempStr delete [] tempBuf; // delete temp memory, realease space return is; // return stream&#125; Lec13 Inheritance Basic Syntax 12345678910class DerivedClass : public BaseClass&#123; &lt;member variables unique to Derived Class&gt; ...&#125;;class Student : public Person&#123; int studentID;&#125;; Override We can override a function by just reimplementing it in our derived class. To access the original implementation from the base class, we use its fully qualified name in the derived class. 123456789101112void Person::printInfo()&#123; cout &lt;&lt; “Name: “ &lt;&lt; name &lt;&lt; endl; cout &lt;&lt; “Addr: “ &lt;&lt; address &lt;&lt; endl; cout &lt;&lt; “Phone: ” &lt;&lt; phone &lt;&lt; endl; &#125;;void Student::printInfo()&#123; Person::printInfo(); cout &lt;&lt; “Student ID: “ &lt;&lt; studentID &lt;&lt; endl;&#125;; Virtual Functions Say we overwrite the printInfo function in Person and define a global function that takes in a Person class and call the printInfo function on that class. When we pass a Student instance to it, it will actually use the printInfo function of Person instead of Student. That’s because the compiler thinks the function just takes in a Person. 1234567void printPersonInfo(Person &amp;aPerson)&#123; aPerson.printInfo();&#125;;Student s;printPersonInfo(s); // prints out Name, Addr, Phone If you want to use the overridden version of the function, you will have to declare the function in base class as a virtual function. By defining a virtual function, we tell the compiler to call the overridden version no matter what type that instance may be cast to. However, to achieve this effect, we should also pass in an reference or pointer of instance of our derived class. Only in this way can the compiler knows what type our object was declared as. If we just pass by value (a copy of that instance), it will create a copy of our instance with whatever type specified in the function. More specifically, it calls the copy constructor of the specified class. It has no knowledge of what the original type of the argument was. 123456789class Person&#123; virtual void printInfo();&#125;void printPersonInfo(Person &amp;aPerson) // use overriden versionvoid printPersonInfo(Person *aPerson) // use overriden versionvoid printPersonInfo(Person aPerson) // use function in Person; //in fact in the last function, aPerson only has the &quot;Person&quot; part and doesn&#x27;t contain any information specific to the derived class Lec14 Polymorphism We can dynamically allocate an instance of the derived class and store it in a base class pointer variable. Since Instructor is derived from Person, this is legal. 12Person *aPerson = new Student(); // a pointer of base class(Person) pointing to its derived class(Student)aPerson-&gt;printInfo(); // calls the overridden method in derived class Abstract Class We can make a function to be pure virtual (abstract) by adding a = 0 after its declaration. Any new class derived from this class must implement pure virtual methods if the class is going to work. A class with pure virtual functions is an abstract class. Virtual Destructors If you have an abstract class, you would need to have an abstract/virtual destructor. That is because when a derived class’s destructor is called, it will (implicitly) call destructors in all base classes it inherits from as well. 12345// Person.hvirtual ~Person() &#123;cout&lt;&lt;&quot;base class destructor called&quot;&lt;&lt;endl;&#125;// Student.h~Students() &#123;cout&lt;&lt;&quot;derived class Studenet destructor called&quot;&lt;&lt;endl;&#125; Lec15 Stream Simple Stream I/O put/get: For any stream, the simplest I/O routines let you input or output one character at a time. End of File eof: a special character (usually has value -1) that signals you’ve reached an end of file state. When we reach eof, we cannot read any further from the file. (Ctrl+Z on Windows, Ctrl+D on other OS) getline: pass in a whole line of characters ( read in until encountering with a \\n) When you type in “This” while running the following code without hitting Enter, it will not print anything, because all characters you typed in have not been sent into the buffer yet. After you hit Enter, “This” will be echoed back. So everything got sent into the buffer, we get one out of it each time, and put it to the outstream, repeat the process until we encounter an eof (Ctrl+Z). 1234while (!cin.eof()) &#123; char c = cin.get(); cout.put(c);&#125; Error Handling Once an cin attempt failed, an error flag is set and future attempts to get input will fail. Failure happens when type entered doesn’t match the type of the variable you are assigning value to. cin.fail(): returns true if the last cin assignment failed. cin.clear(): repairs the stream by clearing the error flag in cin. cin.ignore(n, c): ignores the following n characters or one c character. Therefore, cin.ignore(100,'\\n') ignore all input until you’ve already ignored 100 of them or ignore 1 ‘’ character. 123456cin &gt;&gt; id;while (cin.fail() || id&lt;0 || id&gt;99) &#123; cin.clear(); cin.ignore(99, &#x27;\\n&#x27;); cout &lt;&lt; &quot;invalid number, try again &gt; &quot;; cin &gt;&gt; id;&#125; Int Stream Manipulator dec: decimal, base 10 oct: octal, base 8 hex: hexadecimal, base 16 setbase(n): set to n base These stream manipulators are “sticky”. They will remain the format of your output (even though you start another sentence of cout), until you set another stream manipulator. 123cout &lt;&lt; oct &lt;&lt; 8; //10cout &lt;&lt; 9; // 11cout &lt;&lt; setbase(10) &lt;&lt; 16; // 16 Float Stream Manipulator fixed: print out float number in decimal/fixed point notation scientific: print out float number in scientific notation setprecision(n): always print out 3 digits after the decimal point They are all “sticky”. You’ll have to manually set it back to previous state. 123int curPrecision = cout.precision(); // current settingcout &lt;&lt; setprecision(2) &lt;&lt; 3.12545 &lt;&lt; endl; // 3.13cout.precision(curPrecision); // Restore original setting Fixed Width left: align to left, the output is padded to the field width appending fill characters at the end right: align to right, the output is padded to the field width by inserting fill characters at the beginning These two stream manipulators are sticky. We also use setw(n) to make sure at least n characters are printed. If the string to print has fewer than n characters, fill with space. If it has more than n characters, print everything. setw(n) is not sticky. That’s because most output methods automatically calls setw(0) each time you call them. setw(n) is in the library #include &lt;iomanip&gt;. 123456789#include &lt;iomanip&gt;int n = -77, m = 13579;cout &lt;&lt; setw(6) &lt;&lt; left &lt;&lt; n &lt;&lt; endl;cout &lt;&lt; setw(6) &lt;&lt; right &lt;&lt; n &lt;&lt; endl;cout &lt;&lt; setw(2) &lt;&lt; m &lt;&lt; endl;//-77 // -77//13579 Custom Manipulator Manipulators are just globally defined functions that take an ostream reference and return an ostream reference. Following are some examples: 12345678ostream&amp; beep(ostream &amp;output)&#123; return output &lt;&lt; “\\a”;&#125; // displaying \\a causes beepostream &amp;aReallyLongTokenForNewline(ostream &amp;output)&#123; return output &lt;&lt; “\\n”;&#125;cout &lt;&lt; “This will cause a beep: “ &lt;&lt; aReallyLongTokenForNewLine;cout &lt;&lt; beep; Lec16 Functional Programming auto keyword The auto keyword is used to declare a variable whose type is determined by the value it is initialized to. It must be initialized at the moment it is declared (or it will cause a static time compiler error). 12auto f = 3.14 // f is made a doubleauto k; // NO INITIALIZER – This would be a compiler error Function Pointers When we define a function pointer, we need to define its return type and what type of arguments it takes in: return_type (* function_name) (argument_type1, argument_type2, ...). Note: All these parameters are required. We can declare a function pointer with no allocation. We can assign it to any function that matches the argument type and return type as we do to most pointers. We can also use the C++11 style function in STL functional: std::function&lt; return_type (argument_type1, argument_type2, ...) &gt; function_name, but this is much heavier. 123456789101112131415int SimpleAdd(int arg1,int arg2)&#123; return arg1 + arg2; &#125;int main(int argc, char *argv[])&#123; int (*f)(int start,int stop); // define a function pointer that takes in two ints and returns an int f = SimpleAdd; // f now points at the function “SimpleAdd” int x = (*f)(3,4); // dereference f, get the function it points to and applies it to 3,4 int y = f(3,4); // A syntactic sugar provided. Compiler will do the dereference cout &lt;&lt; &quot; x is: &quot; &lt;&lt; x &lt;&lt; &quot;, y is: &lt;&lt; y &lt;&lt; endl; function&lt;int(int,int)&gt; g; g = simpleAdd; cout &lt;&lt; g(3,4) &lt;&lt; endl; // also gives 7 // *g(3,4) doesn&#x27;t work because g here is an std::function, not a pointer to a C-style function&#125; Function as Parameter When we want to pass a function as a parameter of another function, we can pass it as a C-style pointer or C++11 std::funciton. We can also use a template and let the compiler to figure it out. 12345678910void OldCallMe(int (*f)(int), int x) &#123;...&#125;void NewCallMe(std::function&lt;int(int)&gt; f, int x) &#123;...&#125;template&lt;typename T&gt;void CallMe(T fn, int x)&#123; // a syntax error will result if the fn passed in of type T doesn&#x27;t support the following line int newValue = fn(x); cout &lt;&lt; &quot;CallMe-newValue is: &quot; &lt;&lt; newValue &lt;&lt; endl;&#125; Lambda Expressions A lambda expression evaluates to a function pointer. It takes the following format: [vars](args) -&gt; returntype &#123; // body of function &#125;;, where return type and arrow can be omitted. 12345678910111213// Declare a lambda with the auto keyword (we don&#x27;t know what type of a function that is)// func is a function that takes no variables or arguments and simply prints out Hello Worldauto func = []() &#123; cout &lt;&lt; &quot;Hello world!&quot; &lt;&lt; endl; &#125;;// Declare a lambda with a function pointer:// func2 is a pointer to a function that takes in a string as parameter// More specifically, that function takes in a string and prints it outvoid (*func2)(string) = [](string s) &#123; cout &lt;&lt; “Hello “ &lt;&lt; s &lt;&lt; endl; &#125;;// Use function template (C++11) to store lambdastd::function&lt;void(string)&gt; func3 = [](string s) &#123; cout &lt;&lt; “Hello “ &lt;&lt; s &lt;&lt; endl; &#125;; Capture Local Variables We can use lambda expressions to capture local variables. This will be an important way to still be able to use variables in a function that no longer exists when the lambda finally gets executed. If you want to capture local variables, always use C++11 std::function when defining either the lambda expression or the function you want to take this lambda expression. 12345678void CallMe(std::function&lt;int()&gt; fn) &#123;...&#125;;// template also works because it will automatically identify fn as an std::functiontemplate&lt;typename T&gt;void CallMe(T fn) &#123;...&#125;;int myX = 300;CallMe([myX]()-&gt;int&#123; return myX*2; &#125;); You also have the option of capturing local variables by reference. That means if the lambda expression modifies them, the modifications persist back into the “hosting” function where these variables were defined. (Just like any pass by reference function call). Pass by reference or pass a pointer will do. 12345int myX = 300; int *myY = new int; *myY = 3;CallMe([&amp;myX]()&#123; myX *= 2; &#125;);CallMe([myY]()&#123; *myY *= 2; &#125;);cout &lt;&lt; &quot;myX is &quot; &lt;&lt; myX &lt;&lt; endl &lt;&lt; &quot;myY is &quot; &lt;&lt; *myY &lt;&lt; endl; // 600 Lec17 Files I/O ofstream We use ofstream to write to file. A constructor of ofstream takes in two arguments name of the file to open specifies which mode to use: ios::out open file for writing, overwrite existing file ios::app open file for writing, append to existing file We don’t have to close the stream after writing, ofstream has a destructor that is automatically called at the end of the program. That being said, we can still call out.close() manually. We use out.is_open() to make sure the file is indeed successfully opened and ready to be written in. Directly evaluating the stream variable out itself as a boolean also does the check. 123456#include&lt;fstream&gt;ofstream out(“myFile”,ios::out); // create an ofstream, pass in name of the file and ios::out to indicate you want to use it for outputif (out.is_open()) // make sure we successfully opend the file out &lt;&lt; “Hello world!” &lt;&lt; endl;out.close(); ifstream We use ifstream to read from a file. 1234ifstream in(“myFile”,ios::in);string str = &quot;Hello&quot;;if (in.is_open()) in &gt;&gt; str; Sequential Files Suppose we have a csv file that uses comma as the delimiter and space as a record separator. If we want to change only a specific record, how are we supposed to move around in that file? tellg(): returns the offset from the beginning of the file where the next read operation will get data from. tellp(): returns the offset from the beginning of the file where the next write operation will put data to. seekg(n): sets the “get” offset to the nth character in the file seekp(n): sets the “put” offset to the nth character in the file Reading and Writing at the Same Time When declare an fstream variable, we can specify using multiple “modes” at the same time by putting the or operator | between different modes. We can then use whatever function those modes give 123fstream file(“ages.dat”, ios::in | ios::out) //reads and write to &quot;ages.dat&quot; at the same timestring str; file &gt;&gt; str; // read worksfile &lt;&lt; &quot;That&#x27;s good&quot;; // write also works Lec18 Standard Template Library Iterator begin() points to the first element in the object. end() points to one after the last element. Common operators like + &lt; &gt; are all overloaded for iterators. 123for (vector&lt;string&gt;::iterator p = stringVector.begin(); p &lt; stringVector.end(); ++p) cout &lt;&lt; “Next Vector Element is: “ &lt;&lt; *p &lt;&lt; endl; Vector 123vector&lt;string&gt;::iterator q = stringVector.begin();stringVector.erase(q+5); // erase the 6th elementstringVector.erase(q,q+5); // erase [q, q+5), so erase 1st to the 5th element Map Map is based on valuetype, which has type &lt;key, value&gt;. All operations come from this pair. We can use typedef to name some very complicated data type that is frequently used. 12345678910111213141516typedef map&lt;int,string&gt;::value_type IDRecord; // IDRecord is in fact of &quot;pair&lt;int,string&gt;&quot; typetypedef map&lt;int,string&gt;::iterator IDRecordIterator;int main()&#123; map&lt;int,string&gt; ids; IDRecord rec1(12345,&quot;Ron DiNapoli&quot;); IDRecord rec2(34564,&quot;Darpan Kaplan&quot;); ids.insert(rec1); // alway insert a key-value pair ids.insert(rec2); cout &lt;&lt; &quot;ID 34564 belongs to: &quot; &lt;&lt; ids[34564] &lt;&lt; endl; // use array-like way to access map IDRecordIterator p = ids.find(12345); // find returns the address of that entry with a matched key , returns map::end() if key doesn&#x27;t exist IDRecordIterator q = ++p; cout &lt;&lt; &quot;Next entry of ID 12345 is: &quot; &lt;&lt; (*q).second &lt;&lt; endl;&#125; Lec19 Exceptions Basic Syntax Exceptions can be of any type. We can do throw 3.14, throw \"Unexpected\", or throw some object. 1234567891011enum MathErr &#123; noErr, divByZero, genericOverflow &#125;;throw divByZero;try &#123; ...&#125; catch(MathErr e) &#123; ...&#125;// Orcatch(...) &#123;&#125; // catches all kinds of Exceptions Exception Object and Inheritance As said in previous section, we can throw an object. 1234567891011121314151617181920212223242526272829class MyIndexError &#123; MyIndexError(int i,char *msg):badIndex(i),theMsg(msg)&#123;&#125; int getBadIndex() &#123; return badIndex; &#125; string getMessage() &#123; return theMsg; &#125;private: int badIndex; string theMsg;&#125;;char &amp;MyString::operator[](int index)&#123; if ((index &lt; 0) || (index &gt;= stringLength)) throw MyIndexError(index,”Index out of bounds”); return storagePtr[index];&#125;class BaseException&#123;public: BaseException(string msg,int err=0):message(msg), errorCode(err)&#123;&#125; virtual string getMessage() &#123; return “BASE EXCEPTION: “ + message; &#125; int getErrorCode() &#123; return errorCode; &#125;protected: string message; int errorCode;&#125;; Lec20 Custom Templates Basic Syntax 1234567891011template &lt;class placeholder&gt; // declare placeholdersclass SimpleClass // regular class definition&#123;public:…&#125;;// define a function outside of template classvoid SimpleClass&lt;placeholder&gt;::FunctionName() &#123;...&#125;// define constructor/destructor outside of template classvoid SimpleClass&lt;placeholder&gt;::SimpleClass() “Definition” template class should also be in the same .h file. Because the compiler needs to generate a separate set of member functions for each type used to create an instance of this class at compile time. That means that these definitions are needed at compile time and not at link time, so .cpp won’t enable us to actually call those functions. Non-Type Parameters We specified a data type calls placeholder in the template class. We can also specify a constant expression when we declare a template class. This will have the same effect as setting a const value specific for that instance, except previously we couldn’t assign values to const variable. 12template &lt;class storageType,int size&gt; class MyArray &#123;...&#125;template &lt;class storageType=int,int size=5&gt; class MyArray &#123;...&#125; // give a default value Lec22 STL Algorithms #include&lt;algorithm&gt; for all functions below. fill(iterator begin, iterator end, T value): take two iterators/pointers and one value. Fill every position in between with that value.: 12char *ptr = new char[10] // An array of 10 charsfill(ptr,ptr+9,’A’); generate(iterator begin, iterator end, function g): assigns every position in between the two iterators/pointers according to the generating function g. 1234567int nextVal() &#123; static int number = 0; return number++;&#125;int main(int argc,char *argv[]) &#123; std::vector&lt;int&gt; intVector(10); // A vector of integers std::generate(intVector.begin(),intVector.end(),nextVal);&#125; fill_n(begin,count,value): fill from begin to begin+count with specified value generate_n(begin,count,function): fill from begin to begin+count with generated value remove(begin,end,value): remove all elements == value in range from begin to end replace(begin,end,value,replaceWith): replace all elements == value in range from begin to end WITH replaceWith Lec23 Smart Pointers Shared Pointer You can declare multiple pointers pointing to the same thing using shared pointer and they will all be released when you release one of them, so it’s safer than the classic pointer, where the pointer will hang over there. You can call the use_count() method to get how many shared pointers are out there pointing to this same thing. 123456int main(int argc,char *argv[]) &#123; shared_ptr&lt;Point&gt; pointPtr(new Point(1,2)); shared_ptr&lt;Point&gt; pointPtr2(pointPtr); cout &lt;&lt; “x coordinate is: “ &lt;&lt; (*pointPtr).x &lt;&lt; endl; cout &lt;&lt; “reference count is: “ &lt;&lt; pointPtr.use_count();&#125; Unique Pointer There is only this one pointer pointing to that thing. No other shared pointers can be created pointing to the same thing. For the same reason, use_count() is not available either. Lec24 Namespaces and C/C++ Differences Namespace Declaring Namespace We define a namespace by putting it inside a namespace declaration and its corresponding scope, just like what we do to a class. What’s different is that a single namespace may span multiple files. Therefore, we can declare/define a single namespace in multiple files. 123456namespace CornellCS2024 &#123; // Defines a namespace named CornellCS2024 class MyString &#123; public: ...&#125;; class AnotherClass &#123;...&#125;&#125; Using Namespace We can use anything declared in the namespace by quoting the fully qualified name 1CornellCS2024::MyString aString; We can designate a specific class to use in the rest of the file. 12using CornellCS2024::MyString;MyString aString; We can simply state that we want to use everything declared in this namespace. That’s what we usually do to std in small file. 12using namespace CornellCS2024;MyString aString; C/C++ Difference only supports /* block comments */ variable declarations had to appear the beginning of a scope before any other statements were encountered only has struct, no class no overloads, Namespaces, Declaring a counter variable in a loop, String type, Exceptions, Templates does not use new/delete for dynamic memory allocation/deallocation. Instead, C uses malloc() allocates memory. It needs to be given the exact number of bytes you want to dynamically allocate calloc() is the same as malloc() but initializes all allocated memory to 0 realloc() “grow” a dynamic allocation: basically allocates new space and copies all original memory to new space. free() releases allocated memory Lec99 From Assignments new keyword returns a pointer to an object. You don’t have to use new when creating a new object. Reference 123test t = test(&quot;rrr&quot;, 8);test t(&quot;rrr&quot;, 8);test *t = new test(&quot;rrr&quot;, 8);","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Windows下配置PostgreSQL","slug":"2020-09-05-Windows下配置PostgreSQL","date":"2020-09-05T04:00:00.000Z","updated":"2022-06-08T19:34:22.000Z","comments":true,"path":"2020-09-05-Windows下配置PostgreSQL/","permalink":"https://yao-lirong.github.io/blog/2020-09-05-Windows%E4%B8%8B%E9%85%8D%E7%BD%AEPostgreSQL/","excerpt":"如果你在纠结要不要装，别装了吧，对自己好一点","text":"如果你在纠结要不要装，别装了吧，对自己好一点 Download and Install PostgreSQL. During the process, PostgreSQL will ask you to create a username and password, the default username is “postgres” and password is up to you. Run the pg_env.bat under installation folder or the following env.vbs file if that bat doesn’t work 1234567891011on error resume nextset sysenv=CreateObject(&quot;WScript.Shell&quot;).Environment(&quot;system&quot;) &#x27;系统环境变量的数组对象Path = CreateObject(&quot;Scripting.FileSystemObject&quot;).GetFolder(&quot;.&quot;).Path&#x27;添加变量sysenv(&quot;PGHOME&quot;)=&quot;C:\\Hacking\\PostgreSQL&quot; &#x27;!!!change to your own directory!!!&#x27;sysenv(&quot;PGHOST&quot;)=&quot;localhost&quot;sysenv(&quot;Path&quot;)=sysenv(&quot;PGHOME&quot;)+&quot;\\bin;&quot;+sysenv(&quot;Path&quot;)sysenv(&quot;PGLIB&quot;)=sysenv(&quot;PGHOME&quot;)+&quot;\\lib&quot;sysenv(&quot;PGDATA&quot;)=sysenv(&quot;PGHOME&quot;)+&quot;\\data&quot; wscript.echo &quot;PostgreSQL Environment Variable Successfully set&quot; Initialize database by running initdb -D C:\\Hacking\\PostgreSQL\\data Start Server by running pg_ctl -D C:\\Hacking\\PostgreSQL\\data start Register a server service by running pg_ctl register -N \"PostgreSQL\" -D C:\\Hacking\\PostgreSQL\\data (If not working, run terminal as administrator) You should then be able to see a service called “PostgreSQL” in Windows Services (Win+R services.msc) Create Database by running createdb -U postgres &lt;DatabaseName&gt; and entering the password you entered in step 1. Note it will give a “Password Authentication Failure” if you try createdb &lt;DatabaseName&gt; and enter the password that way. That is because PostgreSQL uses your computer username as default and no such use is registered in PostgreSQL in the first place. You have to use -U option to specify the user you want to log in as. Allow operations on database without password by changing authentication method in PostgreSQL\\data\\pg_hba.conf all from md5 to trust: (might take effect after a reboot) 12345678910# TYPE DATABASE USER ADDRESS METHOD# IPv4 local connections:host all all 127.0.0.1/32 trust (was md5)# IPv6 local connections:host all all ::1/128 trust# Allow replication connections from localhost, by a user with the# replication privilege.host replication all 127.0.0.1/32 trusthost replication all ::1/128 trust Now when we run command createdb db, we may get a message Fatel Error: User \"harmo\" does not exist. This is a similar problem as in step 6. We want to create a user called “harmo” by running createuser -s -r -U postgres harmo. Note simply creatuser -s -r harmo will not work because the default user to use to create such a user harmo is also harmo, which doesn’t exist in the first place. What a stupid logic PostgreSQL applies. Now PostgreSQL is ready to use. 过了一天服务启动不起来了，查了一堆没有结果，我还用你妈了个嗨，小爷一个周末就整你这傻逼玩意，草了真的是 好像是启动 pgAdmin 之后，又可以用了。刚刚启动的服务本身就是为了让 local server 运作的，而 pgAdmin 可以帮我们完成这个任务。于是我们登录进 pgAdmin 就相当于让那个服务运作起来了，也就可以正常在终端中使用命令了 Reference 主要：Windows下在命令行安装postgresql，并注册成window服务 Windows上PostgreSQL安装配置教程 Windows下Postgresql下载与配置方法 postgresql 口令： psql: 致命错误: 用户 认证失败 本机psql设置需要/不需要密码 Postgres psql: 致命错误: 角色 “postgres” 不存在","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Kinect as Web Cam","slug":"2020-06-24-Kinekt-as-Web-Cam","date":"2020-06-24T04:00:00.000Z","updated":"2022-06-08T19:34:22.000Z","comments":true,"path":"2020-06-24-Kinekt-as-Web-Cam/","permalink":"https://yao-lirong.github.io/blog/2020-06-24-Kinekt-as-Web-Cam/","excerpt":"高二升高三暑假参加夏令营让我白嫖的 Kinect2，大材小用当做网络摄像头来用","text":"高二升高三暑假参加夏令营让我白嫖的 Kinect2，大材小用当做网络摄像头来用 下载 Kinect For Windows SDK 2.0，下载别人开发的 FullFastKinectCamV2 ver. 2.2，用就完事了 Reference KinectCamV2 for Kinect V2","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Look Back on Cornell 20SP","slug":"2020-05-27-Cornell-20SP-总结","date":"2020-05-27T04:00:00.000Z","updated":"2022-06-08T19:56:48.000Z","comments":true,"path":"2020-05-27-Cornell-20SP-总结/","permalink":"https://yao-lirong.github.io/blog/2020-05-27-Cornell-20SP-%E6%80%BB%E7%BB%93/","excerpt":"CS3110 Data Struct &amp; Functional Programming Ranting When you find yourself saying, “I don’t know,” be sure to follow it up with ” - but I’ll find out.” It’s a great way to admit what you don’t know, but then take responsibility like a pro. – The Pragmatic Programmer, Ch2. The Cat Ate My Source Code","text":"CS3110 Data Struct &amp; Functional Programming Ranting When you find yourself saying, “I don’t know,” be sure to follow it up with ” - but I’ll find out.” It’s a great way to admit what you don’t know, but then take responsibility like a pro. – The Pragmatic Programmer, Ch2. The Cat Ate My Source Code ​ Nate Foster 完美诠释了如何做到这一点，当你问他 XXX 什么时候搞完， XXX 为什么还没上传，XXX 是不是出错了的时候，他总能告诉你 “I’ll look into it”, “I don’t know but I’ll get it done by today”, “I have another meeting coming up, but you can come to my office hour to talk about that” (当你真去了的话他会和你说他接下来又有另一个事要处理并非常礼貌地请你滚)。网上的所有评价都是 Both 3110 professors are amazing，并且 reddit 上经常有人感谢 Foster，就差赞美 Foster 胜过他亲爷爷了。不知道是 Foster 给了这些人好处，还是 Foster 自导自演，我和 19FA 咨询的一名 TA 意见是相同的：“Clarkson is more organized than Foster”. 我虽然没有上过 Clarkson 的3110，但是我觉得再差也不会差到哪里去了。要不是 SP 的 3410 讲师天气勺评价更差，我绝不选 SP 上 3110。 ​ 不知道是不是计算机院都有的问题：讲义写的实在太好了以至于上课不如自己看讲义。但是 Myers 讲的环环相扣，Foster 则是引入了 iclicker 以强制你在一个可以容纳 300 个人的教室里上计算机课时禁止使用计算机，而是看着他现场 debug 自己本应在课前 debug 好的 demo 代码。Foster 这学期唯一的用处大概就是解答了我关于红黑树的问题 (当然了严格来说不是他解答的，这个我口中的最大用处其实就是我问 “为什么 OCaml 可以几十行实现其他语言几百行的代码，是不是因为 OCaml 的 pattern matching 减少了很多指针操作”，他回答了个 “Yes, you can say that” 而已) Assignment A0: 某一个 Consultant 说我把最后一个函数 tail-recursive Fibonacci 的两个变量搞倒巴了，于是我把它正了过来，这题0分。一个median为满分的小热身，我得了几乎两个 deviation 低于 median。appeal的结果也是不给我分，原因是我的 spec 里面两个变量的顺序是正确的。你说这TM不是废话吗，因为当时改的时候忘改spec了呗。就因为这个作业搞了我的心态所以我后面对 Foster 和这门课一直印象不好。 A3: 写了Maplewood… A5: sxy又救了我一命，说实话我一直到写完都不知道自己在写什么。这个作业完全是被sxy带的 Project 做了一个还蛮好玩的二分法解任意一元函数的可编程计算器。 CS2802 Discrete Structures (Honors) - So let’s think about some applications of graph in real life. Say who is the most influential person in this class. - You, professor! - … Yeah, of course. ​ Joe Halpern 是个非常好玩的老头，除了有的时候课上扯太多的例子以至于讲不完课以外，这是一堂要是有时间讲完图论就完美了的课。从一开始的时候必须每次都去 OH 写出来证明 (尤其是某一次的 inductive definition of transitive closure)，到最后某个作业自己独立完成了 “We know M accepts A, WTS A* is regular (accepted by some automata)” 然后这个题还拿了满分，切实的感觉到自己还是学了些有一般适用性的东西的。 MATH3110 Introduction To Analysis ​ 因为某人选的这课，临走前请教了刘晓东，他害怕这节课会难所以建议我只是旁听但不要上，吓得我这学期没再多选课，结果不能说完全不难，但是比 2802 这个 3 学分的课整体来说花的时间少。再也不信刘晓东了，第一学期临走前还跟我说别把多元微积分考掉，最后我还是考掉了( ​ 坚定下来选这门课还有一个原因是上学期的线性代数金牌讲师 Meyer 在讲，这学期跟她上了一个学期感觉虽然有的时候课上讲不太到点子上 (Meyer 下学期要去 Carleton 了，哭哭)。解释的不是很明白以外，整个课下来非常有组织性。另一名讲师法国佬 Saloff-Coste 虽然课讲得真不怎么样，但是 OH 特别有用，让人不禁疑惑 OH 的时候讲得这么好为什么课能讲得这么烂。 CS2043 UNIX Tools and Scripting ​ 讲的东西都很有用很有意思，但是这老师实属不行，就纯念 PPT WRIT1380 FWS: Elements of Acad Wtg ​ 为了成绩能高点又水了一年 FWS，比去年差远了，这老师没有Brad 能扯皮也没他有意思，作业还比 Brad 多。不过确实教了点有用的写作技巧","categories":[],"tags":[{"name":"Cornell","slug":"Cornell","permalink":"https://yao-lirong.github.io/blog/tags/Cornell/"},{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"}]},{"title":"Introduction to Vim","slug":"2020-03-15-Introduction-to-Vim","date":"2020-03-15T04:00:00.000Z","updated":"2020-04-02T22:54:56.000Z","comments":true,"path":"2020-03-15-Introduction-to-Vim/","permalink":"https://yao-lirong.github.io/blog/2020-03-15-Introduction-to-Vim/","excerpt":"","text":"First Steps in Vim Editing h j k l: huang he (W), Java (S), Kosovo (N), Los Angeles (E) i: begin insertion at current cursor a: begin insertion after current cursor o: creates a new line after this line and begin insertion O: opens a line above the cursor Deleting Characters x: delete one character forward dl X: deletes one character backward dh dd: delete a whole line J: delete the line break in this line. Undo and Redo u: undo your last edit ctrl-R: redo changes Moving Around Move by Words a string of English characters “thisIsAWord” or symbols “^&amp;%” is considered a word* in vim. A mix of both is not. w: move to the start of next word (forward) b: move to the start of last word (back) e: move to the next end of a word (end) ge: move to the previous end of a word These lowercases command consider the appearance of different kinds of characters as separation of word. There uppercases version W B E gE consider only whitespaces to be delimiter. Using Counts and Combination of Commands You can add numbers before commands to perform this command multiple times. e.g. 3w moves you to the start of the third next word. 5gE moves you to the end of the fifth previous word delimited by whitespaces. 4a!&lt;esc&gt; appends ! four times after the position of our cursor. dw: delete the word forward d$: delete from the cursor to the end of the line d2e: delete two words from the cursor to the end of the second end of word ahead Search Characters f&lt;character&gt;: move to the position of character’s first occurrence on right (forward) F&lt;character&gt;: move to the position of character’s first occurrence on left (backward) Move by Lines &lt;number&gt;G: go to specific line in file G: go to the end of the file gg: go to the top of the file &lt;number0~100&gt;%: go to some percentage of the file H: go to the top line of what’s visible (home) M: go to the middle part of what’s visible (middle) L: go to the last line of what’s visible (last) Scrolling ??? “``”: jumps back to the position you just came from (a jump is a move by lines) Searching Words /xxx search for xxx in our “file” searching has history, you can use arrow key up and down to go through histories *: use asterisk on a word to search this word in a file match whole words /the: matches “the”, “there”, “soothe” /\\&lt;the: matches “the”, “there” /the\\&gt; matches “the”, “soothe” /\\&lt;the\\&gt; matches only “the” You can also use regular expressions in searching Making Small Changes Changing Text c works just like d except it leaves you in the editing mode after deletion. c2wbe: deletes two words forward and then insert “be” cc: changes (deletes and leaves you in editing mode) a whole line (as dd does) c$: deletes to the end of the line and leaves user in editing mode r&lt;single character&gt; replaces the character under our cursor with the &lt;single character&gt; we typed in. e.g. rw replaces the character with “w”. Repeating a Change . repeats the last change we made (edit of the file) e.g. we want to delete all occurrences of “” 123456f&lt; find first &lt; df&gt; delete to &gt;f&lt; find next &lt;. repeat df&gt; f&lt; find next &lt; . repeat df&gt; e.g. We want to change all occurrences of “four” to “five” 123456789/four&lt;Enter&gt; find the first string &quot;four&quot;cwfive&lt;Esc&gt; change the word to &quot;five&quot;n find the next &quot;four&quot;. repeat the change to &quot;five&#x27;n find the next &quot;four&quot;. repeat the change etc. Visual Mode 其实就是一个可以选中文字的模式。 vllld selects three characters right to the cursor and deletes them. V selects whole line (commands like hl move the cursor but don’t change the selected area) Vjjd selects and deletes this line and two lines below it o: gets you to the other side of selected area in visual mode Moving Text When you delete something with the d, x, or another command, the text is saved. You can paste it back by using the p command. (The Vim name for this is put). p: puts the word deleted after the cursor P: puts the word deleted before the cursor xp: a combination of command to swap two characters if you typed them wrong (e.g. teh -&gt; the) Copying Text y: copy selected word (use p to paste it) yy: copy the whole line Y: also copy the whole line (not like D, which deletes until the end of the line) Clipboard use the \"* to use Vim clipboard \"*yy: copy a whole line to the clipboard \"*p: puts a line from the clipboard Text Objects daw: delete the word your cursor is at (aw: a word) das: deletes a sentence, including any trailing whitespace (as: a sentence) dis: deletes a sentence until the period, not including the trailing whitespaces (is: interior sentence) Set Your Settings Vimrc find .vimrc by :scriptnames and add the following commands at the end of the file :set incsearch: makes Vim display the match for the string while you are still typing it :set ruler: This will display the cursor position in the lower right corner of the Vim window :set autoindent: use the indent of previous line for a newly created line Mapping map a shorter combination of commands to a complicated function :map &lt;F5&gt; i&#123;&lt;Esc&gt;ea&#125;&lt;Esc&gt;: insert curly brackets around the current word by pressing function key F5 One key commonly used in mapping is the backslash \\ For example, :map \\p i(&lt;Esc&gt;ea)&lt;Esc&gt;: press \\p to insert parenthesis :map \\c i&#123;&lt;Esc&gt;ea&#125;&lt;Esc&gt;: press \\c to insert curly brackets Plugin Global Plugins You can just add global plugins under ~/.vim/plugin/ directory and Vim will load them automatically when it starts. Don’t forget you can organize these plugins by putting them into different folders: ~/.vim/plugin/perl/ or ~/.vim/plugin/cpp/ Filetype Plugins Start these plugins by :filetype plugin on Filetype plugins are stored under ~/.vim/ftplugin/. When name your plugin, you have to name them according to the file type (extension names). e.g. ~/.vim/ftplugin/stuff.vim works only for file of type “stuff”. When you have two plugins for stuff and want to distinguish these two, you should use underscore to separate filetypes and the names of plugin. e.g. stuff_too.vim is an extension for filetype stuff with name “too”. More Options :set nowrap: let the text continue right after the window :set &lt;option&gt;&amp;: set the value of this option back to default e.g. :set iskeyword&amp; :set list: display tab as ^I, end of line as $ :set listchars=tab:&gt;-,trail:-: set tabs to be displayed as &gt;---, trailing whitespaces as ----. :set iskeyword+=_: “_” now also becomes a part of keyword (e.g. stuff_abc is now a single word, w or b will not stop at the “a” when they move) :set cmdheight=3: set the command line height at the bottom to be 3 (more space for command line)","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"},{"name":"Vim","slug":"Vim","permalink":"https://yao-lirong.github.io/blog/tags/Vim/"}]},{"title":"CS2043 Unix Tools and Scripting","slug":"2020-01-24-CS2043-Unix-Tools-and-Scripting","date":"2020-01-24T05:00:00.000Z","updated":"2023-05-18T03:26:44.000Z","comments":true,"path":"2020-01-24-CS2043-Unix-Tools-and-Scripting/","permalink":"https://yao-lirong.github.io/blog/2020-01-24-CS2043-Unix-Tools-and-Scripting/","excerpt":"About The goal of CS2043 is to introduce you to the UNIX/Linux “command line” and its accompanying tools. When done with this class you should feel comfortable navigating any UNIX shell prompt, installing UNIX/Linux systems and understanding any shell script that you may encounter down the road. We’ll cover basic commands through script writing and visit some of the more common tools used today!","text":"About The goal of CS2043 is to introduce you to the UNIX/Linux “command line” and its accompanying tools. When done with this class you should feel comfortable navigating any UNIX shell prompt, installing UNIX/Linux systems and understanding any shell script that you may encounter down the road. We’ll cover basic commands through script writing and visit some of the more common tools used today! LEC02 File System (01/24) Root Directory Unlike Windows, UNIX has a single global “root” directory (instead of a root directory for each disk or volume). The root directory is just /. Absolute paths start with a /, and always refer to the root directory. cat: concatenate and print a file cat &gt;&gt; &lt;filename&gt;: concatenate your following input in shell to the file specified wc -l &lt;filename&gt;: count the number of lines in a file wc -w &lt;filename&gt;: count the number of words in a file touch: create a file if not existed mkdir -p test/a/b: make directory and all its parent directory if they do not exist cp –r &lt;src&gt; &lt;dest&gt;: To copy a complete directory cp –f &lt;src&gt; &lt;dest&gt;: To overwrite more aggressively LEC03 Permission (01/27) Reading permission: Linux Representation Permission by user type -rwx—— User permissions —-rwx— Group permissions ——-rwx Other permissions r- read, w- write, x - execute groups &lt;username&gt;: check which group this user is in and you can manage permission by groups chmod &lt;mode&gt; &lt;filename&gt;: change permissions: &lt;mode&gt;: +774: add user and group all rwx permissions, give others only r permission to the file -222: deprive user, group, and other’s permissions to write the file =111: change user, group, and other’s permissions to only execute the file. They will lose permissions to read or write if they previously had su: makes you the super user sudo: grants you the super power temporarily LEC04 More Commands (01/29) more | less: to view file man: \\something to search for “something”, n to go to its next occurrence find &lt;directory&gt; -&lt;criteria&gt; &lt;specification&gt;: Search is recursive (will search all subdirectories too), so sometimes you may need to limit the depth with -maxdepth &lt;int&gt; Modifiers for find are evaluated in conjunction (a.k.a AND). But you can condition your arguments with an OR using the –o flag find can also execute command on found files / directories by using the –exec modifier, and find will execute the command for you The variable name is {} You have to end the command with either a : Semicolon (;): execute command on each result as you find them- Plus (+): find all the results first, then execute command arguments for &lt;criteria&gt;: -name: the file’s name -amin n: file last access was n minutes ago -atime n: file last access was n days ago examples: find ./ -name *.sh: find under the current directory all files containing the extension name “.sh” find . –amin -10 –exec cat &#123;&#125; \\+: Display all the contents of files accessed in the last 10 minutes find . –type f –readable –executable: All files that are readable and executable find . –type f –readable –o –executable: All files that are readable or executable find . –amin +10: Find all files accessed at least 10 minutes ago find . –amin -10: Find all files accessed at most 10 minutes ago LEC05 Zipping (01/31) Zipping tar -c -v -f &lt;zipped_filename&gt; &lt;files_to_zip&gt;: tar files only create a bundle of file,s it doesn’t compress Remember to put -f as the last one, or at least -f must come right before &lt;zipped_filename&gt; -c : create a new bundle -v : verbose (output information about what’s going on) -f : save in file tar -xvf &lt;archived_filename&gt; &lt;files_to_zip&gt;: -x: extract files zip archive.zip file1 file2 ...: zip files zip -r archive.zip folder/: zip folder recursively unzip -Z &lt;zip_filename&gt;: show what’s inside the zip file unzip &lt;zip_filename&gt;: unzip the file Piping 1&lt;command1&gt; | &lt;command2&gt; ls -al /bin | less: show everything in directory /bin as scrollable history | tail -20 | head -10 : the most recent 10th - 19th file Redirection If you don’t specify, the output or input of a command comes from the terminal command &gt; file: write the output of the command into file (overwrite) command &lt; file: take the file as input of a command line command 2&gt; file: outputs the error message to a file (stderr(2) in C) command &gt;&gt; file: append the output of the command into file (doesn’t overwrite) LEC06 Loops and Variables (02/03) Environment and Variables environment variables: in the computer local variables: only in current shell Shebang #!/bin/sh: execute the file using Bourne shell (sh): describes the shell programming language, usually its implementation points to /bin/bash #!/bin/bash: execute the file using bash shell (bash): an sh-compatible implementation with modern implementation exit code returned value of main will be printed out if executing the script in Linux exit N: exit with status N executing multiple commands in a row cmd1; cmd2: execute cmd 1 first, then cmd 2 cmd1 &amp;&amp; cmd2: execute cmd2 only if cmd 1 returns 0 (exited normally) cmd1 || cmd2: execute cmd2 only if cmd 1 doesn’t return 0 (failed) Scripting We mostly use bash in our scripting. So remember to include #!/bin/bash in the top Variables storing command output: var = \"$(echo hello world)\" if statement 123456789if [ CONDITION_1 ] then # statementselif [ CONDITION_2 ]then # statementselse # statementsfi if...then...fi part is necessary. elif and else are allowed, but not necessary. Shorten codes with ; to write them in one line, like if [[ 0 –eq 0]]; then echo “Hiya”; fi for loop 123for (( i = 0; i &lt;= 11; ++i )); do echo “i: $i”; done while loop 1234567891011121314151617s=“s” while [[ &quot;$s&quot; != &quot;ssss&quot; ]]; do echo &quot;$s&quot; s=&quot;s$s&quot;donex=0 while (( x &lt;= 11 )); do echo &quot;x: $x&quot; (( ++x ))done# Loop through lines in a filefile=“filename.txt”while read -r line; do echo &quot;Line: $line&quot;done &lt; &quot;$file&quot; Comparing Values Numbers $n1 –eq $n2 tests if n1 == n2 $n1 –ne $n2 tests if n1 != n2 $n1 –lt $n2 tests if n1 &lt; n2 $n1 –le $n2 tests if n1 &lt;= n2 $n1 –gt $n2 tests if n1 &gt; n2 $n1 –ge $n2 tests if n1 &gt;= n2 Strings “$s1” == “$s2” tests if s1 and s2 are identical “$s1” != “$s2” tests if s1 and s2 are different Path Testing Test if /some/path exists: -e /some/path Test if /some/path is a file: -f /some/path Test if /some/path is a directory: -d /some/path Test if /some/path can be read/written/execute: -r/-w/-x /some/path Arithmetic Expression Put expressions inside (( )) . In script, you need to put $ before expressions to read values. Below are some examples 123echo $(( 2 + 3 )) #5x=10; sum=$(( $x+10 ))echo $sum #20 Passing Arguments $1, $2, …, $10: values of the first, second, etc. arguments If 3 arguments are given, $4, $5, … higher are empty $0 is the name of the script $# is the number of arguments (argc in C) $? Is the exit code of the last program executed You can have your script set this with exit &lt;number&gt; (read man exit) No explicit call to exit is the same as exit 0 (a.k.a, success!) $* expands \\$1 .. \\$n into one string, has the same effect as “\\$1 \\$2 … \\$n” (one string) $@ expands $1 .. $n into individual strings, same as “$1” “$2” .. “$n” (n strings) Be careful with spacing comparing two variables Lec07 Your Shell, Job, and Processes (02/05) Resource Monitoring Commands ps &lt;PID&gt; (process snapshot): report the current running processes, including PID ps -C &lt;command_name&gt;: report the current process using its corresponding shell command top: displays CPU usage of current processes htop: better version of top, though not pre-installed in many Linux distributions free -h: display available memory in human-readable format nvidia-smi: display Nvidia GPU information Examples 1234ps –C firefox #find firefox&#x27;s pid through its command name61860 ... firefoxhtop -p 61860 #display usage of this specific process Modifying Processes nice -n &lt;priority:int&gt; &lt;command name&gt;: initialize command with non-default priority renice -n &lt;priority&gt; -p &lt;PID&gt;: readjust the priority of a running process kill &lt;PID&gt;: kill this process killall &lt;command name&gt;: kill processes by name, kill all processes related to this program Jobs When we are executing ping or installing big packages, we may lose control of our command line temporarily. And we may want to run these commands in the background. &lt;command&gt; &amp;: run the command in background, but will still print output in the terminal jobs: report jobs working in background bg &lt;job_id&gt;: resumes the job in background (note: job id should come after %, like %1, or the command will take it as the PID) fg &lt;job_id&gt;: resume job in the foreground Lec08 Your Shell (02/10) source &lt;script_name&gt;: the command runs script in the current shell, not as usual in a spawned shell exec $shell: restart the current shell (source) alias &lt;new_name&gt;=&lt;old_name&gt;: e.g. x = 'cd /Desktop' ssh -X: allows X11 rendering (allows graphic interface through remote server) scp [flags] &lt;from&gt; &lt;to&gt; (secure copy): copy files from the internet (remote host): Must specify the user on the remote host. Syntax for remote client: user@host:/path (Note You need the : to start the path) ctrl + r reverse search your history for the most recent command that has the string you just typed in. Lec10 Shell Expansions and Search (02/14) Grammar of Shell Expansions *: multiple character wildcard: match any string, including the empty string ?: single character wildcard: match a single character: matches exactly one but what that character is doesn’t matter [brackets]: [a-z, A-Z] matches one character in the range [^ ...]: not, [^abc] matches any character that is not a, b, or c &#123;... , ...&#125;: matches any pattern inside the comma separated braces. &#123;Hello,World&#125; matches either “Hello” or “World” \\ : escape space: &#123;Hello, Goodbye&#125; World = Hello Goodbye World &#123;Hello, Goodbye&#125;\\ World = Hello World Goodbye World (the space is escaped, so “World” is now taken as a part of the set of words) $: to read values (echo $PWD reads the PWD variable and then echo its value) &lt;: create instream from file &gt; &gt;&gt;: direct output to a file (overwrite or append) GREP grep &lt;pattern&gt; [input] Globally search a Regular Expression and Print. GREP can be used to search or filter large amounts of data. grep -r &lt;pattern&gt; ./ search current directory and all its subdirectories for the pattern specified grep -i &lt;p&gt; ./ ignore upper/lower case distinctions -v display those lines that do NOT match -n precede each matching line with the line number -c print only the total count of matched lines Regular Expressions a?: search for a with 0 or 1 appearance a*: search for a with 0 or multiple appearance a+: search for a with 1 or multiple appearance .: wildcard Lec11 Sed, Cut, and Paste cut cut -c M-N file: print out the Mth to Nth characters (-c) in file cut -d \" \" -f 1 t.txt: print out the first field of each line in file t.txt, field is determined by delimiter space sed sed is the Stream EDitor. It goes line by line searching for the regular expression Print sed '/pattern/p': print out all occurrences that meet pattern. Replace sed 's/pattern1/pattern2/': sed 's/no spoon /a fork/g' no_spoon.txt: replace every occurrence in the whole document (globally: /g) of “no spoon” with “a fork” Delete sed '/pattern/d': sed '/[Dd]avid/d' david.txt: executes delete command (/d) Extended regular expression sed -E '...': let sed to use the more usual version of regex, where + ? () have special meanings. See here for an explanation of Extended Syntax xargs xargs: can read from stdin, so it can pass output from other commands to scripts that only take in arguments, not from stdin. xargs -n 1 &lt;command&gt;: only feeds 1 arguments to the next command. See the next grep 688 for an example xargs -I '&#123;&#125;' &lt;command&gt; '&#123;&#125;': -I specify where to use the the argument read in by xargs. Specifically, for the results fed into xargs, we give them a name &#123;&#125;, when we later run the next command, replace occurrence of &#123;&#125; in that command with those results. e.g. grep 688 ./ | xargs -n 1 -I '&#123;&#125;' mv '&#123;&#125;' ./results/ for each of the file that was found to contain 688 in their name, move that file to results/ we can also use a token other than &#123;&#125;, for example cat directories.txt | xargs -I % sh -c 'echo %; mkdir %' shift &amp; paste shift &lt;number&gt;: drop the first &lt;number&gt; arguments paste: merge multiple files paste –d , names.txt phones.txt &gt; result.csv: merge names and phones together, -ddelimit them with ‘,’ paste –d , -s names.txt phones.txt &gt; result.csv: merge file serially (-s) instead of in parallel LEC12 awk, gawk, and Process Substitution awk Use awk on delimited fields on a per-line basis The basic structure of an awk program is: 1234BEGIN &#123;commands&#125;pattern1 &#123; commands1 &#125;pattern2 &#123; commands2 &#125;END &#123;commands&#125; awk '/[Mm]onster/ &#123;print&#125;' frankenstein.txt: find regex [Mm]onste and print the lines out. awk '/[Mm]onster/' frankenstein.txt: if not specified, the default action is to print the whole line awk '/[Mm]onster/ &#123;print $0&#125;' frankenstein.txt: $0 refers the whole line awk '/[Mm]onster/ &#123;print $1&#125;' frankenstein.txt: prints the first word of the line contains our pattern awk '/Ron/&#123;print $3&#125;' marks.txt: prints the third column of the line containing ‘Ron’ in the file ‘marks.txt’ You can also use awk without specifying a pattern: awk 'BEGIN&#123;x=5; y=10; z=x+y; print z&#125;': arithmetic in awk awk '&#123;print $1 \",\"&#125;' t.txt: print out the first field of each line plus a colon in file t.txt, field is determined by delimiter space &amp;&amp; || a?b:c !(a&amp;&amp;b): also work in awk. If you want to do regular expression, you need to enclose them with /regular expression/ awk '/s/?/8./:/9./ &#123;print&#125;' marks.txt: If there’s an ‘s’, look for grade in 80s, otherwise grade in 90s awk '!/s/ &#123;print&#125;' marks.txt: Look for all lines that do not contain an ‘s’ Process Substitution We can treat a command of series of commands as if they were a file &lt; (list): treat the list of commands as input e.g. echo \"This is a test\" &gt; &gt;(wc –w) &gt; (list): treat the list of commands as output file e.g. while read x; do echo $x; done &lt; &lt;(git log) LEC13 Advanced Bash Scripting Condition Statements: case case employs a patter match, using shell expansion 123456789case &quot;$var&quot; in&quot;A&quot; ) #commands to execute if [[ $var == &quot;A&quot; ]]2 ) #commands to execute if [[ $var -eq 2 ]][2-4] ) ##commands to execute if [[ $var -ge 2 ]] &amp;&amp; [[ $var -le 4 ]]* ) #default commands Arrays 1234567891011121314151617arr = ( use parentheses and seperate items by space )my_arr = ( &quot;a string&quot; 1 ) % can be of multiple types# You can also customize the indexes inside the array (so its more like a dictionary instead of a traditional array)my_arr[44] = &quot;string&quot;# perform an array operation by $&#123;expr&#125;echo &quot;Index 51: $&#123;arr[51]&#125;&quot;#iterate through the array as individual itemsfor x in &quot;$&#123;arr[@]&#125;&quot;; do echo &quot;$x&quot;; done#iterate through the array as a joined long seriesfor x in &quot;$&#123;arr[*]&#125;&quot;; do echo &quot;$x&quot;; done#iterate through the list of indexesfor idx in &quot;$&#123;!new_arr[@]&#125;&quot;; do echo “$idx”; done Lec99 Practical Tasks find . -name \"*.png\" -type f -print0 | xargs -0 rm -v -rf \"&#123;&#125;\" reference find . -name \"*.png\" -type f: search files (type -f) with this name -print0: names will be terminated by a null character, and spaces and strange characters will be catered for. xargs -0: xargs is also going to consider filenames to be null-terminated, and spaces and strange characters will not cause problems. rm -v -rf \"&#123;&#125;\": The “{}” is replaced by each filename.","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Installing and Configuring Ocaml on Linux","slug":"2020-01-20-Installing-Ocaml-on-Linux","date":"2020-01-20T05:00:00.000Z","updated":"2022-06-08T19:25:40.000Z","comments":true,"path":"2020-01-20-Installing-Ocaml-on-Linux/","permalink":"https://yao-lirong.github.io/blog/2020-01-20-Installing-Ocaml-on-Linux/","excerpt":"Install Ocaml run sh &lt;(curl -sL https://raw.githubusercontent.com/ocaml/opam/master/shell/install.sh) and install opam at /usr/bin (Caution: install it under this directory to ensure you can also access opam at user’s directory)","text":"Install Ocaml run sh &lt;(curl -sL https://raw.githubusercontent.com/ocaml/opam/master/shell/install.sh) and install opam at /usr/bin (Caution: install it under this directory to ensure you can also access opam at user’s directory) Configure Ocaml and VSCode initiate with sandbox (bubblewrap) disabled opam init --bare -a -y --disable-sandboxing switch OPAM to the OCaml 4.09.0 compiler: 123# Note: do NOT prefix these commands with sudoopam switch create 4.09.0 ocaml-base-compiler.4.09.0eval $(opam env) install OPAM packages needed 12opam install -y utop ounit qtest yojson lwt lwt_ppx menhir ansiterminal lambda-term merlin ocp-indent user-setup bisect_ppx-ocamlbuildopam user-setup install install “OCaml and Reason IDE” and configure settings.json in vscode as such: 12345678910&#123; &quot;workbench.colorTheme&quot;: &quot;Solarized Light&quot;, &quot;editor.tabSize&quot;: 2, &quot;editor.rulers&quot;: [ 80 ], &quot;editor.formatOnSave&quot;: true, &quot;reason.path.ocamlmerlin&quot;: &quot;/home/mint/.opam/4.09.0/bin/ocamlmerlin&quot;, &quot;reason.path.ocamlfind&quot;: &quot;/home/mint/.opam/4.09.0/bin/ocamlfind&quot;&#125; note that it first showed that “cannot locate ocamlmerlin binary.” I fixed the problem by changing directory of “ocamlmerlin” and “ocamlfind” in the settings. (Find their path with which ocamlmerlin)","categories":[],"tags":[{"name":"CS3110","slug":"CS3110","permalink":"https://yao-lirong.github.io/blog/tags/CS3110/"}]},{"title":"Look Back on Cornell 19FA","slug":"2019-12-22-Cornell-19FA-总结","date":"2019-12-22T05:00:00.000Z","updated":"2025-09-03T01:42:46.384Z","comments":true,"path":"2019-12-22-Cornell-19FA-总结/","permalink":"https://yao-lirong.github.io/blog/2019-12-22-Cornell-19FA-%E6%80%BB%E7%BB%93/","excerpt":"CS2112 OO Design Data Structs (Honors) My goal in teaching this course has been to make you guys fearless about taking ideas and turning them into systems of codes that work – Andrew Myers, 2019/12/10","text":"CS2112 OO Design Data Structs (Honors) My goal in teaching this course has been to make you guys fearless about taking ideas and turning them into systems of codes that work – Andrew Myers, 2019/12/10 从A1到A7肉眼可见自己的成长，真的想学CS的话一定要上这个课，因为这个课所以我觉得这一学期过得特别快，两周一个project，一学期就这样过去了。 A4-A7 Git Commit History A1: 什么嘛，还是挺简单的吗（结果拿了低于一个deviation） A2(Isaac)：因为亲爱的中国朋友们自己有自己的队友，只有一个黑哥找我当队友。这个黑哥好像是课上唯一一个黑哥，真的非常努力。但是说实话有的时候忙不到点上去，忙了白忙。最后RSA因为一个莫名其妙的bug，又是低于一个median (128 commits) A3(Ralph)：我最爱的数据结构，Chinese American 队友，强是真的强。可以直接无障碍读Java源码，他的hashTable估计就是这样写出来的，不然更强，因为他的hashTable写得实在太好了。基本上被他带了，我就写了个trie而已。但有可能是因为这次作业太简单，而且相对独立，我俩都觉得不需要和对方交流，结果没什么团队合作的感觉，做的还不如A2带感 (152 commits) A4(Xinyu &amp; Faizaan)：终于有中国人没有队友了（，所以是一个中国妹子和一个美国人做得队友。这个美国人，真的不正经，明明能提前做完非要拖到最后一秒写。幸亏因为A4整个结构是我写的，我几乎对它有绝对了解和掌控，进行得还是很顺利的。这个中国小姑娘是真的强，给她讲明白了就能写，写出来还一点都没bug A5(Xinyu &amp; Faizaan)：因为A4我要关心的东西太多，整个给我整虚脱了，于是我把A5全权交给了美国人，结果作为代码量最大的project之一，他又没写完，应该是我做的最差的一次作业。但是最后成绩没有想象的那么差，主要批的松。 A6(Xinyu &amp; Faizaan)：因为我懒得学习JavaFx一堆乱七八糟的语法，所以我又全权交给了美国人，这个时候其实有点破罐子破摔的感觉。同时我把A5用不了的地方整个重写了一遍。最后一晚上非常刺激，虽然我们deadline之前一个小时做完了，但是最后一刻我改了改一些错误信息展示的代码让整个GUI更好看，结果在测试的时候发现了一个bug。我以为是我的错误，于是git reset回去，交上了以前的版本。刺激在于我卡着deadline交上的，更刺激的是我交完以后发现我交的仍然是最新版本，所以bug仍然存在，心情起伏最大的一晚上。（不过最后发现和我那一个小时的改动没关系，是model不是GUI出了问题） A7(Xinyu &amp; Faizaan)：server我觉得还是要学一下的，和A4一样试着写了个结构。小姑娘帮我把server全写完了，最可怕的是一个bug都没有，真的吓人。美国人可能因为要准备自己的Final，所以这一次竟然在deadline之前六七个小时就写完了，然后早早提交上去了。非常满意的一次作业（也是除A3外唯一的高于median的作业）(475 commits in total) 本来指着Final里面擅长的算法和数据结构提提分，结果Final出点过于简单，并拉不开分差。要是出到和给的example同一个难度，我是有信心这节课拿A的。 MATH 2210 Linear Algebra 我的教授是个老爷子(James West) ，老爷子快九十了吧，一句话说到最后一个词的时候我已经忘了他第一个说的词是什么了，人真的很好，可是脑子确实老化了，并不是那个最好的讲师。 后来换到 Myer 的 session，讲得确实不错（West 那里的人全跑她那去了），最后一个 OH 还多留了三十分钟专门给我把 Inner Product Space 讲明白了，一位优雅又善良的女老师。 DEA 1500 Intro to Environ. Psychology 本专业唯一一节我觉得可以上的课，而且评价也很好，于是就上了。Gary Evans 绝对是数一数二的讲师，清晰有条理，课程内容也很有趣（虽然后期有些同质化）。如果说美国1%的人掌握着99%的财富，那我也要说美国1%的聪明并真正有着人文情怀的人领到了99%的傻子，就是这1%的人构建了部分人心目中的人类文明灯塔。Gary 就是那个1%。能从他的课中感受到他对这个星球甚至整个人类的爱。 Sometimes we focus on China’s carbon footprint and says it should regulate more. However, look at the data of the United States per capita. I’m not saying China shouldn’t regulate more, but isn’t it a little hypocritical to ask China to do more regulations while we are producing more than them? 虽然不想捧一踩一，但是Gary其实应该也很老了。即使如此却一直用饱满的精神为大家呈现这门课，最后几个lecture不知道他怎么了，有的时候会失掉自己的声音，整个声音突然就变哑了，那个时候真的是心疼他。他的最后一课我会永远记在心里。 我知道你们中有很多人是各方面的积极分子，比如环保主义者，你有的时候也觉得为什么大家都不理解我在做的事都是为了他们好，你有的时候也会觉得自己非常无助。我要向大家展示这一句话：这些做出巨大改变的运动，往往是由一小群人领导的。回想一下，他（甘地）当时是大多数吗？他（马丁路德金）也不是。你可能觉得这些人都是一开始就非常伟大的 … 这个当时只有十二岁的小女孩，向世界揭露了纳粹的罪行，她当时有多么强大的能力吗？… 有时你可能会觉得很无力，但是记住Margaret说了什么：永远都是那一小部分人。 – Gary Evans, 2019/12/09 Don’t Feel Helpless WRIT 1370 FWS: Elements of Acad Wtg Brad是个稍微带点痞气的不错的人，虽然这课叫做 Metaphor in Art, Science and Culture 但我实际上觉得他其实这三个都并不很懂… Don’t be Strangers. – Brad Zukovic, 2019/12/06 CS5199 Comp Program &amp; Problem Solving 狗屎课，课程都进行了一大半了才告诉别人用什么方式计算attendance，幸亏最后运气好刚刚好到了pass的线，不然要是第一学期成绩单上就有个fail或者withdraw，那是真的难看。 PE1395 Self Defense 挺好玩的","categories":[],"tags":[{"name":"Cornell","slug":"Cornell","permalink":"https://yao-lirong.github.io/blog/tags/Cornell/"},{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"}]},{"title":"Add pdf file to hexo","slug":"2019-12-17-add-pdf-file-to-hexo","date":"2019-12-17T05:00:00.000Z","updated":"2022-06-08T19:34:22.000Z","comments":true,"path":"2019-12-17-add-pdf-file-to-hexo/","permalink":"https://yao-lirong.github.io/blog/2019-12-17-add-pdf-file-to-hexo/","excerpt":"Say you’ve put book.pdf in ./source/books/, then put something like [Download my book!](/books/book.pdf) anywhere in the article and it will create the link for you. Cited From hexo","text":"Say you’ve put book.pdf in ./source/books/, then put something like [Download my book!](/books/book.pdf) anywhere in the article and it will create the link for you. Cited From hexo","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"Import Junit and JavaFx into VSCode","slug":"2019-11-21-import-Junit-and-JavaFx-into-VSCode","date":"2019-11-21T05:00:00.000Z","updated":"2022-07-17T05:37:34.000Z","comments":true,"path":"2019-11-21-import-Junit-and-JavaFx-into-VSCode/","permalink":"https://yao-lirong.github.io/blog/2019-11-21-import-Junit-and-JavaFx-into-VSCode/","excerpt":"The import javafx cannot be resolved I have java 11 installed on my computer. JavaFx was not bundled with Java 11 so I have to first download it from here and put it at the same directory as Java jdk. But VScode still gives me the error “The import javafx cannot be resolved”. This was resolved after I ran Java: Clean the Java Language Server workspace, though I have no idea how VSCode magically found the directory of JavaFx.","text":"The import javafx cannot be resolved I have java 11 installed on my computer. JavaFx was not bundled with Java 11 so I have to first download it from here and put it at the same directory as Java jdk. But VScode still gives me the error “The import javafx cannot be resolved”. This was resolved after I ran Java: Clean the Java Language Server workspace, though I have no idea how VSCode magically found the directory of JavaFx. The import Junit.Jupiter cannot be resolved I downloaded Junit.Jupiter.api (the package I used for my tests) from here and put the jar file under workspace\\lib. Then problem was solved. Tests are now runnable.","categories":[],"tags":[{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"}]},{"title":"P1162 填涂颜色","slug":"2019-07-07-P1162-填涂颜色","date":"2019-07-07T04:00:00.000Z","updated":"2019-10-06T21:02:30.000Z","comments":true,"path":"2019-07-07-P1162-填涂颜色/","permalink":"https://yao-lirong.github.io/blog/2019-07-07-P1162-%E5%A1%AB%E6%B6%82%E9%A2%9C%E8%89%B2/","excerpt":"题目来源：洛谷P1162 填涂颜色 如果被包围的部分没什么特点，那么就可以看看其他部分有没有什么特点，本题中就是可以从外围开始搜索，搜到墙就停下，最后没被搜到的部分就是被墙包围的部分 为了防止外围起点就是墙，或者是外围的0被墙分为好几部分导致我们无法搜索到被分割的部分，可以多开一圈数组，使得外围相互连接起来，确保不被包围的0一定可以被搜到","text":"题目来源：洛谷P1162 填涂颜色 如果被包围的部分没什么特点，那么就可以看看其他部分有没有什么特点，本题中就是可以从外围开始搜索，搜到墙就停下，最后没被搜到的部分就是被墙包围的部分 为了防止外围起点就是墙，或者是外围的0被墙分为好几部分导致我们无法搜索到被分割的部分，可以多开一圈数组，使得外围相互连接起来，确保不被包围的0一定可以被搜到 1234567891011121314151617181920212223242526272829303132333435363738#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;int n,dir[4][2]=&#123;&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,1&#125;,&#123;0,-1&#125;&#125;;bool graph[32][32],vis[32][32];///graph 记录这个图原来是0还是1（空还是墙），vis记录被没被访问过///注意这两个数组都是开的32（数据是1~30，本来开31就够了）保证了外面又一圈0，也就是最外面的一圈都是联通的，避免了第一个点1,1就是墙导致循环循环不下去的情况，也避免了一堵墙把中间全部堵死，只扫了墙左边的0，没扫同在墙外但在墙右边的0的情况void dfs(int, int);int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) for(int j=1;j&lt;=n;j++) cin&gt;&gt;graph[i][j]; dfs(0,0); for(int i=1;i&lt;=n;i++)&#123; for(int j=1;j&lt;=n;j++)&#123; if(vis[i][j]) cout&lt;&lt;&quot;0 &quot;;///如果访问过，那么一定是墙外的0 else if(graph[i][j]) cout&lt;&lt;&quot;1 &quot;;///输入时原本是墙，那么它还是墙（这一句可以和上面那一句互换位置） else cout&lt;&lt;&quot;2 &quot;;///及没被访问过，输入时也不是墙，那么肯定是被包围在里面的0 &#125; cout&lt;&lt;endl; &#125; return 0;&#125;void dfs(int x, int y)&#123; if( x&lt;0 | x&gt;n+1 | y&lt;0 | y&gt;n+1 | graph[x][y] | vis[x][y] ) ///如果1.越界（注意是 &lt;0|&gt;n+1 这里可以看出来我们多开了一圈0在原输入外面） ///2.是墙|被访问过 那么不做操作 return; vis[x][y]=true; for(int i=0;i&lt;4;i++) dfs(x+dir[i][0],y+dir[i][1]);&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1141 01迷宫","slug":"2019-07-04-P1141-01迷宫","date":"2019-07-04T04:00:00.000Z","updated":"2019-10-06T21:02:22.000Z","comments":true,"path":"2019-07-04-P1141-01迷宫/","permalink":"https://yao-lirong.github.io/blog/2019-07-04-P1141-01%E8%BF%B7%E5%AE%AB/","excerpt":"题目来源：洛谷P1141 01迷宫 BFS找连通块 连通块肯定还是DFS找得快，因为一开始不知道什么是连通块所以写了个BFS 搜索连通块不需要记录从某一个点出发最远能到达哪个点（BFS搜索最短距离），只需要记录从某一个点出发一共经过了多少个符合条件的点就行了（BFS搜索连通块），因为如果A和B联通，B和C联通，那么A和C必然联通，所以一次搜索能达到的所有点必定在同一个连通块内","text":"题目来源：洛谷P1141 01迷宫 BFS找连通块 连通块肯定还是DFS找得快，因为一开始不知道什么是连通块所以写了个BFS 搜索连通块不需要记录从某一个点出发最远能到达哪个点（BFS搜索最短距离），只需要记录从某一个点出发一共经过了多少个符合条件的点就行了（BFS搜索连通块），因为如果A和B联通，B和C联通，那么A和C必然联通，所以一次搜索能达到的所有点必定在同一个连通块内 如果对于每个查询我们都搜索一遍的话肯定会超时，所以我们用vis数组记录下来一次搜索中所有经过的点所在的（必定是同一个）连通块的序号，ans数组记录相应序号的连通块中共有几个点，下次如果询问的点已经被搜索过就可以免去搜索直接输出答案了。 程序向本来被封死的地方走了一步，可能是初始化的问题。初始化就别用那些花里胡哨的句子了，还是老老实实写for循环吧，反正时间空间够用，别的句子太容易出错 莫名其妙输出来几十万的数据，估计是爆内存了，数组一定要根据题目要求开得足够大 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;using namespace std;struct node&#123; int x,y;&#125;;int m,n,question[100005][2],dir[4][2]=&#123;&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,1&#125;,&#123;0,-1&#125;&#125;,vis[1003][1003],ans[100005],connect=1; bool maze[1003][1003];///question 存储了每组询问的起点坐标///vis = 0 代表这个点没走过，现在不在任何一个连通块里面；vis = x (x&gt;0) 代表这个点走过了，并且在x号连通块里面；这个数组一定要开大，只开到1003是不行的，必须要开到和question一个数量级///ans 存储每个连通块里面共有几个点///maze 存储这个迷宫///connect 代表各个连通块的序号int bfs(int, int); bool legal(node, node);int main()&#123; cin&gt;&gt;n&gt;&gt;m; for(int i=1;i&lt;=n;i++) for(int j=1;j&lt;=n;j++)&#123; char a; cin&gt;&gt;a; if(a==&#x27;0&#x27;) maze[i][j]=0; else if(a==&#x27;1&#x27;) maze[i][j]=1; &#125;///因为每个输入之间是不空格的，所以用char来输入做一个巧妙的处理 for(int i=1;i&lt;=m;i++) cin&gt;&gt;question[i][0]&gt;&gt;question[i][1]; ///初始化最后出了很大问题，还是用最保险的for循环 for(int i=0;i&lt;100005;i++) ans[i]=1; for(int i=0;i&lt;1003;i++) for(int j=0;j&lt;1003;j++) vis[i][j]=0; for(int i=1;i&lt;=m;i++) cout&lt;&lt;bfs(question[i][0],question[i][1])&lt;&lt;endl; return 0;&#125;bool legal(node now, node next)&#123; ///这里不需要分情况讨论“如果 now==0 则必须 next==1” 或相反，只需要确保now和next的值不同就行了 return maze[now.x][now.y]!=maze[next.x][next.y] &amp;&amp; next.x&gt;=1 &amp;&amp; next.x&lt;=n &amp;&amp; next.y&gt;=1 &amp;&amp; next.y&lt;=n &amp;&amp; vis[next.x][next.y]==0;&#125;int bfs(int x, int y)&#123; queue &lt;node&gt; q; node start; start.x = x; start.y = y; if(vis[start.x][start.y]==0)&#123;///如果这个点不在现有的任何一个连通块内，则进入队列，开始bfs vis[start.x][start.y] = connect; q.push(start); &#125; else return ans[vis[start.x][start.y]];///如果这个点在某个连通块内，则直接输出此连通块对应的结果 while(!q.empty())&#123; node now = q.front(); q.pop(); for(int i=0;i&lt;4;i++)&#123; node next; next.x = now.x + dir[i][0]; next.y = now.y + dir[i][1]; if(legal(now,next))&#123; vis[next.x][next.y] = connect;///记录下来点next属于连通块connect ans[connect]++;///连通块connect中总点数+1 q.push(next); &#125; &#125; &#125; return ans[connect++];///所有属于connect的点都搜完了，返回答案，并使connect++，表示接下来要扫的是新的连通块&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1118 Backward Digital Sums","slug":"2019-07-02-P1118-Backward-Digital-Sums","date":"2019-07-02T04:00:00.000Z","updated":"2019-10-06T21:02:08.000Z","comments":true,"path":"2019-07-02-P1118-Backward-Digital-Sums/","permalink":"https://yao-lirong.github.io/blog/2019-07-02-P1118-Backward-Digital-Sums/","excerpt":"题目来源：洛谷P1118 数字三角形 暴力搜索 看到题目的第一眼想的就是直接搜，发现不少TLE，我还减了不少枝… 超时，只有20分，通过几个运行结果来看，写法应该对了，只是全都TLE而已","text":"题目来源：洛谷P1118 数字三角形 暴力搜索 看到题目的第一眼想的就是直接搜，发现不少TLE，我还减了不少枝… 超时，只有20分，通过几个运行结果来看，写法应该对了，只是全都TLE而已 ^在c++中不是幂运算，是异或（如果两个相应位为“异”（值不同），则该位结果为1，否则为0。0^0=0; 0^1=1; 1^0=1; 1^1=0）。幂运算用pow(base,power) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;using namespace std;int n,tri[13][13],ans[13]; bool flag=true;void dfs(int);int main()&#123; int start; cin&gt;&gt;n&gt;&gt;start; tri[n][1]=start; dfs(n-1); if(!flag) for(int i=1;i&lt;=n;i++) cout&lt;&lt;ans[i]&lt;&lt;&quot; &quot;; return 0;&#125;void dfs(int level)&#123; if(flag)&#123; int lbound=pow(2,level-1),ubound=tri[level+1][1]-lbound;///lbound(lower_bound)是假设第一行全是1，得出的后续几行的最小值 for(int i=lbound;i&lt;=ubound;i++)&#123; if(!flag) continue; tri[level][1]=i; bool jump=false; for(int j=1;j&lt;=n-level;j++)&#123; tri[level][j+1]=tri[level+1][j]-tri[level][j]; ///可以加一个判定条件，如果小于XXX，就停止本次搜索 if(tri[level][j+1]&lt;lbound) &#123;jump=true; break;&#125;///与前面的lbound定义一样，如果一不下心搜出来0或者-1那么肯定不对 if(tri[level][j+1]==tri[level][j]) &#123;jump=true; break;&#125;///自己发现的规律，相邻两项不可能相等，因为相邻的两项相等会导致再上面一层也有两项相等，直到第一层有两项相等，意味着答案不符合条件 &#125; if(jump) continue; for(int j=1;j&lt;=n-level+1;j++)&#123; cout&lt;&lt;tri[level][j]&lt;&lt;&quot; &quot;; &#125; cout&lt;&lt;endl; if(level==1)&#123; int sum=0; for(int i=1;i&lt;=n;i++)&#123; sum+=pow(2,tri[1][i]); sum-=pow(2,i); &#125;///这个地方用了一个二进制的性质算我们的答案第一行是否是1~N的数字且互不相同(每个1~N的数都有两种状态，在答案里有或者没有) if(sum==0)&#123;///sum==0 代表sum(pow(2,tri[1][i]))==sum(pow(2,i)) for(int i=1;i&lt;=n;i++) ans[i]=tri[1][i]; flag=false; &#125; /// 停止所有搜索，已找到答案 &#125; if(level&gt;1&amp;&amp;flag==true) dfs(level-1); &#125; &#125;&#125; 杨辉三角 以下为杨辉三角一维dfs，只有70分，还是需要剪枝 首先要搞清楚这个数字三角形究竟是什么吧。大家可以自己在草稿纸上写一下，假设n为一个比较小的数(比如，按样例，4)，设第一行的n个数分别为a,b,c,…(我这里是a,b,c,d)，然后模拟加一下，就会发现sum是…… 如果n为4，那么sum是a+3b+3c+d。 如果n为5，那么sum是a+4b+6c+4d+e。 如果n为6，那么sum是a+5b+10c+10d+5e+f。 观察各项的系数，你发现了什么？ 如果你有敏锐的数学眼光，你会发现，各项系数恰与杨辉三角有关！ 那么我们就可以枚举每个a,b,c,…，逐一与sum比较，就可以得出答案了。~ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;cmath&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;using namespace std;int yanghui[13][13],ans[13],n,sum; bool vis[13];int compute();void dfs(int);int main()&#123; cin&gt;&gt;n&gt;&gt;sum; memset(vis,true,sizeof(vis)); ///初始化杨辉三角 yanghui[1][1]=1;yanghui[2][1]=1;yanghui[2][2]=1; for(int i=3;i&lt;=n;i++)&#123; yanghui[i][1]=1; for(int j=2;j&lt;=i;j++) yanghui[i][j]=yanghui[i-1][j-1]+yanghui[i-1][j]; &#125; dfs(1);///从第一个位置开始搜索 return 0;&#125;int compute()///当已经枚举到了最后一位时，可以计算现在答案的值是否真的等于我们输入的sum&#123; int temp=0; for(int i=1;i&lt;=n;i++) temp+=ans[i]*yanghui[n][i]; return temp;&#125;void dfs(int pointer)&#123; for(int i=1;i&lt;=n;i++)&#123; if(vis[i])&#123; ans[pointer]=i; vis[i]=false; if(pointer==n&amp;&amp;compute()==sum) &#123; for(int i=1;i&lt;=n;i++) cout&lt;&lt;ans[i]&lt;&lt;&quot; &quot;; break; &#125; if(pointer&lt;n) dfs(pointer+1); ans[pointer]=0; vis[i]=true; &#125; &#125;&#125; 二维DFS 上面方法的问题在于：只在所有位置都选完的时候才判定是否符合条件(sum=sum_to_find|(在这个程序中)sum=compute())，但是有很多情况下在我们没选完的时候就已经不符合条件了(sum&gt;sum_to_find)。我们必须要排除这种情况，这样的话就必须在搜索的同时记录现在状态下合(sum)的大小，那么我们就需要写一个二维的dfs ~~实在太蠢了，把所有要判定的地方都加上&amp;flag运行时间直接从1500ms提到了101ms ~~ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;iostream&gt;#include&lt;cmath&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;using namespace std;int yanghui[13][13],ans[13],n,sum_to_find; bool vis[13],flag=true;int compute();void dfs(int,int);int main()&#123; cin&gt;&gt;n&gt;&gt;sum_to_find; memset(vis,true,sizeof(vis)); ///初始化杨辉三角 yanghui[1][1]=1;yanghui[2][1]=1;yanghui[2][2]=1; for(int i=3;i&lt;=n;i++)&#123; yanghui[i][1]=1; for(int j=2;j&lt;=i;j++) yanghui[i][j]=yanghui[i-1][j-1]+yanghui[i-1][j]; &#125; dfs(1,0); return 0;&#125;void dfs(int pointer,int sum)&#123; if(flag)&#123;///这个地方代码这么丑是因为我把所有要判定的地方都加上&amp;flag，不然TLE for(int i=1;i&lt;=n;i++)&#123; if(vis[i]&amp;&amp;flag)&#123; ans[pointer]=i; vis[i]=false; sum+=ans[pointer]*yanghui[n][pointer]; if(pointer==n&amp;&amp;sum==sum_to_find&amp;&amp;flag) &#123; for(int i=1;i&lt;=n;i++) &#123;cout&lt;&lt;ans[i]&lt;&lt;&quot; &quot;; flag=false;&#125; break; &#125; if(pointer&lt;n&amp;&amp;sum&lt;sum_to_find&amp;&amp;flag) dfs(pointer+1,sum); sum-=ans[pointer]*yanghui[n][pointer]; ans[pointer]=0;///这两句的顺序可不能搞混了，要先还原sum值，再还原ans值，因为sum值和ans值有关 vis[i]=true; &#125; &#125;&#125;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1019 单词接龙","slug":"P1019 单词接龙","date":"2019-06-26T04:00:00.000Z","updated":"2019-06-27T12:39:42.000Z","comments":true,"path":"P1019 单词接龙/","permalink":"https://yao-lirong.github.io/blog/P1019%20%E5%8D%95%E8%AF%8D%E6%8E%A5%E9%BE%99/","excerpt":"题目来源：洛谷P1019 单词接龙 调了好几天，最后请教了醉神(@magolor)，十分钟给我调好了… 这个程序一个问题就是循环根本就不会吧dict全循环一遍，那可能就是初始化出了问题： m=pointer 的位置，当时记得应该写一个副本 m 代替 pointer 被改变，但是写着写着忘了 m 具体应该在哪被初始化了，问题就出现在这 回溯的状态：一定要明确回溯应当回溯到具体那个状态，是ans_temp已经被改变的状态吗？还是未改变的状态？本题中是ans_temp未改变的状态","text":"题目来源：洛谷P1019 单词接龙 调了好几天，最后请教了醉神(@magolor)，十分钟给我调好了… 这个程序一个问题就是循环根本就不会吧dict全循环一遍，那可能就是初始化出了问题： m=pointer 的位置，当时记得应该写一个副本 m 代替 pointer 被改变，但是写着写着忘了 m 具体应该在哪被初始化了，问题就出现在这 回溯的状态：一定要明确回溯应当回溯到具体那个状态，是ans_temp已经被改变的状态吗？还是未改变的状态？本题中是ans_temp未改变的状态 我的解与标解 跟我的做法一样，区别只是标解用for循环枚举j而不是跟我一样用直接用dfs函数 下面是我的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;cstring&gt;using namespace std;char dict[21][30],ans[500]; int n,vis[21],ans_max=0,ans_temp=0;void dfs(int,int);int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) cin&gt;&gt;dict[i]; for(int i=1;i&lt;=n;i++) vis[i]=2; cin&gt;&gt;dict[0][0];///开头的字母 dfs(0,0); cout&lt;&lt;ans_max; return 0;&#125;void dfs(int word,int pointer)&#123; if(pointer&lt;strlen(dict[word]))&#123; ans_temp++; int ans_mark=ans_temp; ans_max=max(ans_max,ans_temp); dfs(word,pointer+1);///这句话以上是将本单词中的下一个字母加入答案字符串中，以下是将查看以这个字母为基准，能不能接上其他单词的龙 ans_temp=ans_mark; &#125; int m; for(int i=1;i&lt;=n;i++)&#123;///枚举n个词中哪个词的首字母可以和现在字符串的最后一个相同 if(vis[i]&gt;0)&#123; int j=0; ///!!! m=pointer; ///!!!其他人的做法是直接枚举单词 word 中的所有字母，不像我是通过 dfs(word,pointer+1)来枚举，所以不需要考虑pointer如何如何，我们这里如果对 pointer 后面的字母(pointer+1,pointer+2,...)一个个进行比对，必然会改变pointer的值，所以用了一个副本 m 进行比对，保证pointer值不变 if(dict[word][m]==dict[i][j])&#123;///如果头一个字母相同 bool flag=true; while(m&lt;strlen(dict[word]))&#123;///一直比对到最后一个字母，并且这里判断了“相邻的两部分不能存在包含关系”这一条件 if(dict[word][m]!=dict[i][j]) flag=false; m++; j++; &#125; if(flag)&#123;///如果每个字母都一样 ///!!! int ans_mark=ans_temp;///记录一下现在的长度，因为再进行其他的dfs，ans_temp会被更新 ans_temp+=j-1;///答案字符串的长度就可以加上新加入的单词的长度 ///!!!注意这个地方这两句话应该是先记录 ans_temp 再更新 ans_temp ，因为我们最后回溯的时候是要回溯到没有接龙接上当前单词的状态，所以应该是先记录，后更新 ans_max=max(ans_max,ans_temp); vis[i]--; dfs(i,j); vis[i]++; ans_temp=ans_mark;///把ans_temp更新回来 &#125; &#125; &#125; &#125;&#125; 先预处理得到两两单词间的最短重合部分，然后搜索得到答案 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;cstring&gt;#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;string dict[31]; int ans_now=1,ans_max=0,n,vis[31],overlap[31][31];///这个地方把 ans_now 设为1，是因为第一个开头的字母不会被算在长度内，为了把这个开头的字母算进去，ans_now 从1开始int find_overlap(string,string);///寻找两个单词的最小重合部分void dfs(int);int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) cin&gt;&gt;dict[i]; for(int i=1;i&lt;=n;i++) vis[i]=2; cin&gt;&gt;dict[0];///开头的字母 memset(overlap,0,sizeof(overlap)); for(int i=0;i&lt;=n;i++) for(int j=1;j&lt;=n;j++) overlap[i][j]=find_overlap(dict[i],dict[j]); dfs(0); cout&lt;&lt;ans_max; return 0;&#125;int find_overlap(string a, string b)///其中，a是将要被别人接上去的字符串，b是想要接上去的字符串&#123; for(int i=a.size()-1;i&gt;=0;i--)&#123;///倒序寻找最小重合部分的大小 int ja=i,jb=0; bool flag=true; while(ja&lt;a.size())&#123;///正序看看这个部分是否重合 if(a[ja]!=b[jb]) &#123;flag=false; break;&#125; ja++; jb++; &#125; if(flag)&#123; int ans_overlap=a.size()-i; ///得到重叠部分的大小 if(ans_overlap!=a.size()&amp;&amp;ans_overlap!=b.size()) return ans_overlap; ///判断重叠部分是不是包含部分，如果不是就返回答案 else if(a.size()==1) return ans_overlap; ///如果是开头的字母(cin&gt;&gt;dict[0])，永远有 ans_overlap==a.size()==0 那么不管ans_overlap!=a.size()是否成立，我们都要返回答案 else return 0;///其他的情况下，重叠部分是包含部分，不能接龙，所以返回0 &#125; &#125; return 0;///没找到重叠部分&#125;void dfs(int word)&#123; for(int i=1;i&lt;=n;i++)&#123; if(overlap[word][i]&gt;0 &amp;&amp; vis[i]&gt;0)&#123; ans_now+=dict[i].size()-overlap[word][i];///接上的字符串的长度 ans_max=max(ans_max,ans_now); vis[i]--; dfs(i); vis[i]++; ans_now-=dict[i].size()-overlap[word][i]; &#125; &#125;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1101 单词方阵","slug":"P1101 单词方阵","date":"2019-06-17T04:00:00.000Z","updated":"2019-06-27T12:38:12.000Z","comments":true,"path":"P1101 单词方阵/","permalink":"https://yao-lirong.github.io/blog/P1101%20%E5%8D%95%E8%AF%8D%E6%96%B9%E9%98%B5/","excerpt":"题目来源：洛谷P1101 单词方阵 dir 这个数组是很好用的，不需要为8个方向特意写8个不同的函数，只需要写一个函数但是改变取哪一个dir[i]来判定哪一个方向符合条件就行了","text":"题目来源：洛谷P1101 单词方阵 dir 这个数组是很好用的，不需要为8个方向特意写8个不同的函数，只需要写一个函数但是改变取哪一个dir[i]来判定哪一个方向符合条件就行了 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;char matrix[103][103], yizhong[8]=&quot;yizhong&quot;; bool mark[103][103];/// matrix 存储每个位置的字母，yizhong 存储我们要找的字符串 &quot;yizhong&quot;，mark存储这个位置符不符合要求，最后要不要被变成 &quot;*&quot; 输出int dir[8][2]=&#123;&#123;0,1&#125;,&#123;1,1&#125;,&#123;1,0&#125;,&#123;1,-1&#125;,&#123;0,-1&#125;,&#123;-1,-1&#125;,&#123;-1,0&#125;,&#123;-1,1&#125;&#125;;/// dir[8] 存储了8个方向，dir[i][0] 是x轴坐标，dir[i][1] 是y轴坐标void dfs(int,int); /// 这个题虽然放在dfs里面但是因为这个字符串的方向是固定的，不会拐弯抹角，所以好像和dfs没什么关系int main()&#123; int n; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) for(int j=1;j&lt;=n;j++) cin&gt;&gt;matrix[i][j]; for(int i=1;i&lt;=n;i++) for(int j=1;j&lt;=n;j++) dfs(i,j); for(int i=1;i&lt;=n;i++)&#123; for(int j=1;j&lt;=n;j++)&#123; if(mark[i][j]) cout&lt;&lt;matrix[i][j]; else cout&lt;&lt;&quot;*&quot;; &#125; cout&lt;&lt;endl; &#125; return 0;&#125;void dfs(int x, int y)&#123; for(int i=0;i&lt;8;i++)&#123; ///对于任意一个点，我们都看看它周围八个方向各自符不符合条件 int j=0;bool flag=true; while(j&lt;7)&#123; ///分别检视每一个字符 与 yizhong 是否匹配 if(matrix[x+j*dir[i][0]][y+j*dir[i][1]] != yizhong[j])&#123; ///非常精髓 flag=false; break; /// 不匹配，flag=false &#125; j++; &#125; if(flag)&#123; ///匹配的话就记录下来它们是符合要求的，最后直接输出当前位置的字符 for(int j=0;j&lt;7;j++) mark[x+j*dir[i][0]][y+j*dir[i][1]]=true; &#125; &#125;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1219 八皇后","slug":"P1219 八皇后","date":"2019-06-16T04:00:00.000Z","updated":"2019-06-27T12:35:52.000Z","comments":true,"path":"P1219 八皇后/","permalink":"https://yao-lirong.github.io/blog/P1219%20%E5%85%AB%E7%9A%87%E5%90%8E/","excerpt":"题目来源：洛谷P1219 八皇后 我的解法 自己的程序出现的几个问题： 对角线表达式太过复杂，各层绝对值都可以简化，应该相信大部分情况下复杂的都是错误的 不需要记录这一行放没放过棋子，因为我们是按照一行一行的顺序放过来的，上一行必定有棋子，下一行必定无棋子","text":"题目来源：洛谷P1219 八皇后 我的解法 自己的程序出现的几个问题： 对角线表达式太过复杂，各层绝对值都可以简化，应该相信大部分情况下复杂的都是错误的 不需要记录这一行放没放过棋子，因为我们是按照一行一行的顺序放过来的，上一行必定有棋子，下一行必定无棋子 注意初始化的部分，一开始就是把循环语句里写成了 NE_SW[i]=true; NE_SW[i+1]=true 导致了错误而且一直没检查出来 最后那个存储 solution 的地方还是有问题，如果找到了某一行的一个解并且继续向下搜寻这一行的其他解的话，会出现不存储前面几行棋子位置的问题，没想到解决方法只能在输出的地方做了些操作 大部分标解中dfs只有一个参数就是行数x然后对于每个x进行for循环枚举纵坐标y，我这样的做法也可以 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;using namespace std;bool board[14][14],col[14],line[14],NE_SW[26],NW_SE[26]; int n,ans,solution[3][15];bool flag(int, int);void dfs(int, int);int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) &#123;col[i]=true; line[i]=true; NE_SW[i]=true; NW_SE[i]=true; NE_SW[i+n]=true; NW_SE[i+n]=true;&#125;/// 这个地方初始化一定要注意 dfs(1,1); for(int i=0;i&lt;3;i++)&#123; for(int j=1;j&lt;=n;j++)&#123; if(solution[i][j]==0) solution[i][j]=solution[i-1][j]; cout&lt;&lt;solution[i][j]&lt;&lt;&quot; &quot;; &#125; cout&lt;&lt;endl; &#125; cout&lt;&lt;ans;&#125;bool flag(int x,int y)///判定这个点能不能放棋子&#123; if(col[x]&amp;&amp;line[y])&#123;if(NE_SW[x+y-1])&#123; if( y&lt;=x &amp;&amp; NW_SE[n-int(abs(x-y))] ) return true; else if( y&gt;x &amp;&amp; NW_SE[n+int(abs(x-y))] ) return true; //cout&lt;&lt;&quot;最后一个对角线出了问题&quot;&lt;&lt;endl; &#125;//cout&lt;&lt;&quot;第一个对角线出了问题&quot;&lt;&lt;endl; &#125; //cout&lt;&lt;&quot;x,y有问题&quot;&lt;&lt;endl; return false;&#125;void dfs(int x,int y)&#123; ///cout&lt;&lt;&quot;function called at&quot;&lt;&lt;x&lt;&lt;&quot; &quot;&lt;&lt;y&lt;&lt;endl; if(flag(x,y))&#123; //if(x==n) ans++; if(ans&lt;=3) solution[ans][x]=y; ///cout&lt;&lt;endl&lt;&lt;&quot;one unit placed at&quot;&lt;&lt;x&lt;&lt;&quot; &quot;&lt;&lt;y&lt;&lt;endl&lt;&lt;endl; col[x]=false; line[y]=false; if(y&lt;=7-x) NE_SW[x+y-1]=false; ///左上角部分的 右上-左下对角线 else NE_SW[x+y-1]=false; /// 右下角部分的 右上-左下对角线 if(y&lt;=x) NW_SE[n-int(abs(x-y))]=false; ///右上角部分的 左上-右下对角线 else NW_SE[n+int(abs(x-y))]=false; ///左下角部分 左上-右下对角线 if(x==n) &#123;ans++;&#125; ///cout&lt;&lt;&quot;-------------&quot;&lt;&lt;endl&lt;&lt;&quot;one solution found, total solution is now &quot;&lt;&lt;ans&lt;&lt;endl&lt;&lt;&quot;-------------&quot;&lt;&lt;endl;&#125; else dfs(x+1,1); col[x]=true; line[y]=true; if(y&lt;=7-x) NE_SW[x+y-1]=true; ///左上角部分的 右上-左下对角线 else NE_SW[x+y-1]=true; /// 右下角部分的 右上-左下对角线 if(y&lt;=x) NW_SE[n-int(abs(x-y))]=true; ///右上角部分的 左上-右下对角线 else NW_SE[n+int(abs(x-y))]=true; ///左下角部分 左上-右下对角线 ///cout&lt;&lt;&quot;value changed back at&quot;&lt;&lt;x&lt;&lt;&quot; &quot;&lt;&lt;y&lt;&lt;endl; &#125; if(y&lt;n) dfs(x,y+1);&#125; 更可读的代码以及问题优化 优化了对角线的表达 优化了答案位置的记录 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;using namespace std;bool col[14],line[14],NE_SW[26],NW_SE[26]; int n,ans=0,solution[14]; /// col[x] line[y]/// NE_SW 右上向左下方向的对角线，从左上角(1,1)为第一条，第二条是(2,1)-(1,2)/// NW_SE 左上到右下方向的对角线，从右上角(1,n)为第一条，第二条是(n-1,1)-(n,2)void dfs(int,int);bool flag(int,int);int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) &#123;col[i]=true; line[i]=true; NE_SW[i]=true; NW_SE[i]=true; NE_SW[i+n]=true; NW_SE[i+n]=true;&#125; dfs(1,1); cout&lt;&lt;ans; return 0;&#125;bool flag(int x,int y)&#123; return col[x]&amp;&amp;line[y]&amp;&amp;NE_SW[x+y]&amp;&amp;NW_SE[n+y-x];&#125;void dfs(int x,int y)&#123; if(flag(x,y))&#123; solution[x]=y;///注意这个地方特别精髓，因为如果我们每个答案都开新的一行数组记录的话有可能会出现上面问题4说的情况，所以我们只需要每次覆盖记录就好了，反正x永远是按从上到下的顺序来的 //col[x]=false; line[y]=false; NE_SW[x+y]=false; NW_SE[n+y-x]=false; col[x]=!col[x]; line[y]=!line[y]; NE_SW[x+y]=!NE_SW[x+y]; NW_SE[n+y-x]=!NW_SE[n+y-x]; ///注意 !col[x] 只是表达 col[x] 取反的一个值，只有 col[x]=!col[x] 才能给原波尔值赋值为它的反 if(x==n)&#123;///最后一行也被填好了，我们得到一个解 ans++; if(ans&lt;=3)&#123; for(int i=1;i&lt;=n;i++) cout&lt;&lt;solution[i]&lt;&lt;&quot; &quot;; cout&lt;&lt;endl; &#125; &#125; else dfs(x+1,1);///还不到最后一行的话就接着去找下一行 ///回溯：拿走刚刚放下的棋子 //col[x]=true; line[y]=true; NE_SW[x+y]=true; NW_SE[n+y-x]=true; col[x]=!col[x]; line[y]=!line[y]; NE_SW[x+y]=!NE_SW[x+y]; NW_SE[n+y-x]=!NW_SE[n+y-x]; &#125; ///如果这个位置不符合条件(flag(x,y)==false)/这个位置符合条件的情况已经被全部枚举了(flag(x,y)==true) 那么我们就可以去找本行的下一个位置是否满足条件 if(y&lt;n) dfs(x,y+1);&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1031 均分纸牌","slug":"P1031 均分纸牌","date":"2019-06-04T04:00:00.000Z","updated":"2019-06-27T12:40:50.000Z","comments":true,"path":"P1031 均分纸牌/","permalink":"https://yao-lirong.github.io/blog/P1031%20%E5%9D%87%E5%88%86%E7%BA%B8%E7%89%8C/","excerpt":"题目来源：洛谷P1031 均分纸牌 标解 注意本题中平均数的运用 首先，一定要想到每堆排的张数减去平均张数，这样，题目就变成了移动正数，加到负数中，是大家都变成了0，这就意味着成功了60%！！！！（关键）。以例题来说，平均张数为10，原张数变为-1，-2,+7，-4，因为没有为0的数，所以从最左边出发，将-1移动到-2中，变为0，-3，+7,4，再讲-3向右移动……一次类推，直到全为0为止。没移动一次，步数便加1。关键是，负数怎么移动，其实，移动-x张牌，其实就是从另一堆中移动x张牌，步数相同。还有就是要过滤0，如排数为4，4,2,6，则减去平均数后为0,0，-2,2，就要从第三对开始移动。注意有些0是不能过滤的，如1,0,1，-2中的0。还有就是每次移动好都要过滤。如-2，2,1,3，-4，第一步后变为0,0,1,3，-4，可以省略第二堆的移动。","text":"题目来源：洛谷P1031 均分纸牌 标解 注意本题中平均数的运用 首先，一定要想到每堆排的张数减去平均张数，这样，题目就变成了移动正数，加到负数中，是大家都变成了0，这就意味着成功了60%！！！！（关键）。以例题来说，平均张数为10，原张数变为-1，-2,+7，-4，因为没有为0的数，所以从最左边出发，将-1移动到-2中，变为0，-3，+7,4，再讲-3向右移动……一次类推，直到全为0为止。没移动一次，步数便加1。关键是，负数怎么移动，其实，移动-x张牌，其实就是从另一堆中移动x张牌，步数相同。还有就是要过滤0，如排数为4，4,2,6，则减去平均数后为0,0，-2,2，就要从第三对开始移动。注意有些0是不能过滤的，如1,0,1，-2中的0。还有就是每次移动好都要过滤。如-2，2,1,3，-4，第一步后变为0,0,1,3，-4，可以省略第二堆的移动。 1234567891011#include &lt;iostream&gt; using namespace std; int main() &#123; int a,p=0,js=0; cin &gt;&gt;a;int q[a]; for (int y=0;y&lt;a;y++)&#123;cin &gt;&gt;q[y]; p+=q[y];&#125; p/=a; for (int y=0;y&lt;a;y++)q[y]-=p; for (int y=0;y&lt;a;y++) &#123;if (q[y]==0)continue; q[y+1]+=q[y]; js++; &#125; cout &lt;&lt;js; return 0;&#125; 我的解法 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;int main()&#123; int sum=0,card[103],num,mean,no_count=0; cin&gt;&gt;num; for(int i=1;i&lt;=num;i++) &#123;cin&gt;&gt;card[i]; sum+=card[i];&#125; mean=sum/num; int pointer=0,local_sum=0; for(int i=1;i&lt;=num;i++)&#123; local_sum+=card[i]; pointer++;///我的思路是记录有没有一个 local_max==local_sum，如果有 local_sum，即pointer所指的那一堆，之前的就全部排好了，不需要再操心了 if(pointer==1 &amp;&amp; card[i]==mean) &#123;pointer=1; no_count++;&#125; ///直接等于平均数的堆要拿出来特殊讨论，因为他们只有跟在已经排好序的堆后面的时候才不需要再经过一次移动，而前面的堆已经排好序的标志就是 pointer==1，这种情况下我们可以少移动一个，并且重新设置 pointer==1 代表前面的堆都有序 if(local_sum==pointer*mean &amp;&amp; pointer!=1)&#123; /// local_sum==pointer*mean 此时我们找到一个堆，可以使前面的所有堆获得符合要求的解，并且他只需要向别人输送牌，自己不需要接受，所以有一个 no_count++ /// pointer!=1 这是 local_sum 就是他本身，必然相等 no_count++; pointer=0; local_sum=0; &#125; &#125; cout&lt;&lt;num-no_count;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P2678 跳石头","slug":"P2678 跳石头","date":"2019-06-03T04:00:00.000Z","updated":"2019-06-27T12:27:22.000Z","comments":true,"path":"P2678 跳石头/","permalink":"https://yao-lirong.github.io/blog/P2678%20%E8%B7%B3%E7%9F%B3%E5%A4%B4/","excerpt":"题目来源：洛谷P2678 跳石头 这是一道标准的 “最大值最小”或“最小值最大“ 的题，遇到这种题，我们就可以使用 贪心+二分查找 的方法来做 二分答案/二分查找 有序（单调）的，有界的就可以用二分法查找。","text":"题目来源：洛谷P2678 跳石头 这是一道标准的 “最大值最小”或“最小值最大“ 的题，遇到这种题，我们就可以使用 贪心+二分查找 的方法来做 二分答案/二分查找 有序（单调）的，有界的就可以用二分法查找。 有界：对于本题，我们可以发现，这个所谓的最短跳跃距离显然不能超过一个范围（跳一次从头跳到尾）。也就是说，答案是有一个确定的范围限制的（开头到结尾的距离内），我们就可以考虑一种另外的方法去解决——枚举答案，并去验证答案是否可行，这实际上是一种倒推 二分：那么如何确保我们可以最快的找到答案呢？二分是最好选择 单调：二分的前提条件是什么？是答案区间是整体有序的。我们只考虑合法解，并称之为可行解。考虑所有可行解，我们肯定是要从这些可行解中找到一个最好的作为我们的答案， 这个答案我们称之为最优解。最优解一定可行，但可行解不一定最优。 我们假设整个序列具有单调性，且一个数x为可行解，那么一般的，所有的x’(x’&lt;x)都是可行解。 并且，如果有一个数y是非法解，那么一般的，所有的y’(y’&gt;y)都是非法解。 总结来说，可以使用二分查找的条件：解的上下界确定(l=0,r=L),可以写出判断条件(f(x)&lt;=m),解具有区间单调性(在某个值之前条件都成立，之后都不成立) 本题和 P2855 River Hopscotch 是同一道题 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int rocks[50003],ending,num,removed,result;void finding(int,int);int main()&#123; cin&gt;&gt;ending&gt;&gt;num&gt;&gt;removed; for(int i=1;i&lt;=num;i++) cin&gt;&gt;rocks[i]; rocks[0]=0; rocks[num+1]=ending; sort(rocks+1,rocks+num+1); finding(0,ending); cout&lt;&lt;result; return 0;&#125;void finding(int m, int n)&#123; int mid=(m+n)/2, removing=0; int now=0,pointer=0;///now 表示我们现在所在的位置，pointer 表示下一个可以跳到的位置 while(pointer&lt;num)&#123;///人家这个方法直接一步子迈过去了，根本不需要记录哪个石头被拿掉了，或者判定一个原本有石头的地方被没被拿掉，毕竟题目本身就叫跳石头，为什么要一个个石头看呢，直接跳不就好了 pointer++; if(rocks[pointer]-rocks[now]&lt;mid)///我们认为mid是最短跳跃距离，如果有某种情况使得跳跃距离比这个最短的还短，我们就需要拿走这块石头来增大这个地方的跳跃距离，使其大于最短跳跃距离 removing++; else//如果比最短距离长的话，我们就可以跳过去 now=pointer; &#125; if(m&lt;=n)&#123; ///这个地方我写 m&lt;n 或者 m&lt;=n 有区别吗？我的m到最后的时候只能通过mid+1这一种方式更新，+1又不影响/2以后mid的值，所以这两个判定不是一样的吗？ ///确实判定的时候没什么区别，最后都会更新到m=4 n=4 mid=4，但是 m&lt;n 运行到 m=4 n=4 mid=4 会发现 m!&lt;n 所以不会更新 result if(removing&gt;removed) finding(m,mid-1); ///如果我们以mid为最小距离的情况下移动的石块比我们本应该移动的石块多的话，说明这个答案是不合法的，并且所有大于mid的都不合法（越大于mid移动的石块只会越来越多），所以减少 最小移动距离 使得我们不要移动那么多石块 else if(removing&lt;=removed) &#123; result=mid; finding(mid+1,n);&#125; ///如果以mid为最小距离的情况下移动的石块比我们本应该移动的石块多的话，说明这个答案合法，但是因为我们要寻找最大的最小值，所以增大 最小距离 看看有没有更优的解 &#125;&#125;/*25 5 2 2111417 21*/ 二分查找模板 非递归形式的二分查找模板 123456789101112int l=1,r=ll;/// 1 是答案的最小值，ll是答案的最大值 while(l&lt;=r) &#123; ///当左右边界重合的时候就是答案，退出循环 int mid=(l+r)&gt;&gt;1,q=check(mid);//“&gt;&gt;1”相当于“/2” if(check) ///当该距离满足条件的时候 &#123; ///去寻找右半部分，看看还有没有符合条件的更大的值 ll=mid+1;///ll上mid右边，找右半部分 ans=mid;///记录答案（更新中） &#125; else l=mid+1;///若这个值不满足，就找左部分&#125; 下面是一个二分查找的样例 P1824 Aggressive Cows 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int stall[100005],cow_num,stall_num,ans;void finding(int,int);int main()&#123; cin&gt;&gt;stall_num&gt;&gt;cow_num; for(int i=1;i&lt;=stall_num;i++) cin&gt;&gt;stall[i]; sort(stall+1,stall+stall_num+1); finding(1,stall[stall_num]); cout&lt;&lt;ans; return 0;&#125;void finding(int m, int n)//看到一个符合要求的就填进去，最后看填进去的cow和一共有的是多是少&#123; int mid=(m+n)/2,now=1,pointer=1,cow_mid=1;///cow_mid 从1开始，如果从0开始实际上算的是间距，n+1才是牛的数量 while(pointer&lt;stall_num)&#123; pointer++; if(stall[pointer]-stall[now]&gt;=mid)&#123;///这里注意是 &gt;= 只要比最短的距离(mid)大，我们就可以放一头奶牛在这里 cow_mid++; now=pointer;&#125; &#125; if(m&lt;=n)&#123; if(cow_mid&gt;=cow_num) &#123;ans=mid; finding(mid+1,n);&#125; ///如果这次放的比我们需要放的多，说明我们的最短间距太小了，所以要增大最短间距 else finding(m,mid-1); &#125;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1090 合并果子","slug":"P1090 合并果子","date":"2019-05-30T04:00:00.000Z","updated":"2019-06-27T12:32:00.000Z","comments":true,"path":"P1090 合并果子/","permalink":"https://yao-lirong.github.io/blog/P1090%20%E5%90%88%E5%B9%B6%E6%9E%9C%E5%AD%90/","excerpt":"题目来源： 洛谷P1090 合并果子 一维数组做法 本题是一个简单的 Huffman树。Huffman编码 在 UTF-8 &amp; Unicode 中都有它思想的体现，即出现频率高的编码长度短，出现频率低的编码长度长，用以缩短整体编码长度 这里我也运用了前面 P1309 的思想：因为每次需要重新排序的时候只有一个数据需要被插入整个数列当中,所以并不需要假定数据无序的 quick sort，反而是线性的排序更快","text":"题目来源： 洛谷P1090 合并果子 一维数组做法 本题是一个简单的 Huffman树。Huffman编码 在 UTF-8 &amp; Unicode 中都有它思想的体现，即出现频率高的编码长度短，出现频率低的编码长度长，用以缩短整体编码长度 这里我也运用了前面 P1309 的思想：因为每次需要重新排序的时候只有一个数据需要被插入整个数列当中,所以并不需要假定数据无序的 quick sort，反而是线性的排序更快 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int main()&#123; int n,berry[10005];long long ans=0; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++) cin&gt;&gt;berry[i]; sort(berry+1,berry+1+n); for(int i=1;i&lt;=n-1;i++)&#123; berry[i+1]=berry[i]+berry[i+1];///计算每个果堆的重量 ans+=berry[i+1];///答案是每次搬的果堆的重量之和 if(berry[i+1]&gt;berry[i+2])&#123;///解决前两个数之和大于第三/四个数的情况（比如有 1 1 1 1）最优解为4而不是7 for(int j=i+1;j&lt;=n-1;j++)&#123; if(berry[j]&gt;berry[j+1]) swap(berry[j],berry[j+1]);///线性排序 &#125; &#125; &#125; cout&lt;&lt;ans; return 0;&#125; 归并做法 据说是离散化算法 就是先把原本的从小到大排序排好。然后用两个队列，一个是存储原本的，另一个是存储合成的（由于原本的是从小到大所有新开的也是从小到大）。然后在两个队列的头取最小的，执行两次然后把这两个合并加入第二个队列中。 然后由于输入： (1≤ai≤20000)(1≤ai≤20000)(1≤ai≤20000) ，所以用桶排序就可以 O(n)O(n)O(n) 时间复杂度 要义是储存原本果堆的a1是按顺序排列的，所以存储两两合成的新果堆的a2也是按顺序排列的。取这两个果堆序列中最小的两个果堆，必定获得这一步能获得的最小的果堆。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;using namespace std;int k,x,num,n1,n2,a1[30001],a2[30001],t[20001],w,sum;int main()&#123; scanf(&quot;%d&quot;,&amp;num); memset(a1,127/3,sizeof(a1)); memset(a2,127/3,sizeof(a2)); for (int i=1;i&lt;=num;i++) &#123; scanf(&quot;%d&quot;,&amp;x); t[x]++;//桶 &#125; for (int i=1;i&lt;=20000;i++) &#123; while (t[i])//通排序 &#123; t[i]--; a1[++n1]=i; &#125; &#125; int i=1,j=1; k=1; while (k&lt;num) &#123; if (a1[i]&lt;a2[j])//取最小值 &#123; w=a1[i]; i++; &#125; else &#123; w=a2[j]; j++; &#125; if (a1[i]&lt;a2[j])//取第二次 &#123; w+=a1[i]; i++; &#125; else &#123; w+=a2[j]; j++; &#125; a2[++n2]=w;//加入第二个队列 k++;//计算合并次数 sum+=w;//计算价值 &#125; printf(&quot;%d&quot;,sum);&#125; 二叉（小根）堆 s 代表 son, p 代表 parent, size 代表整个二叉堆中存储的数据数量 完美二叉树, 完全二叉树和完满二叉树的区分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn=10000+10;int n,heap[maxn],size=0;void up(int p) //二叉小根堆向上调整（子节点小于父节点就调整）&#123; while(p&gt;1) &#123; if(heap[p]&lt;heap[p/2]) &#123; swap(heap[p],heap[p/2]); p/=2; &#125; else break; &#125;&#125;void insert(int val) //二叉堆插入，新元素放在堆底，向上调整&#123; heap[++size]=val; up(size);&#125;void down(int p) //二叉小根堆向下调整&#123; int s=p*2; while(s&lt;=size) &#123; //下面这句话是从左右儿子中选一个更小的做交换 if(s&lt;size&amp;&amp;heap[s+1]&lt;heap[s]) s++; if(heap[s]&lt;heap[p]) &#123; swap(heap[s],heap[p]); p=s; s=p*2; &#125; else break; &#125;&#125;void extract() //二叉堆删除堆顶&#123; heap[1]=heap[size--]; //将堆底移至堆顶，向下调整 down(1);&#125;int gettop() //返回堆顶的值&#123; return heap[1];&#125;int main()&#123; cin&gt;&gt;n; for(int i=1; i&lt;=n; i++) &#123; int a; cin&gt;&gt;a; insert(a); //建立二叉堆 &#125; long long ans=0; //其实这里不会越界，但好像原题数据是3万 while(size&gt;=2) //如果还可合并 &#123; int top1=gettop(); //取出堆顶（堆中最小值）后删除堆顶 extract(); int top2=gettop(); //同上 extract(); ans+=(top1+top2); insert(top1+top2); //将两数之和加入二叉堆，重复运算 &#125; cout&lt;&lt;ans&lt;&lt;endl; //输出答案 return 0;&#125;","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"P1309 瑞士轮","slug":"P1309 瑞士轮","date":"2019-05-30T04:00:00.000Z","updated":"2019-06-27T12:26:56.000Z","comments":true,"path":"P1309 瑞士轮/","permalink":"https://yao-lirong.github.io/blog/P1309%20%E7%91%9E%E5%A3%AB%E8%BD%AE/","excerpt":"题目来源：洛谷P1309 瑞士轮 胜者组和败者组分别是有序的，使用 mergesort 将两个有序同向数组进行归并（严格上来说不是归并排序），大大降低了时间复杂度 = O(n)。如果我们使用 quicksort，则默认整个数据是无序的，对每个数据都重新排序所以会超时","text":"题目来源：洛谷P1309 瑞士轮 胜者组和败者组分别是有序的，使用 mergesort 将两个有序同向数组进行归并（严格上来说不是归并排序），大大降低了时间复杂度 = O(n)。如果我们使用 quicksort，则默认整个数据是无序的，对每个数据都重新排序所以会超时 一种可以替代结构体的方法：排名的时候我们可以只对每个选手的序号进行排序，这样做既可以保证我们有各个人的排序，又可以保证他们的成绩和实力得到记录（序号对应着成绩和实力，在对序号根据实力排序的同时，每个序号对应的成绩和实力的顺序是不变的） sort 函数中 cmp 的使用方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int num,round,inquiry,score[200005],power[200005],No[200005],winner[100005],loser[100005];///其中，No里每个下标代表选手的排名，power &amp; score 的下标代表选手的序号bool cmp(int, int);void mergesort();void compete();int main()&#123; cin&gt;&gt;num&gt;&gt;round&gt;&gt;inquiry; for(int i=1;i&lt;=2*num;i++) cin&gt;&gt;score[i]; for(int i=1;i&lt;=2*num;i++) cin&gt;&gt;power[i]; for(int i=1;i&lt;=2*num;i++) No[i]=i; //for(int i=1;i&lt;=2*num;i++) cout&lt;&lt;No[i]&lt;&lt;&quot; &quot;&lt;&lt;score[i]&lt;&lt;endl; sort(No+1,No+1+2*num,cmp); //for(int i=1;i&lt;=2*num;i++) cout&lt;&lt;No[i]&lt;&lt;&quot; &quot;&lt;&lt;score[i]&lt;&lt;endl; for(int i=1;i&lt;=round;i++) &#123; compete(); mergesort(); &#125; cout&lt;&lt;No[inquiry]; //while(true); return 0;&#125;bool cmp(int m, int n)&#123; ///cmp函数使用范例 if(score[m]==score[n]) return m&lt;n; else return score[m]&gt;score[n];&#125;void compete()&#123; for(int i=1;i&lt;=2*num;i=i+2) &#123; if(power[No[i]]&gt;power[No[i+1]]) &#123; score[No[i]]++; winner[i/2+1]=No[i]; loser[i/2+1]=No[i+1]; &#125; else if(power[No[i]]&lt;power[No[i+1]]) &#123; score[No[i+1]]++; winner[i/2+1]=No[i+1]; loser[i/2+1]=No[i]; &#125; &#125; /*cout&lt;&lt;&quot;compete&quot;&lt;&lt;endl; for(int i=1;i&lt;=num;i++) cout&lt;&lt;winner[i]&lt;&lt;&quot; &quot;&lt;&lt;loser[i]&lt;&lt;endl; cout&lt;&lt;endl;*/&#125;void mergesort()&#123; int i=1,j=1; while(i&lt;=num &amp;&amp; j&lt;=num) &#123; //if(score[winner[i]]&gt;score[loser[j]]) if(cmp(winner[i],loser[j])) ///完全无法理解这个地方只写一个cmp是怎么过的，难道不会有位于后面的相等score项实际比前面的相等score项序号更小这种情况吗 &#123; No[i+j-1]=winner[i]; i++; &#125; //else if(score[loser[j]]&gt;score[winner[i]]) else &#123; No[i+j-1]=loser[j]; j++; &#125; /*else if(score[loser[j]]==score[winner[i]]) &#123; int k=0,temp[200005]; while(score[winner[i]]==score[winner[i+1]]) temp[++k]=winner[i++]; while(score[loser[j]]==score[loser[j]]) temp[++k]=loser[j++]; sort(temp,temp+k); for(int a=1;a&lt;=k;a++) No[a+i+j-1]=temp[a]; &#125;*/ &#125; while(i&lt;=num) &#123; No[i+num]=winner[i]; i++; &#125; while(j&lt;=num) &#123; No[j+num]=loser[j]; j++; &#125; /*cout&lt;&lt;&quot;mergesort&quot;; for(int i=1;i&lt;=num*2;i++) cout&lt;&lt;No[i]&lt;&lt;&quot; &quot;; cout&lt;&lt;endl;*/&#125;/*2 4 17 6 6 710 5 20 15*/","categories":[],"tags":[{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]},{"title":"Introduction to Git Command","slug":"Intro-to-Git-Command","date":"2019-02-10T05:00:00.000Z","updated":"2022-11-02T21:13:14.000Z","comments":true,"path":"Intro-to-Git-Command/","permalink":"https://yao-lirong.github.io/blog/Intro-to-Git-Command/","excerpt":"Creating repository git init: create a repository git add File_Name: add “File_Name” to repository git add . : add all files git commit -m \"message\": commit changes and tell others what changes have been made git commit -m \"Title\" -m \"Description ..\": commit with a short title then long description","text":"Creating repository git init: create a repository git add File_Name: add “File_Name” to repository git add . : add all files git commit -m \"message\": commit changes and tell others what changes have been made git commit -m \"Title\" -m \"Description ..\": commit with a short title then long description Way-back Machine git status: tell you which files have been changed git diff: check what content exactly has been changed in each file Time Travelling commitID: git uses commit ID, a hex number calculated by SHA1 to record your commit history HEAD: HEAD is the current version, HEAD^ is the previous, HEAD^^ is the one before the previous, HEAD~100 is the last 100. git log: check the commit history git reflog: check the command history git reset --hard CommitID : Going back to the “Commit ID” version (e.g. git reset --hard HEAD^ : going back to the previous version) When you go back, the “future version” will no longer appear in “git log”. However, you can use “git reflog” to trace “commit ID” from the future Working Directory and Repository Undo Changes messed up with working directory: use git checkout -- File_Name to discard changes in working directory and make “File_Name” to go back to the latest “committed” or “added” version messed up with working directory but don’t want to delete all the changes: git stash saves your local modifications away and reverts the working directory to match the HEAD commit. messed up with working directory and added it to stage: use git reset HEAD File_Name to discard changes in stage but keep “File_Name” in working directory changed, therefore going back to situation 1 committed the mess to master branch: use the Time Traveling technique in the previous section Deleting Files If you want to delete files that are already committed to the master branch: delete the file in working directory: rm File_Name delete the file from git / restore the file delete the file from git: git rm File_Name &amp; git commit restore the file: git checkout -- File_Name Remote Repository Change Git Remote URL 12git remote set-url &lt;remote_name&gt; &lt;remote_url&gt; # remote name is usually &quot;origin&quot;git remote -v # verbose print info to check changed successfully Change Remote Branch a Local Branch is Tracking 1234git branch &lt;local_branch_name&gt; --set-upstream-to &lt;remote_name&gt;/&lt;remote_branch_name&gt;git branch yao --set-upstream-to=origin/yao# or you can ignore local branch name if you&#x27;re currently on that branchgit branch --set-upstream-to=origin/yao Push to Different Remote Branch 12git push &lt;remote_name&gt; &lt;branch_name&gt;git push origin master Managing Branch Creating and Deleting Branch 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; Configuration show configuration: git config -l --local git config -l --global modify configuration: For this current repo: 12git config user.name &quot;Your Name Here&quot;git config user.email your@email.com For global settings: 12git config --global user.name &quot;Your Name Here&quot;git config --global user.email your@email.com Quick look up a specific attribute: git config user.name If you are using HTTPS remote, it will ask for your username and password each time you push/pull from the remote. You can then cache your credentials with the following command for 7200 seconds (2 hour). If ignore the part in the quote, the credentials will be saved forever. 1git config --global credential.helper &#x27;cache --timeout 7200&#x27; Others Keep a local copy of files, don’t update with server change: git update-index --skip-worktree &lt;path-name&gt; from Stackoverflow When use GitHub, use https to track remote branch. SSH works weird when you have multiple accounts on your local machine. Revert an accidental pull request: the following are 2 equivalent ways 12git reset --mergegit merge --abort","categories":[],"tags":[{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-09T05:00:00.000Z","updated":"2022-06-08T19:43:56.000Z","comments":true,"path":"hello-world/","permalink":"https://yao-lirong.github.io/blog/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment Real Start 实际上你应该使用 npx hexo &lt;command&gt; 来运行 hexo.","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yao-lirong.github.io/blog/tags/ML/"},{"name":"Journal","slug":"Journal","permalink":"https://yao-lirong.github.io/blog/tags/Journal/"},{"name":"Manual","slug":"Manual","permalink":"https://yao-lirong.github.io/blog/tags/Manual/"},{"name":"Logistics","slug":"Logistics","permalink":"https://yao-lirong.github.io/blog/tags/Logistics/"},{"name":"Review","slug":"Review","permalink":"https://yao-lirong.github.io/blog/tags/Review/"},{"name":"Book","slug":"Book","permalink":"https://yao-lirong.github.io/blog/tags/Book/"},{"name":"Tsinghua","slug":"Tsinghua","permalink":"https://yao-lirong.github.io/blog/tags/Tsinghua/"},{"name":"Cornell","slug":"Cornell","permalink":"https://yao-lirong.github.io/blog/tags/Cornell/"},{"name":"Vim","slug":"Vim","permalink":"https://yao-lirong.github.io/blog/tags/Vim/"},{"name":"CS3110","slug":"CS3110","permalink":"https://yao-lirong.github.io/blog/tags/CS3110/"},{"name":"NOI","slug":"NOI","permalink":"https://yao-lirong.github.io/blog/tags/NOI/"}]}
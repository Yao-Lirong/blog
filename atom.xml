<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yao Lirong&#39;s Blog</title>
  
  
  <link href="https://yao-lirong.github.io/atom.xml" rel="self"/>
  
  <link href="https://yao-lirong.github.io/"/>
  <updated>2025-09-02T23:49:42.150Z</updated>
  <id>https://yao-lirong.github.io/</id>
  
  <author>
    <name>Yao Lirong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search</title>
    <link href="https://yao-lirong.github.io/2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/"/>
    <id>https://yao-lirong.github.io/2024-12-25-Matryoshka-Representation-Learning,-Adaptive-Retrieval-and-Binary-Vector-Search/</id>
    <published>2024-12-25T05:00:00.000Z</published>
    <updated>2025-09-02T23:49:42.150Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Intro to Matryoshka Representation Learning</strong></p><p>In Matryoshka Representation Learning (MRL), we want to construct anencoding <spanclass="math inline"><em>e</em><sub><em>d</em></sub></span> withdimension <span class="math inline"><em>d</em></span> such that itstruncations of different lengths (<spanclass="math inline"><em>e</em><sub><em>d</em>/16</sub>, <em>e</em><sub><em>d</em>/8</sub>, <em>e</em><sub><em>d</em>/4</sub>, <em>e</em><sub><em>d</em>/2</sub></span>​)are each (somewhat) valid representations. Suppose you’re training on aclassification problem with the classic encoder + classifier headarchitecture. At train time:</p><ul><li>classic setting: you just use the vector <spanclass="math inline"><em>e</em><sub><em>d</em></sub></span> as input tothe classifier head</li><li>MRL: construct multiple classifier heads (in our case 5) and put oneon top of encoding of each length (<spanclass="math inline"><em>e</em><sub><em>d</em>/16</sub>, …, <em>e</em><sub><em>d</em></sub></span>)and average the loss of each classifier head. So we build heads of size<code>[d, num_class], [d/2, num_class], ... [d/16, num_class]</code>Note these classifier heads share weights.</li></ul><p><strong>Application: Adaptive Retrieval</strong></p><p>Online retrieval is one of the tasks where latency matters the most.Given a user query <span class="math inline"><em>q</em></span>, it isslow to compute KNN from a dataset of size 1M (<spanclass="math inline">10<sup>6</sup></span>) indexes if each index hasdimension 3072. With MRL, we can decompose the process into twostages:</p><ol type="1"><li>Shortlist: First retrieve 2K indexes where the distance is computedusing only 1024-d vector (the first 1024 elements of the 3072vector)</li><li>Rerank: Find KNN among these 2K indexes where the distance iscomputed using the full length 3072 vector</li></ol><p>The FLOP is therefore reduced from <spanclass="math inline">3072 × 10<sup>6</sup></span> to <spanclass="math inline">1024 × 10<sup>6</sup> + 3072 × 2<em>K</em></span>.Ce Gao tested full length 3072-dim vector vs adaptive retrieval usingMatryoshka 1024-dim. The accuracy dropped from 99% to 89% with RequestsPer Second (RPS) raises from 300 to 1000.</p><p>Find more details of Matryoshka Representation Learning and itsapplications in this wonderful <ahref="https://aniketrege.github.io/blog/2024/mrl/#what-is-mrl-really-this-time">blogpost. Read from section <em>What is MRL? (Really this Time)</em></a></p><p><strong>Binary Vector Search</strong></p><p><ahref="https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors">CeGao suggested</a> another way to reduce memory and FLOP use. He proposesto turn the length <span class="math inline"><em>d</em></span> FP32vector into a length <span class="math inline"><em>d</em></span> binaryvector, where original positive value is set to 1 and original negativevalue is set to 0.</p><p>Without using adaptive retrieval, the accuracy dropped from 99% to83%, but the latency (RPS = 3000) and memory has a significantimprovement because previously one single vector / encoding consists of<span class="math inline"><em>d</em></span> 32-bit number, whereas nowit only consists of <span class="math inline"><em>d</em></span> 1-bitnumber.</p><p>If you adapt the Adaptive Retrieval setup mentioned earlier:</p><ol type="1"><li>Shortlist: retrieve 2K indexes using full-length but binaryvector</li><li>Rerank: find KNN among 2K indexes using full-length, FP32vector</li></ol><p>you get a precision drop from 99% only to 96% with an RPS of1700.</p><p>P.S. I discovered this method on <ahref="https://simonwillison.net/2024/Mar/26/binary-vector-search/">SimonWillison’s blog</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Intro to Matryoshka Representation Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In Matryoshka Representation Learning (MRL), we want to construct an
</summary>
      
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>YouTube Recommendation Algorithms (2016)</title>
    <link href="https://yao-lirong.github.io/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/"/>
    <id>https://yao-lirong.github.io/2024-10-15-YouTube-Recommendation-Algorithms-(2016)/</id>
    <published>2024-10-15T04:00:00.000Z</published>
    <updated>2025-09-02T23:58:14.676Z</updated>
    
    <content type="html"><![CDATA[<p>This is a detailed reading of Google’s paper <ahref="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">DeepNeural Networks for YouTube Recommendations</a></p><span id="more"></span><h2 id="candidate-generation">Candidate Generation</h2><h3 id="problem-setup">Problem Setup</h3><p>We pose recommendation as an extreme multi-class classificationproblem where we <strong>predict which video will be watchednext</strong>. Specifically, we classify a specific video watch <spanclass="math inline"><em>w</em><sub><em>t</em></sub></span> at time <spanclass="math inline"><em>t</em></span> among millions of videos <spanclass="math inline"><em>i</em></span> (classes) from a video corpus<span class="math inline"><em>V</em></span> based on a user <spanclass="math inline"><em>U</em></span> and context <spanclass="math inline"><em>C</em></span>. And <span class="math inline">$u\in \R^d$</span> represents a high-dimensional “embedding” of theuser-context pair and the <span class="math inline">$v_j \in\R^d$</span> represent embeddings of each candidate video. <spanclass="math display"><em>P</em>(<em>w</em><sub><em>t</em></sub> = <em>i</em> ∣ <em>U</em>, <em>C</em>) = <code>Softmax</code>(<em>V</em>, <em>u</em>)<sub><em>i</em></sub></span></p><h3 id="input">Input</h3><p>what we didn’t use: explicit feedback (thumbs up/down, in-productsurveys) because there’s too few of them in the tail of videos.</p><ul><li><p><strong>embedded video watches</strong>:</p><ul><li><p><strong>representation of each video</strong>: in myunderstanding, YouTube didn’t extract information from their videos, andfed the extracted info into the embedder. Instead, they directly fed thevideo ids into the embedder. To my understanding, “Inspired bycontinuous bag of words language models” means they fed the video idsinto the embedder, just like NLP feeds BoW representation into embedder.It doesn’t mean YouTube decomposes a video into word count like BoW.</p><p>I reached this conclusion from the last sentence of 4.1 FeatureRepresentation - Embedding Categorical Features, where<code>1000000*32 / (2048*2048) = 7</code></p><blockquote><p>The overwhelming majority of model parameters are in thesehigh-cardinality embedding spaces - for example, one million IDsembedded in a 32 dimensional space have 7 times more parameters thanfully connected layers 2048 units wide.</p></blockquote></li><li><p><strong>representation of watch history</strong>: watch historyis a variable-length list of videos. YouTube used average-pooling totransform them into a fixed-length vector.</p></li><li><p><strong>why order-independent pooling?</strong></p></li></ul></li><li><p><strong>embedded search tokens</strong>: each query is tokenizedinto unigrams and bigrams. each token is embedded. All these embeddedtokens from all queries are then pooled and fed into the model as asummarized dense search history.</p></li><li><p><strong>geographic embeddings</strong>: The user’s geographicregion and device are embedded and concatenated.</p></li><li><p><strong>Simple binary and continuous features</strong>: such asthe user’s gender, logged-in state and age are input directly into thenetwork as real values normalized to [0, 1].</p></li><li><p><strong>example (sample) age</strong>: ML model often has biastowards past samples because there’s more data about them. It’s commonto promote video freshness during re-rank phase, but YouTube also try toreduce this bias as early as in candidate generation phase. We definethe example age to be the time between training and obtaining thissample. e.g. <code>t</code> days earlier, after having watched video<code>v</code>, user searched word <code>w</code> and clicked on anothervideo <code>y</code>. We use this sample in training, so its sample ageis <code>t</code>. By introducing this feature, our model no longerreflects the average watch likelihood in the training window of severalweeks, but the likelihood at a specific time step. At serving time, thisfeature is set to zero (or slightly negative) to reflect that the modelis making predictions at the very end of the training window.</p><p>总结某<ahref="https://zhuanlan.zhihu.com/p/52504407">知乎讨论下的内容</a>:example age 和消除第一种 ad position bias 做法类似</p><ul><li><p>纯feed应用中前置内容点击率被高估：新闻客户端，position靠前的是虚高的，这部分叫positionbias，输入给模型的时候也要输入它们的位置，在线上预估时置0</p></li><li><p>搜索偏向应用中前置内容点击率被低估：手百，手淘，美团等，都会在首页默认展示feed，但很多目的明确的用户压根不会用这些推荐功能，导致这部分展示的内容点击率是被低估了。实际操作中大家可能只针对有feed互动的用户进行采样，抛弃了完全过路型用户的行为，也算是修正bias了</p></li></ul></li></ul><h3 id="data-gathering-details">Data Gathering Details</h3><ul><li><p><strong>Class Balance</strong>: generate a fixed number oftraining examples per user, effectively weighting our users equally inthe loss function</p></li><li><p><strong>Permutation-Invariant Pooling</strong>: in pooling,YouTube chose average pooling among sum, max, etc, because averageperformed the best. The important thing is, they decided to abandonsequence information whatsoever. Their explanation is below and I don’tquite buy it because they’re definitely better way to solve this problemthan discarding the info altogether. In addition, they did publish <ahref="https://research.google/pubs/latent-cross-making-use-of-context-in-recurrent-recommender-systems/">anotherpaper on sequence-based recommendation system</a> later. I think at thispaper’s publishing time, they either didn’t want to publicize it or havenot tested that in detail.</p><blockquote><p>Consider the user has just issued a search query for “taylor swift”.Since our problem is posed as predicting the next watched video, aclassifier given this information will predict that the most likelyvideos to be watched are those which appear on the corresponding searchresults page for “taylor swift”. Unsurprisingly, reproducing the user’slast search page as homepage recommendations performs very poorly. Bydiscarding sequence information and representing search queries with anunordered bag of tokens, the classifier is no longer directly aware ofthe origin of the label.</p></blockquote></li><li><p><strong>Next-Watch Heldout</strong>: at the time, manycollaborative filtering systems implicitly choose the labels and contextby holding out a random item and predicting it from other items in theuser’s history. They decided to always hold out and predict user’s nextwatch and achieved much better results. This is now already thestandard. In fact it appeared in my college class CS4780 byKillian.</p></li></ul><h3 id="training">Training:</h3><p>In these experiments, a vocabulary of 1M videos and 1M search tokenswere embedded with 256 floats each in a maximum bag size of 50 recentwatches and 50 recent searches. The softmax layer outputs a multinomialdistribution over the same 1M video classes with a dimension of 256(which can be thought of as a separate output video embedding).</p><ul><li><p><strong>negative sampling</strong>: for the same user, inaddition to his watched videos, we also sample some unwatched videos togeneralize the model.</p></li><li><p><strong>Importance Weighting</strong>: we do not take softmaxover all the 1M videos. Instead, we sample ~5000 of them, compute theirprobability, re-weight them based on importance (<strong>watch time,click rate?</strong>) and only compute loss over these samples.</p></li><li><p><strong>loss</strong>: this is a multi-class classificationproblem, so we use cross-entropy loss naturally</p></li></ul><h3 id="inference-serving">Inference / Serving:</h3><ul><li><p><strong>kNN</strong>: In serving, we need an algorithm sublinearto number of classes (videos to recommend). Say the last layer of thenetwork has hidden dimension <span class="math inline"><em>d</em></span>and we have <span class="math inline"><em>N</em></span> videos topredict. Decoding hidden dimension back into per-video logits and take asoftmax takes <spanclass="math inline"><em>O</em>(<em>d</em><em>N</em>)</span>. Sortingtakes <spanclass="math inline"><em>O</em>(<em>N</em>log <em>N</em>)</span>. Thetotal time is <spanclass="math inline"><em>O</em>(<em>d</em><em>N</em> + <em>N</em>log <em>N</em>)</span>.On the other hand, naive kNN takes <spanclass="math inline"><em>O</em>(<em>d</em><em>N</em>)</span> time intotal and some heuristic version like Ball tree can take <spanclass="math inline"><em>O</em>(<em>d</em>log <em>N</em>)</span>​​.</p><p>distance is based on dot product</p></li><li><p><strong>how do we get video embedding? (where isdecoder?)</strong></p><figure><img src="./images/youtube-candidate-generation-architecture.png"alt="youtube-candidate-generation-architecture" /><figcaptionaria-hidden="true">youtube-candidate-generation-architecture</figcaption></figure><p>All classification models have to include a decoder (FC layer) at theend of the network but before the softmax layer to decode the hiddenvector back to per-video logits in video ID space to make prediction. Ifwe have 1M videos and hidden vector is of dimension 256, the decoder isa matrix of size [256, 1M]. However, the graph presented in the paper isvery confusing because the authors omit drawing the decoder and made itan implicit part of the softmax layer.</p><p>Anyway, we know we do have that decoder, so it’s natural to use thevectors in the decoder as our video embedding. The i-th video’sembedding is simply <code>decoder[:, i]</code>.</p></li><li><p><strong>weight sharing / weight tying</strong>: this is a conceptI encountered in nanoGPT and has become clear here. At the beginning ofthe network, we have a video encoder from video ID space to hiddenspace; at the end of the network, we have a video decoder from hiddenspace back to video ID space. It is possible to share weights (use thesame weights) in encoder and decoder to save space (recall this partcosts the most parameter). This is just mentioned by people in <ahref="https://zhuanlan.zhihu.com/p/52504407">comment section</a> and isnot implemented by Google.</p></li></ul><h2 id="ranking">Ranking</h2><h3 id="problem-setup-1">Problem Setup</h3><p>We pose ranking as <strong>predicting the expected watch time of avideo</strong>. Ranking by click-through rate often promotes deceptivevideos that the user does not complete (“clickbait”) whereas watch timebetter captures engagement</p><h3 id="input-feature-engineering">Input (Feature Engineering)</h3><ul><li>user’s previous interaction with the item itself and other similaritems: e.g. user’s past history with the channel that uploaded the videobeing scored (how many videos has the user watched from this channel?When was the last time the user watched a video on this topic?)</li><li>propagate information from candidate generation into ranking in theform of features: e.g. which sources nominated this video candidate?What scores did they assign?</li><li>frequency of past video impressions: If a user was recentlyrecommended a video but did not watch it then the model will naturallydemote this impression on the next page load.</li></ul><h3 id="input-details">Input Details</h3><ul><li>embedding space should increase approximately proportional to the<strong>logarithm</strong> of the number of unique values of dataspace</li><li>very large space (video id &amp; query token) is <strong>truncatedby click-through-rate</strong>. So we only recommend videos above acertain CTR. Note these filtered out videos can still be searchedout.</li><li>Out-of-vocabulary values (new / truncated videos) are mapped to the<strong>zero embedding</strong>.</li><li>Continuous features are always <strong>normalized</strong> to [0,1)</li><li>In addition to the raw normalized feature <spanclass="math inline"><em>x</em></span>, we also input <spanclass="math inline">$\sqrt x$</span> and <spanclass="math inline"><em>x</em><sup>2</sup></span>​, giving the networkmore expressive power by allowing it to easily form <strong>super- andsub-linear</strong> functions of the feature. Feeding powers ofcontinuous features was found to improve offline accuracy.</li></ul><h3 id="training-1">Training</h3><p>We use Logistic Regression to predict expected watch time (EWT) of avideo, but LR only predicts 0 or 1. How can we use it to predict EWT? Weuse weighted logistic regression, where the positive (clicked)impressions are weighted by the observed watch time on the video.Negative (unclicked) impressions all receive unit weight.</p><p>In training, we use the weighted cross entropy loss: <spanclass="math display"><code>WeightedCrossEntropy</code> = −∑<sub><em>i</em></sub>[<em>T</em><sub><em>i</em></sub><em>y</em><sub><em>i</em></sub>log <em>p</em><sub><em>i</em></sub> + (1 − <em>y</em><sub><em>i</em></sub>)log (1 − <em>p</em><sub><em>i</em></sub>)]</span>### Serving</p><p>In serving, we directly output <spanclass="math inline"><em>e</em><sup><em>θ</em><em>x</em></sup></span> asthe predicted watch time. Why is this the watch time? Recall in LogisticRegression, we have a binary classification problem, so we can define<spanclass="math inline"><em>P</em>(<em>Y</em><sub><em>i</em></sub> = 1|<em>X</em><sub><em>i</em></sub>) = <em>p</em>, <em>P</em>(<em>Y</em><sub><em>i</em></sub> = 0|<em>X</em><sub><em>i</em></sub>) = 1 − <em>p</em></span>and <span class="math display">$$p = \frac{1}{1 + e^{-\theta^T x}}$$</span> In statistics, we define odds as: <spanclass="math display">$$\texttt{odds} = \frac{p}{1-p} = \frac{1}{e^{-\theta^Tx}} = e^{\theta^Tx}$$</span> If we take a log at both sides, we have the log odds, or<strong>logits</strong> <span class="math display">$$\ln(\texttt{odds}) = \ln(\frac{p}{1-p}) = \theta^Tx$$</span> Now let’s look at the our weighted logistic regressionproblem: Positive impressions are weighted by watch time <spanclass="math inline"><em>T</em><sub><em>i</em></sub></span>. Negativeimpressions receive unit weight. We have a total <spanclass="math inline"><em>N</em></span> videos, and <spanclass="math inline"><em>k</em></span> of them are positive (clicked). Wehave <span class="math inline"><em>k</em> = <em>p</em><em>N</em></span>and the expected watch time of all videos is <spanclass="math inline">$\mathbb E[T] = \frac{\sum_i^k T_i}{N}$</span>. Nowlook at our weighted odds: <span class="math display">$$\begin{align}\texttt{wghted odds} &amp;= \frac{\text{weighted posprob}}{\text{weighted neg prob}}\\&amp;= \frac{\sum_i^k T_i}{N-k} = \frac{\sum_i^k T_i}{N-pN} =\frac{\sum_i^k T_i}{N(1-p)} = \frac{\sum_i^k T_i}{N}\frac{1}{1-p}\\&amp; = \mathbb E[T](1+p) \approx \mathbb E[T] \hspace{20px}\text{($p$is small)}\end{align}$$</span> Therefore, at serving time, we can directly output <spanclass="math inline"><em>e</em><sup><em>θ</em><em>x</em></sup></span>because it is the expected watch time.</p><h3 id="evaluation-metric">Evaluation Metric</h3><p>Since we’re essentially predicting a video’s watch time, we“wrongly-predicted video’s watch time” as our evaluation metric. In thepaper, author called it “weighted, per-user loss”. Specifically, foreach user, we feed the model both positive (clicked) and negative(unclicked) impressions shown to him on a single page. We first predicttheir respective watch time with our model. “mispredicted watch time” isthe positive video’s watch time when the negative impression receives alonger predicted watch time than the positive impression. This user’sloss is total mispredicted watch time / total watch time of ground-truth(positive) impressions.</p><h2 id="takeaway">Takeaway</h2><p>example age</p><p>expected watch time</p><p>KNN - quick serving</p><p>feature engineering</p><p>what is <strong>surrogate problem</strong>?</p><p>https://www.youtube.com/watch?v=WK_Nr4tUtl8</p><h2 id="comment">Comment</h2><blockquote><p>RNN这个方法在17年已经由AlexBeutel做上线了，其实在16年初就想做，只是效果还没有完全出来，后来Alex发现了原先做法的一些弱点，很快改进之后就上线了，作为重要的candidatesgeneration来源；排序目标只写了EWT，一是Google的技术保密要求，毕竟还是要做到HR-safe的，论文只能点到即止，二是相对有效并且能够比分开预测ctr和staytime能节省servinglatenc</p><p>– <ahref="https://www.zhihu.com/people/dcb32aa26dd3a456bd79d2c2cdffa433">严林</a>under <ahref="https://zhuanlan.zhihu.com/p/52169807">重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is a detailed reading of Google’s paper &lt;a
href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf&quot;&gt;Deep
Neural Networks for YouTube Recommendations&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Running MobileBert on Android with TensorFlow Lite</title>
    <link href="https://yao-lirong.github.io/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/"/>
    <id>https://yao-lirong.github.io/2024-09-22-Running-MobileBert-on-Android-with-TensorFlow-Lite/</id>
    <published>2024-09-22T04:00:00.000Z</published>
    <updated>2025-09-02T23:58:14.680Z</updated>
    
    <content type="html"><![CDATA[<p>So Google, fxxk you.</p><span id="more"></span><h2 id="prerequsities">Prerequsities</h2><p>This picture very well explains how TFLite works and also whyTensorFlow 2 has both a <code>tf</code> and a <code>keras</code>.</p><figure><imgsrc="https://web.archive.org/web/20220216170621if_/https://www.tensorflow.org/lite/images/convert/workflow.svg"alt="TFLite Workflow" /><figcaption aria-hidden="true">TFLite Workflow</figcaption></figure><h2 id="detours">Detours</h2><p>This section is mostly rant, but it is meaningful in preventing youfrom taking any of the wrong path. Skip to the next section for atutorial on what to do.</p><ol type="1"><li><p>We first found the Google’s official release <ahref="https://github.com/google-research/google-research/tree/master/mobilebert">http://google-research/mobilebert/</a>,but</p><ul><li>the tutorial was unclear: Why do I need <code>data_dir</code> and<code>output_dir</code> to export TFLite? How do I even read in thepre-trained weights?</li><li>the code itself was pretty messy: why did they have export functionand training function all at this same file <code>run_squad.py</code>and the only way to tell the program whether to train/export is checkingwhether <code>export_dir is None</code> rather than passing a flag?</li></ul><p>In figuring out what each part does in this code, I looked upTensorFlow 1’s doc and good lord they were broken. Google doesn’t evenhost it anywhere: you have to go to <ahref="https://github.com/tensorflow/docs/tree/master/site/en/r1">aGitHub repo</a> to read them in <code>.md</code> format. At this momentI decided I will not touch anything written by TensorFlow 1’s API. (Iactually went through this pain back at my first ML intern in Haier, butnot again)</p></li><li><p>Sidenote before this: I didn’t know you can release model’s onKaggle (thought everyone releases on Hugging Face) and Google <ahref="https://www.kaggle.com/discussions/product-feedback/448425">movedtheir own TensorFlow Hub to Kaggle</a></p><p>So my supervisor found me <ahref="https://www.kaggle.com/models/google/mobilebert/tensorFlow1">amore readable Google release on Kaggle</a> with some high-level API anddoesn’t require you to read the painful source code. The above link has<a href="https://www.kaggle.com/models/tensorflow/mobilebert">a redirectto TensorFlow 2 implementation</a> with an official TFLite release. Howneat.</p><p>However, the official TFLite release</p><ol type="1"><li>doesn’t have <ahref="https://www.tensorflow.org/lite/guide/signatures">signature</a> -TensorFlow’s specification of input and output (remember when you passinputs to a model you need to give name to theme.g. <code>token_ids = ..., mask = ...</code>) which is required forXiaomi Service Framework to run a TFLite. P.S. Yes signature is notrequired to specify when exporting, but for god’s sake all your tutorialteaches people to use it and your own released ditched it? WTFGoogle.</li><li>is broken (as expected?). <ahref="forgot%20where%20the%20guide%20was">When I tried to run it on myPC</a>, I got the following error<code>indices_has_only_positive_elements was not true.gather index out of boundsNode number 2 (GATHER) failed to invoke.gather index out of boundsNode number 2 (GATHER) failed to invoke</code>.Someone encountered <ahref="https://github.com/tensorflow/tensorflow/issues/59730">a similarbug</a> while running the example code provided by TensorFlow and theGoogle SWE found a bug in their example. At this moment I decided not totrust this TFLite file anymore and just convert it on my own.</li></ol></li><li><p>So let’s use this official TensorFlow 2 implementation and <ahref="forgot%20where%20the%20guide%20was">convert it to TFLite</a>. Itwas all good and running on my PC, but</p><ol type="1"><li>Its output format was really weird<ul><li>It output consists of<code>'mobile_bert_encoder', 'mobile_bert_encoder_1', 'mobile_bert_encoder_2', ..., 'mobile_bert_encoder_51'</code></li><li>Each of these has shape <code>(1, 4, 128, 128)</code> for a<code>seq_length = 128, hidden_dim = 512</code> model. I figured 4 beingthe number of heads and the other 128 is <code>hidden_dim</code> foreach head.</li><li>They output attention scores, not the final encoded vector: my inputwas 5 tokens and they output is<code>output[0, 0, 0, :] = array([0.198, 0.138, 0.244, 0.148, 0.270, 0. , 0. , ...</code>.They sum to 1 and any other positions at <code>output</code> are 0 , soattention score was my best guess.</li></ul></li><li>It doesn’t run on Android phone:<code>tflite engine load failed due to java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 153. Are you using old TFLite binary with newer model?</code>A <ahref="https://stackoverflow.com/questions/67883156/tflite-runtime-op-builtin-code-out-of-range-131-are-you-using-old-tflite-bi">StackOverflow answer</a> suggests the TensorFlow used to export TFLiterunning on my PC doesn’t match the version of TFLite run time on thisAndroid phone. It can also be caused by me messing up with the wholeenvironment while installing <ahref="https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model">Optimum</a>to export TFLite last night, but I didn’t bother to look because Ifinally found the solution</li></ol></li><li><p>And comes the savior, the king, the go-to solution in MLOps -Huggingface. Reminded by a discussion I read by chance, I came torealize <code>TFMobileBertModel.from_pretrained</code> actually returnsthe Keras model (and the without <code>TF</code> version returns aPyTorch model). That means I can just use Hugging Face API to read itin, then use the native TensorFlow 2 API to export to TFLite. Andeverything works like a charm now. The final output signature is justHugging Face’s familiar<code>['last_hidden_state', 'pooler_output']</code></p></li></ol><h2 id="converting-tensorflow-model-to-tflite">Converting TensorFlowModel to TFLite</h2><p>Conversion is pretty straight forward. You can just follow thisofficial guide: <ahref="https://www.tensorflow.org/lite/models/convert/convert_models">ForMobile &amp; Edge: Convert TensorFlow models</a>. Though I actuallyfollowed my predecessor’s note (which actually comes from <ahref="https://www.tensorflow.org/lite/guide/signatures">another TFtutorial</a>). He also told me to caution that calling<code>tf.disable_eager_execution()</code> can lead to absence ofsignature, so do not call <code>tf.disable_eager_execution()</code> todisable eager mode.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertTokenizerFast, TFMobileBertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert Model</span></span><br><span class="line"><span class="keyword">if</span> be_sane:</span><br><span class="line">    bert_model = TFMobileBertModel.from_pretrained(kerasH5_model_path) <span class="keyword">if</span> keras_file <span class="keyword">else</span> \</span><br><span class="line">                 TFMobileBertModel.from_pretrained(pytorch_model_path, from_pt = <span class="literal">True</span>)</span><br><span class="line">    converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># be crazy or already knows the messy TensorFlow.SavedModel format</span></span><br><span class="line">    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)</span><br><span class="line">tflite_model = converter.convert()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save Model</span></span><br><span class="line">tflite_output_path = <span class="string">&#x27;/model.tflite&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(tflite_output_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  f.write(tflite_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Signature</span></span><br><span class="line"><span class="comment"># Empty signature means error in the export process and the file cannot be used by Xiaomi Service Framework</span></span><br><span class="line">interpreter = tf.lite.Interpreter(model_path=tflite_output_path)</span><br><span class="line">interpreter = tf.lite.Interpreter(model_content=tflite_model)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line">signatures = interpreter.get_signature_list()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tflite model signatures:&quot;</span>, signatures)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;serving_default&#x27;</span>: &#123;<span class="string">&#x27;inputs&#x27;</span>: [<span class="string">&#x27;attention_mask&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;input_ids&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;outputs&#x27;</span>: [<span class="string">&#x27;last_hidden_state&#x27;</span>, <span class="string">&#x27;pooler_output&#x27;</span>]&#125;&#125;</span><br></pre></td></tr></table></figure></blockquote><p>In addition, summarizing from the detours I took,</p><ul><li>Do not use Hugging Face’s Optimum for (at least vanilla) conversionbecause it just calls the above command (see <ahref="https://github.com/huggingface/optimum/blob/e0f58121140ce4baa01919ad70a6c13e936f7605/optimum/exporters/tflite/convert.py#L363-L371">code</a>)</li><li>Do not even bother to look at <ahref="https://github.com/google-research/google-research/tree/master/mobilebert#export-mobilebert-to-tf-lite-format">Google’soriginal code</a> converting MobileBert to TFLite because nobody knowswhat they’re writing.</li></ul><h2 id="running-tflite-on-pc">Running TFLite (on PC)</h2><p>Running TFLite on Android phone is the other department’s task. Ijust want to run the TFLite file on PC to test everything’s good. To dothat, I strictly followed TensorFlow’s official guide: <ahref="https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"><strong>TensorFlowLite inference: Load and run a model in Python</strong></a>.Ourconverted models have the signatures, you can just follow the “with adefined SignatureDef” guide.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = MobileBertTokenizerFast(<span class="string">f&quot;<span class="subst">&#123;model_path&#125;</span>/vocab.txt&quot;</span>)</span><br><span class="line">t_output = tokenizer(<span class="string">&quot;越过长城，走向世界&quot;</span>, return_tensors=<span class="string">&quot;tf&quot;</span>)</span><br><span class="line">ii, tt, am = t_output[<span class="string">&#x27;input_ids&#x27;</span>], t_output[<span class="string">&#x27;token_type_ids&#x27;</span>], t_output[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line"><span class="comment"># `get_signature_runner()` with empty input gives the &quot;serving_default&quot; runner</span></span><br><span class="line"><span class="comment"># `runner` input parameter is specified by `serving_default[&#x27;inputs&#x27;]`</span></span><br><span class="line">runner = interpreter.get_signature_runner() </span><br><span class="line">output = runner(input_ids = ii, token_type_ids = tt, attention_mask = am)</span><br><span class="line"><span class="keyword">assert</span> output.keys == [<span class="string">&#x27;last_hidden_state&#x27;</span>, <span class="string">&#x27;pooler_output&#x27;</span>]</span><br></pre></td></tr></table></figure><p>On the other hand, for a model without signatures, you need to usethe more primitive API <code>input_details</code> and<code>output_details</code>. They specify the following properties,where <code>index</code> is (probably) the index of this tensor in thecompute graph. To pass input values and get output values, you need toaccess them by this index.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">interpreter.allocate_tensors()</span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"></span><br><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>], input_data)</span><br><span class="line">interpreter.invoke()</span><br><span class="line">output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(output_data)</span><br></pre></td></tr></table></figure><p>The following is the <code>input_details</code> of the non-signatureGoogle packed MobileBert.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">interpreter.get_input_details()</span><br><span class="line">[&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;model_attention_mask:0&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="string">&#x27;shape&#x27;</span>: array([  <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line">  <span class="string">&#x27;shape_signature&#x27;</span>: array([  <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line">  <span class="string">&#x27;dtype&#x27;</span>: numpy.int64,</span><br><span class="line">  <span class="string">&#x27;quantization&#x27;</span>: (<span class="number">0.0</span>, <span class="number">0</span>),</span><br><span class="line">  <span class="string">&#x27;quantization_parameters&#x27;</span>: &#123;<span class="string">&#x27;scales&#x27;</span>: array([], dtype=float32),</span><br><span class="line">   <span class="string">&#x27;zero_points&#x27;</span>: array([], dtype=int32),</span><br><span class="line">   <span class="string">&#x27;quantized_dimension&#x27;</span>: <span class="number">0</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;sparsity_parameters&#x27;</span>: &#123;&#125;&#125;,</span><br><span class="line">  &#123;...&#125;]</span><br></pre></td></tr></table></figure><h2 id="numerical-accuracy">Numerical Accuracy</h2><p>Our original torch/TensorFlow encoder and the converted TFLiteencoder, when both running on PC using Python, has a 1.2% difference intheir output (<code>last_hidden_state</code> or<code>pooled_output</code>). We <strong>do not know</strong> where thisdiscrepancy comes from.</p><h2 id="converting-tokenizer-to-tflite">Converting Tokenizer toTFLite</h2><p>We exported and ran the <em>encoder</em>, but that’s not enough. Wecan’t ask the user to type in <code>token_ids</code> every time.Therefore, we need to integrate the preprocessor (tokenizer) into ourTFLite file. To do that, we first tried integrating <ahref="https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-preprocess/3">Google’sofficial Keras tokenizer implementation</a> into our BERT model andconvert them together into a TFLite (yeah I didn’t learn the lesson).This failed in the converting step for reasons that would become clearlater. And we switched gears to follow some other guide and first try toconvert a standalone tokenizer to TFLite.</p><p>Tokenizer is a part of the TensorFlow Text library. I followed the <ahref="https://www.tensorflow.org/text/guide/text_tf_lite"><strong>officialguide: Converting TensorFlow Text operators to TensorFlowLite</strong></a> with <code>text.FastBertTokenizer</code>. Note whenyou follow it, do it carefully and closely. I encountered a few problemsalong the way:</p><ol type="1"><li><p>When you change the <code>text.WhitespaceTokenizer</code> inguide to our <code>text.FastBertTokenizer</code>, remember to specify a<code>text.FastBertTokenizer(vocab=vocab_lst)</code>. We need not thepath to the vocab but the actual liste.g. <code>[ "[PAD]", "[unused0]", "[unused1]", ...]</code> describesthe vocab where <code>[PAD]</code> maps to token id 0,<code>[unused0]</code> to token id 1, and so on.</p></li><li><p><code>text.FastBertTokenizer</code> (or the standard version)does not add <code>[CLS]</code> token for you. Google says this is tomake sure “you are able to manipulate the tokens and determine how toconstruct your segments separately” (<ahref="https://github.com/tensorflow/text/issues/146">GitHub issue</a>).How considerate you are, dear Google. I spent one and a half dayfiguring out how to add these tokens when the model’s input length needsto be fixed, otherwise it triggers TensorFlow’s compute graph to throw“can’t get variable-length input” error. I finally found a solution in<ahref="https://github.com/google-ai-edge/mediapipe/blob/a91256a42bbe49f8ebdb9e2ec7643c5c69dbec6f/mediapipe/model_maker/python/text/text_classifier/bert_tokenizer.py#L58-L71">Google’smediapipe’s implementation</a>.</p></li><li><p><code>Could not translate MLIR to FlatBuffer</code> when running<code>tflite_model = converter.convert()</code>: as mentioned, you mustfollow the guide very carefully. The guide specifies a TensorFlow Textversion. If not this version, the conversion would fail</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U <span class="string">&quot;tensorflow-text==2.11.*&quot;</span></span><br></pre></td></tr></table></figure></li><li><p><code>Encountered unresolved custom op: FastBertNormalize</code>when running converted interpreter / signature: as stated in the <ahref="https://www.tensorflow.org/text/guide/text_tf_lite#inference">Inferencesection of the guide</a>, tokenizers are custom operations and need tobe specified when running inference. (I can’t find doc for<code>InterpreterWithCustomOps</code> anywhere but it does have anargument <code>model_path</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">interp = interpreter.InterpreterWithCustomOps(</span><br><span class="line">    model_content=tflite_model,<span class="comment"># or model_path=TFLITE_FILE_PATH</span></span><br><span class="line">    custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)</span><br></pre></td></tr></table></figure></li><li><p>TensorFlow Text custom ops are not found on Android: the aboveinference guide writes</p><blockquote><p>while the example below shows inference in Python, the steps aresimilar in other languages with some minor API translations</p></blockquote><p>which is a total lie. Android does not support these operations asthe <ahref="https://www.tensorflow.org/lite/guide/op_select_allowlist#tensorflow_text_and_sentencepiece_operators">customtext op list</a> only mentions python support.</p></li></ol><p>At the end, I did manage to 1 merge the above tokenizer andHuggingFace model, 2 export a TFLite model that reads in a text andoutputs the last hidden state. However, I seem to have lost that pieceof the code. Don’t worry though. Because thanks to Google’s shittyframework, it only works with very few tokenizer implementations anyway.The work-for-all solution is to build your own tokenizer in Java.</p><blockquote><p>P.S. While debugging the FlatBuffer error, I came across the <ahref="https://www.tensorflow.org/lite/guide/authoring">TensorFlowauthoring tool</a> that can explicitly specify a function’s input outputformat and detect op unsupported by TFLite. However, the tools is prettybroken for me. Debugging this tool would probably take longer thanfinding the problem yourself online / ask on a forum.</p></blockquote><h2 id="writing-your-own-tokenizer">Writing Your Own Tokenizer</h2><p>What’s weird is TensorFlow does have an official BERT on Androidexample. Reading it again, I found their tokenizer is actuallyimplemented by C++ (<ahref="https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier#key_features_of_the_bertnlclassifier_api">seethis example</a>). The repo containing the tokenizer code is called <ahref="https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/text/tokenizers/bert_tokenizer.h">tflite-support</a>.Finding <ahref="https://www.tensorflow.org/lite/inference_with_metadata/lite_support#current_use-case_coverage">thislibrary’s doc</a>, it becomes clear that the text-related operations arecurrently not supported.</p><figure><img src="./images/tflite-support.png"alt="TFLite-Support Current use-case coverage" /><figcaption aria-hidden="true">TFLite-Support Current use-casecoverage</figcaption></figure><p>Google seems to have used JNI to call the C++ implementation oftokenizer (<ahref="https://github.com/tensorflow/tflite-support/blob/8ed4a7b70df385a253aad7ed7df782439f42da6c/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/text/nlclassifier/BertNLClassifier.java#L39-L53">seecode</a>).</p><p>Therefore, we’d better write our own tokenizer. Luckily Hugging Facealso has a Bert on Android example - <ahref="https://github.com/huggingface/tflite-android-transformers/tree/master/bert">tflite-android-transformers</a>and writes more accessible code. We directly copied <ahref="https://github.com/huggingface/tflite-android-transformers/tree/master/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization">theirtokenizer implementation</a>.</p><p>However, when switching to Chinese vocabulary, the tokenizer goesglitchy. See the following example where we tokenize thesentence「越过长城 ，走向世界」</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Our Java tokenizer gives the following tokens, which detokenizes to the following string</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">19871</span>, <span class="number">20327</span>, <span class="number">14871</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">14460</span>, <span class="number">13743</span>, <span class="number">17575</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">&#x27;[CLS] 越过长城 ， 走向世界 [SEP]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># On the other hand, official Hugging Face python BertTokenizer gives</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">6814</span>, <span class="number">7270</span>, <span class="number">1814</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">1403</span>, <span class="number">686</span>, <span class="number">4518</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">&#x27;[CLS] 越 过 长 城 ， 走 向 世 界 [SEP]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspecting the first difference, our Java tokenizer seems to have used sentencepiece </span></span><br><span class="line">tokenizer.decode([<span class="number">19871</span>])</span><br><span class="line"><span class="string">&#x27;##过&#x27;</span></span><br></pre></td></tr></table></figure><p>It turns out <ahref="https://github.com/google-research/bert/blob/master/multilingual.md#tokenization">BERTin its original implementation</a> (<ahref="https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/tokenization.py#L207">code</a>)does not use sentence-piece tokenizer on Chinese characters. Instead, ituses character level tokenizer. Therefore, we need to first insert awhitespace to every character to ensure sentence-piece isn’t applied.Note Hugging Face tokenizer follows BERT original python code veryclosely so you can <ahref="https://github.com/huggingface/tflite-android-transformers/blob/dcd6da1bfb28e3cd6bc83b58a112cdcd3d6cc2fe/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization/BasicTokenizer.java#L34">easilyfind where to insert</a> that piece of code.</p><ul><li><p>Bert original implementation in Python, with Chinese logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Tokenizes a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">  text = convert_to_unicode(text)</span><br><span class="line">  text = self._clean_text(text)</span><br><span class="line">  <span class="comment"># Chinese Logic</span></span><br><span class="line">  text = self._tokenize_chinese_chars(text)</span><br><span class="line">  orig_tokens = whitespace_tokenize(text)</span><br></pre></td></tr></table></figure></li><li><p>Hugging Face tokenizer in Java, without Chinese logic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">tokenize</span><span class="params">(String text)</span> </span>&#123;</span><br><span class="line">    String cleanedText = cleanText(text);</span><br><span class="line">    <span class="comment">// Insert Here</span></span><br><span class="line">    List&lt;String&gt; origTokens = whitespaceTokenize(cleanedText);</span><br></pre></td></tr></table></figure></li></ul><h2 id="building-a-classifier">Building a Classifier</h2><p>The final task is actually to build a classifier of 28 online storecommodity classes. As I mentioned in the <a href="#Detours">Detourssection</a>, I do not know and don’t wanna bother to know how to defineor change a signature. Therefore, I again turn to Hugging Face for its<code>MobileBertForSequenceClassification</code>.</p><p>The default classification head only has 1 layer, I changed itsstructure to give it more expressive power.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = MobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, num_labels=<span class="built_in">len</span>(labels), problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    id2label=id2label, label2id=label2id)</span><br><span class="line">model.classifier = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line">    (<span class="string">&#x27;relu1&#x27;</span>, nn.LeakyReLU()),</span><br><span class="line">    (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">1024</span>, num_labels))</span><br><span class="line">]))</span><br><span class="line"><span class="comment"># Fine-tune ...</span></span><br><span class="line">torch.save(model.state_dict(), model_path)</span><br></pre></td></tr></table></figure><p>However, this throws error when you try to read such a fine-tunedmodel back in. <code>MobileBertForSequenceClassification</code> is setto have one-layer classification head, so it cannot read in yourself-defined classifier’s weights.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    num_labels=len(labels), id2label=id2label, label2id=label2id)</span><br><span class="line"></span><br><span class="line">&gt; Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at ./ckpts/ and are newly initialized: [<span class="string">&#x27;classifier.bias&#x27;</span>, <span class="string">&#x27;classifier.weight&#x27;</span>]</span><br></pre></td></tr></table></figure><p>To solve this, you can</p><ol type="1"><li>Save encoder weight and classifier weight separately, then load themseparately</li><li>Create a custom class corresponding to your weights and initializean instance of that class instead</li></ol><p>2 is clearly <ahref="https://github.com/huggingface/transformers/issues/1001#issuecomment-520162877">themore sensible way</a>. You should read the very clearly written<code>MobileBertForSequenceClassification</code> to understand whatexactly needs to be changed. It turns out all we have to do is to extendthe original class and change its <code>__init__</code> part, so it hasa 2-layer classification head.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertForSequenceClassification, TFMobileBertForSequenceClassification</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomMobileBertForSequenceClassification</span>(<span class="params">MobileBertForSequenceClassification</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        self.classifier = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu1&#x27;</span>, nn.LeakyReLU()),</span><br><span class="line">            (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(<span class="number">1024</span>, <span class="number">28</span>))</span><br><span class="line">        ]))</span><br><span class="line">        self.post_init()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFCustomMobileBertForSequenceClassification</span>(<span class="params">TFMobileBertForSequenceClassification</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, *inputs, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config, *inputs, **kwargs)</span><br><span class="line">        self.classifier = keras.Sequential([</span><br><span class="line">            keras.layers.Dense(<span class="number">1024</span>, input_dim=<span class="number">768</span>, name=<span class="string">&#x27;fc1&#x27;</span>),</span><br><span class="line">            keras.layers.LeakyReLU(alpha=<span class="number">0.01</span>, name = <span class="string">&#x27;relu1&#x27;</span>),  <span class="comment"># Keras defaults alpha to 0.3</span></span><br><span class="line">            keras.layers.Dense(<span class="number">28</span>, name=<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_path, problem_type=<span class="string">&quot;multi_label_classification&quot;</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(labels), id2label=id2label, label2id=label2id)</span><br><span class="line">tf_model = TFCustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line">    ..., from_pt=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>However, you may find these two models output different values on thesame input. A closer look at weights unveil that <strong>Hugging Facedidn’t convert classifier’s weights from our Torch model to TensorFlowmodel correctly</strong>. We have to set them manually instead.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf_model.classifier.get_layer(<span class="string">&quot;fc1&quot;</span>).set_weights([torch_model.classifier.fc1.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc1.bias.detach()])</span><br><span class="line">tf_model.classifier.get_layer(<span class="string">&quot;fc2&quot;</span>).set_weights([torch_model.classifier.fc2.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc2.bias.detach()])</span><br></pre></td></tr></table></figure><p>And now we are finally ready to go.</p><h2 id="quantization">Quantization</h2><p>I followed this official doc: <ahref="https://ai.google.dev/edge/litert/models/post_training_quantization">Post-trainingquantization</a>. Because of time limit, I didn’t try Quantization AwareTraining (QAT).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vanilla_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">tflite_model = vanilla_converter.convert()</span><br><span class="line"></span><br><span class="line">quant8_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant8_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">tflite_quant8_model = quant8_converter.convert()</span><br><span class="line"></span><br><span class="line">quant16_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant16_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">quant16_converter.target_spec.supported_types = [tf.float16]</span><br><span class="line">tflite_quant16_model = quant16_converter.convert()</span><br></pre></td></tr></table></figure><p>Below I report several key metrics for this Chinese-MobileBERT + a2-layer classification head of [768*1024, 1024*<code>class_num</code>].This was tested on a Xiaomi 12X with snapdragon 870. The baseline modelis my colleague’s BERT-Large implementation with accuracy 88.50% andsize 1230MB. My model’s accuracy was bad at first: 75.01% withhyper-parameter <code>weight_decay = 0.01, learning_rate = 1e-4</code>,but we searched out a good hyper-parameter of<code>weight_decay = 2e-4,learning_rate = 2e-5</code> giving 86.01%. Wehad 28 classes, 38000 training data in total, and trained for 5 epochswhere the validation accuracy roughly flattens.</p><table><colgroup><col style="width: 12%" /><col style="width: 11%" /><col style="width: 5%" /><col style="width: 24%" /><col style="width: 10%" /><col style="width: 12%" /><col style="width: 10%" /><col style="width: 4%" /><col style="width: 7%" /></colgroup><thead><tr class="header"><th>Quantization</th><th>Logit Difference</th><th>Accuracy</th><th>Accuracy (after hyper-param search)</th><th>Model Size (MB)</th><th>Inference Time(ms)</th><th>Power Usage(ma)</th><th>CPU(%)</th><th>Memory(MB)</th></tr></thead><tbody><tr class="odd"><td>float32 (No quant)</td><td>0</td><td>75.01%</td><td>86.094%</td><td>101.4</td><td>1003.3</td><td>89.98</td><td>108.02</td><td>267.11</td></tr><tr class="even"><td>float16</td><td>0.015%</td><td>75.01%</td><td>86.073%</td><td>51</td><td>838</td><td>64.15</td><td>108.77</td><td>377.11</td></tr><tr class="odd"><td>int8</td><td>4.251%</td><td>63.49%</td><td>85.947%</td><td>25.9</td><td>573.8</td><td>60.09</td><td>110.83</td><td>233.19</td></tr></tbody></table><p>If look at the not fine-tuned, vanilla transformer encoder only, the<code>last_hidden_state</code> has a difference:</p><table><thead><tr class="header"><th>Quantization</th><th>Logit Difference</th><th>Model Size (MB)</th></tr></thead><tbody><tr class="odd"><td>float32 (No quant)</td><td>0</td><td>97</td></tr><tr class="even"><td>float16</td><td>0.1%</td><td>48.1</td></tr><tr class="odd"><td>int8</td><td>19.8%</td><td>24.9</td></tr></tbody></table><h2 id="small-language-models">Small Language Models</h2><p>BERT is the go-to option for classification task. But when it comesto small BERT, we had several options:</p><ul><li><p>mobileBERT</p></li><li><p>distilledBERT</p></li><li><p>tinyBERT</p></li></ul><p>As the post is about, we used mobileBERT at last because it’s byGoogle Brain and Google probably knows their thing best.</p><p>On the other hand, if you’re looking for small generative model,which people mostly call SLM (Small Language Model) as opposed to LLM, Ifound these options but didn’t try them myself.</p><ul><li>openELM: Apple, 1.1B</li><li>Phi-2: Microsoft, 2.7B</li></ul><h2 id="post-script">Post Script</h2><p>If you want to build an app utilizing edge transformer, I wouldrecommend to read the source code of <ahref="https://github.com/huggingface/tflite-android-transformers">HuggingFace’s toy app</a>. It doesn’t have a README or tutorial, nor have Igone through it personally, but everything from TensorFlow sucks(including MediaPipe unfortunately)</p><p>When checking back on this tutorial at date 2024/12/28, I foundGoogle released <ahref="https://github.com/google-ai-edge/ai-edge-torch">AI EdgeTorch</a>, the official tool converting PyTorch models into a .tfliteformat. So you may probably want to try this first, but again, don’ttrust anything from TensorFlow team.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;So Google, fxxk you.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="https://yao-lirong.github.io/2024-09-09-Variational-Inference/"/>
    <id>https://yao-lirong.github.io/2024-09-09-Variational-Inference/</id>
    <published>2024-09-09T04:00:00.000Z</published>
    <updated>2025-09-02T23:58:14.678Z</updated>
    
    <content type="html"><![CDATA[<h2 id="probabilistic-latent-variable-models">Probabilistic LatentVariable Models</h2><p>The two general forms of probabilistic models are:</p><ul><li><span class="math inline"><em>p</em>(<em>x</em>)</span>: a typicalprobabilistic distribution. In this model, we call <spanclass="math inline"><em>x</em></span> the query.</li><li><spanclass="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>)</span>: aconditional probabilistic distribution. In this model, we cal <spanclass="math inline"><em>x</em></span> the evidence and <spanclass="math inline"><em>y</em></span> the query.</li></ul><p>Latent variable models are models that have variables other than thequery and the evidence.</p><ul><li><p><spanclass="math inline"><em>p</em>(<em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>x</em> ∣ <em>z</em>) <em>p</em>(<em>z</em>)</span>​</p><p>A classic latent variable model of <spanclass="math inline"><em>p</em>(<em>x</em>)</span> is the mixture model,where <span class="math inline"><em>p</em>(<em>x</em>)</span> isactually a mixture of several different probabilistic model. Forexample, in the following graph, <spanclass="math inline"><em>z</em></span> is a discrete variablerepresenting which class a datapoint belongs to and is represented bydifferent colors here. <spanclass="math inline"><em>p</em>(<em>x</em> ∣ <em>z</em>)</span> is eachclass’s probability distribution, where in this case can each be modeledby a Gaussian. And <spanclass="math inline"><em>p</em>(<em>x</em>)</span>​ when we observe it, isjust a bunch of uncolored datapoints and is hard to fit a distributionon it. However, we can see it’s roughly spread in 3 clusters so weintroduce the latent variable representing class and we can now well fita Gaussian mixture model on it (a mixture of 3 Gaussians)</p><figure><img src="./images/Mixture-Gaussian-Distribution.png"alt="Gaussian Mixture Model" /><figcaption aria-hidden="true">Gaussian Mixture Model</figcaption></figure></li><li><p><spanclass="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>y</em> ∣ <em>x</em>, <em>z</em>) <em>p</em>(<em>z</em>)</span>or <spanclass="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>y</em> ∣ <em>z</em>) <em>p</em>(<em>z</em> ∣ <em>x</em>)</span>:the conditional probability is a bit more free. You can decompose andmodel it using <span class="math inline"><em>z</em></span>​ as youlike.</p><p>An example of latent conditional model is the mixture densitynetwork, which we use in RL’s imitation learning to deal withmulti-modal situations each requiring a different distribution.</p></li></ul><h3 id="latent-variable-models-in-general">Latent Variable Models inGeneral</h3><p>When we use latent variable models, it means we want to<strong>decompose a complicated distribution into several simple / easydistributions</strong>. By <strong>complicated</strong>, we mean it’snot possible to write it in a well-defined distribution. By<strong>simple / easy</strong>, we mean we can write it as awell-defined parametrized distribution, where the parameters can becomplex, but the distribution itself is easy to write (a Gaussian ofjust mean and sigma, or as a Bernoulli with just one variable, etc.)<spanclass="math display"><em>p</em>(<em>x</em>) = ∫<em>p</em>(<em>x</em> ∣ <em>z</em>)<em>p</em>(<em>z</em>)<em>d</em><em>z</em></span></p><ul><li><span class="math inline"><em>p</em>(<em>z</em>)</span> is an “easy”prior we choose. For example a Gaussian, a categorical distribution,etc.</li><li><span class="math inline"><em>p</em>(<em>x</em> ∣ <em>z</em>)</span>should also be an easy distribution, like a Gaussian: $ p(x z) =(<em>{nn}(z), </em>{nn}(z))$ even though the mapping from <spanclass="math inline"><em>z</em></span> to the actual parameters ofGaussian can be complex, where in this case we have to model the mappingthrough a neural network and this mapping is the learnable part.</li><li><span class="math inline"><em>p</em>(<em>x</em>)</span> iscomplicated, not possible to write out as any well-defined distribution.Therefore, we decompose it into the two parts above that are<strong>easy to parametrize as a probability distribution and learn theparameters inside the distribution</strong>.</li></ul><p>Generative models are not equal to latent variable models. We usuallymodel generative models as latent variable ones because generativemodels are usually complex probability distributions and we can make iteasier by introducing one or more latent variable.</p><h3 id="how-to-train-a-latent-variable-model">How to Train a LatentVariable Model</h3><p>Given dataset <spanclass="math inline">𝒟 = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>N</em></sub>}</span>,to fit a typical probabilistic model <spanclass="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em>)</span>,we use the maximum likelihood estimation: <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N \sum_i \logp_\theta(x_i)$$</span> In the latent variable model set up, we can substitute thedefinition in and an MLE would look like <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N\sum_i \log \left( \int p_\theta(x_i \mid z) p(z) dz \right)$$</span> <spanclass="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em> ∣ <em>z</em>)</span>and <span class="math inline"><em>p</em>(<em>z</em>)</span> aredistributions of our choices, but this integral is still intractablewhen <span class="math inline"><em>z</em></span> is continuous. So nowit’s time to do some math tricks.</p><h2 id="variational-inference">Variational Inference</h2><h3 id="variational-approximation">Variational Approximation</h3><p>First, we construct an easy / simple probability distribution <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>to approximate <spanclass="math inline"><em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</span>- the posterior distribution specific to datapoint <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>. By easy weagain mean it is easy to parametrize (a Gaussian, a Bernoulli, etc.)</p><p>We will show that by introducing this <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>,we can actually construct a lower bound of <spanclass="math inline">log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>.What’s good with this lower bound? Later on, we will also prove thisbound is sufficiently tight, so as we push up the value of this lowerbound, we push up the value of <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>which is exactly what we want.</p><p><span class="math display">$$\begin{align}\log p(x_{i})&amp;=    \log\int_{z}p(x_{i}|z)p(z)\\&amp;=    \log\int_{z}p(x_{i}|z)p(z) \frac{q_i(z)}{q_i(z)} \\&amp;=    \log \mathbb E_{z\sim q_{i}(z)} \left[\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] \\&amp;\geq \mathbb E_{z\sim q_{i}(z)}\left[\log\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] &amp;\text{\# Jensen'sInequality} \\&amp;=    \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] - \mathbb E_{z\sim q_{i}(z)} \left[ \log {q_{i}(z)}\right]\\&amp;=    \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)=    \mathcal L_i(p, q_i)\end{align}$$</span> Recall <span class="math inline"><em>p</em>(<em>x</em>)</span>is a difficult probability distribution, so we decompose it into twoeasy distributions <spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span> and <spanclass="math inline"><em>p</em>(<em>z</em>)</span>, and use an easydistribution <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>to approximate the posterior <spanclass="math inline"><em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</span>.Now the good thing is: everything here is tractable: for the first term,we can fix a <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>of our choice (recall <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> is adistribution we constructed), sample some <spanclass="math inline"><em>z</em></span>, and evaluate the expression. Forthe second term, we notice it is just the entropy of a distribution andfor simple distributions (we constructed <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> to besimple), it has a closed form (even if it doesn’t, you can simply sampleand evaluate)</p><p>We call the final lower bound we derived here the <strong>variancelower bound</strong> or <strong>evidence lower bound (ELBO)</strong>.<span class="math display">$$\begin{align}\log p(x_{i})&amp;\geq \mathcal L_i(p, q_i) \\&amp;=    \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)\end{align}$$</span> ### Effect of Pushing Up ELBO (Intuitively)</p><p>Assume our <span class="math inline"><em>p</em>(⋅)</span>​ is a fixedvalue, what does pushing up ELBO mean? Here, we give out an intuitiveexplanation. First, we look at <strong>the first term</strong> with thetwo log combined. <span class="math display">$$\begin{align}  &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] \\= &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i},z) \right]\end{align}$$</span> To maximize this value, we just have to find a distribution of<span class="math inline"><em>z</em></span>, inside which we have thelargest value of <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)</span>.Therefore, we want <span class="math inline"><em>z</em></span> todistribute mostly under the peak of <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)</span>,Since <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>is the distribution we currently have for z, we want <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>to sit mostly under the peak of <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)</span>.In the following graph, the y-axis is <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)</span>,the distribution we try to maximize, and the x-axis is our latentvariable z. There is also a hidden axis - the probability mass(distribution) of <span class="math inline"><em>z</em></span>. Weproject this hidden axis to the y-axis in this graph. To maximize thisfirst term, we spread <span class="math inline"><em>z</em></span>’s massas much under the peak of <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)</span>as possible, which makes the green part of this graph.</p><figure><img src="./images/p(xz)-with-entropy.png" alt="maximize ELBO" /><figcaption aria-hidden="true">maximize ELBO</figcaption></figure><p>Now we take <strong>the second term entropy</strong> intoconsideration. <spanclass="math display">ℒ<sub><em>i</em></sub>(<em>p</em>, <em>q</em><sub><em>i</em></sub>) = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log <em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)] + ℋ<sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>(<em>q</em><sub><em>i</em></sub>)</span>From our <a href="link!"><strong><em>entropy post</em></strong></a>, weknow entropy measures the expected code length of communicating theevent described by a random variable. So the more random this variableis, the more code words it’s required to communicate it. Therefore, themore spread out / uniform the distribution is, the higher the entropy.If we’re maxing the entropy, we don’t want the distribution to beskinny. See the following graph.</p><figure><img src="./images/entropy-example.png" alt="entropy-example" /><figcaption aria-hidden="true">entropy-example</figcaption></figure><p>When we consider both entropy and the first term, we should achievethis probability distribution depicted in brown. If we don’t have theentropy, <span class="math inline"><em>z</em></span> will want to situnder the most likely point, but since we added entropy, <spanclass="math inline"><em>z</em></span> now tries to cover it. Inconclusion, (equal sign “=” reads “in effect”) maximize evidence lowerbound = cover most of the <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)</span>distribution = maximize approximation between <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> and <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)</span>.</p><figure><img src="./images/p(xz)-with-entropy.png" alt="maximize ELBO" /><figcaption aria-hidden="true">maximize ELBO</figcaption></figure><h3 id="effect-of-pushing-up-elbo-analytically">Effect of Pushing UpELBO (Analytically)</h3><p>Can we measure how good our approximation is? That is, can we measurethe distance between <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)</span>and <span class="math inline"><em>q</em><sub><em>i</em></sub></span>? Infact, we have a nice, analytical way to look at it using <strong>KLdivergence</strong>. For two arbitrary distribution <spanclass="math inline"><em>p</em>, <em>q</em></span> of <spanclass="math inline"><em>x</em></span>, the KL divergence of <spanclass="math inline"><em>q</em></span> from <spanclass="math inline"><em>p</em></span> (the distance from <spanclass="math inline"><em>q</em></span> to <spanclass="math inline"><em>p</em></span>, note KL divergence is notsymmetric) is</p><p><span class="math display">$$\begin{align}D_{\mathrm{KL}}(q|p)&amp;=E_{x\sim q(x)}\left[\log{\frac{q(x)}{p(x)}}\right]\\&amp;=E_{x \sim q(x)}[\log q(x)]-E_{x \sim q(x)}[\log p(x)]\\&amp;=-E_{x \sim q(x)}[\log p(x)]-H(q)\end{align}$$</span> Doesn’t this look similar to our evidence lower bound?Borrowing that explanation, minimizing KL divergence = cover most of the<span class="math inline"><em>p</em>(<em>z</em>)</span> distribution =maximize approximation between <spanclass="math inline"><em>q</em></span> and <spanclass="math inline"><em>p</em></span>.</p><figure><img src="./images/KL-divergence.png" alt="KL-divergence" /><figcaption aria-hidden="true">KL-divergence</figcaption></figure><p>Having understood the definition of KL divergence, let’s use it tomeasure the distance between <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>and <spanclass="math inline"><em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</span>- the distribution we want <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> toapproximate: <span class="math display">$$\begin{align}D_{KL}(q_{i}(z)\|p(z \mid x_{i}))&amp;=  E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)}{p(z|x_{i})}}\right]\\&amp;=  E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)p(x_{i})}{p(x_{i},z)}}\right]\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +E_{z\sim q_{i}(z)}\log q_i(z) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +\mathcal H(q_i) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -\mathcal L(p, q_i) + \log p(x_i)\\\log p(x_i) &amp;= \mathcal L(p, q_i) + D_{KL}(q_{i}(x_{i})\|p(z \midx_{i}))\end{align}$$</span> Therefore, having a good approximation of <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> to <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)</span>= driving KL divergence, which is always a non-negative number, to 0 =the evidence lower bound is a tight bound or even equal to <spanclass="math inline">log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>​- the ultimate thing we want to optimize.</p><p>Looking at our optimization objective <spanclass="math inline">ℒ</span> here: <spanclass="math display">ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>) = log <em>p</em>(<em>x</em><sub><em>i</em></sub>) − <em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub>)∥<em>p</em>(<em>z</em> ∣ <em>x</em><sub><em>i</em></sub>))</span></p><ul><li><p>When we optimize w.r.t. <spanclass="math inline"><em>q</em></span>: note the first term <spanclass="math inline">log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>is independent of <span class="math inline"><em>q</em></span>, so itsvalue stays the same. We are in effect optimizing against the KLdivergence only, making the distance between our approximation <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> and <spanclass="math inline"><em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</span>smaller. The best / extreme case is we have <spanclass="math inline"><em>D</em><sub><em>K</em><em>L</em></sub> = 0</span>,so <spanclass="math inline">ℒ = log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>.</p></li><li><p>When we optimize w.r.t. <spanclass="math inline"><em>p</em></span>: Recall our ultimate goal is tomake <spanclass="math inline">log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>bigger, so we make a better model in theory. Only in theory because wedon’t know whether the bound is tight or not.</p></li></ul><h3 id="the-learning-algorithm">The Learning Algorithm?</h3><p>Therefore, when we optimize <spanclass="math inline">ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>)</span>​w.r.t. <span class="math inline"><em>q</em></span>​, we make the boundtighter (make <span class="math inline">ℒ</span>​ a better approximationof <spanclass="math inline">log <em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>​); when we optimize <spanclass="math inline">ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>)</span>​w.r.t. <span class="math inline"><em>p</em></span>​, we make a bettermodel in general.</p><p>By alternating these two steps, we have <strong>the actual learningalgorithm</strong>. Let’s review: which parts are learnable in these twodistributions?</p><ul><li><p>In our <a href="#Latent-Variable-Models-in-General">latentvariable model setup</a>, we decompose the complicated distribution<span class="math inline"><em>p</em>(<em>x</em>)</span> into two easydistributions <spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span> and <spanclass="math inline"><em>p</em>(<em>z</em>)</span>, where the mappingfrom <span class="math inline"><em>z</em></span> to actual parameters ofthis <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>distribution needs to be modeled by a complex network. Therefore, theonly distribution in the <span class="math inline"><em>p</em></span>part with learnable parameters is <spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>. We denoteit with <spanclass="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>)</span>.</p></li><li><p>In our <a href="#Variational-Approximation">ELBO setup</a>, wealso introduced a simple <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>)</span>for each datapoint <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span> toapproximate the posterior <spanclass="math inline"><em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</span>.To optimize w.r.t. <span class="math inline"><em>q</em></span>, weoptimize the parameters of each distribution. If <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(<em>z</em>) = 𝒩(<em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</span>,we optimize each <spanclass="math inline"><em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub></span>.(<em>we can optimize the entropy value for sure, but I’m not entirelysure how you would take gradient of the expectation term <spanclass="math inline"><em>E</em><sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log <em>p</em>(<em>z</em>)]</span></em>)</p></li></ul><p>Therefore, we have our learning algorithm: <spanclass="math display">$$\begin{align}&amp;\text{for each $x_i$ in $\{x_1, \dots, x_N\}$: }\\&amp;\hspace{4mm} \text{sample $z \sim q_i(z)$}\\&amp;\hspace{4mm} \text{optimize against $p$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_\theta \mathcal L (p_\theta, q_i)= \nabla_\theta \log p_\theta(x_i|z) \\&amp;\hspace{4mm} \hspace{4mm} \theta \leftarrow \theta + \alpha\nabla_\theta \mathcal L (p, q_i) \\&amp;\hspace{4mm} \text{optimize against $q$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_{\mu_i, \sigma_i} \mathcal L(p_\theta, q_i) = \nabla_{\mu_i, \sigma_i} \left[\mathbb E_{z\simq_{i}(z)} \left[\log p(x_{i}|z)+\log p(z) \right] + \mathcal H_{z\simq_{i}(z)} (q_i) \right] \\&amp;\hspace{4mm} \hspace{4mm} (\mu_i, \sigma_i) \leftarrow (\mu_i,\sigma_i) + \alpha \nabla_{\mu_i, \sigma_i} \mathcal L (p, q_i) \\\end{align}$$</span></p><p>There’s a problem with optimizing <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> though. Notewe have a separate <span class="math inline"><em>q</em></span> for eachdata point <span class="math inline"><em>i</em></span>, which means ifwe have <span class="math inline"><em>N</em></span> data points, we willhave to store <spanclass="math inline"><em>N</em> × (|<em>μ</em><sub><em>i</em></sub>| + |<em>σ</em><sub><em>i</em></sub>|)</span>parameters assuming we chose <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> to beGaussian. In machine learning, the number of data points <spanclass="math inline"><em>N</em></span> is usually in millions, makingthis model unwieldily big. It’s true in inference time we do not use<span class="math inline"><em>q</em></span> at all (we’ll see why thisis true in the last chapter about VAE), but in training time, we stillneed them so it’s necessary to keep all these parameters.</p><p>Therefore, instead of having a separate <spanclass="math inline"><em>q</em><sub><em>i</em></sub>(⋅)</span> toapproximate each data point’s <spanclass="math inline"><em>P</em>(⋅|<em>x</em><sub><em>i</em></sub>)</span>specifically, we use a learnable model <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub>(⋅|<em>x</em><sub><em>i</em></sub>)</span>to approximate <spanclass="math inline"><em>p</em>(⋅|<em>x</em><sub><em>i</em></sub>)</span>This learnable network will take in a datapoint <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>, predicts thecorresponding <spanclass="math inline"><em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub></span>.We can then sample <span class="math inline"><em>z</em></span>​ from thispredicted network.</p><h2 id="amortized">Amortized</h2><p>By adapting <span class="math inline"><em>q</em></span> to be alearnable network <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub></span>​ instead,model size does not depend on the number of datapoints anymore.Therefore, it is <strong>amortized</strong>.</p><p>The variational lower bound becomes: <spanclass="math display">ℒ(<em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>), <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)) = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>) + log <em>p</em>(<em>z</em>)] + ℋ(<em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))</span>The learning algorithm naturally becomes: $$ $$</p><h3 id="gradient-over-expectation-policy-gradient">Gradient OverExpectation (Policy Gradient)</h3><p>The question now boils down to how do we calculate this gradient<spanclass="math inline">∇<sub><em>ϕ</em></sub>ℒ(<em>p</em><sub><em>θ</em></sub>, <em>q</em><sub><em>ϕ</em></sub>)</span>.</p><p>The second term entropy is easy. We purposefully chose <spanclass="math inline"><em>q</em></span> to be a simple distribution, sothere is usually a close form of its entropy and we just have to look itup.</p><p>The meat is in the first part. How do we take gradient w.r.t.parameter <span class="math inline"><em>ϕ</em></span> in the expectationterm’s distribution <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub></span> ? Note theterm inside expectation is independent of <spanclass="math inline"><em>ϕ</em></span>, so we can rewrite it as <spanclass="math inline"><em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>) = log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>) + log <em>p</em>(<em>z</em>)</span>and call the whole thing <spanclass="math inline"><em>J</em></span>.<br /><spanclass="math display"><em>J</em>(<em>ϕ</em>) = ∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)]</span>We chose these namings purposefully because we encountered somethingsimilar back in the <ahref="https://slides.com/sarahdean-2/sp24-cs-4789-lec-16?token=KNeurk-c#/11/0/4">policygradient part of reinforcement learning <strong>LINK???</strong></a>.Say we have a trajectory <span class="math inline"><em>τ</em></span>,sampled from the state transition function with learnable policy <spanclass="math inline"><em>π</em><sub><em>θ</em></sub></span>, the finalexpected value we can get from starting state <spanclass="math inline"><em>s</em><sub>0</sub></span> can be written as thefollowing, where <span class="math inline"><em>R</em>(<em>τ</em>)</span>is a reward function returning the reward of this trajectory. <spanclass="math display"><em>J</em>(<em>θ</em>) = <em>V</em><sup><em>π</em><sub><em>θ</em></sub></sup>(<em>s</em><sub>0</sub>) = 𝔼<sub><em>τ</em> ∼ <em>P</em><sub><em>s</em><sub>0</sub></sub><sup><em>π</em><sub><em>θ</em></sub></sup></sub>[<em>R</em>(<em>τ</em>)]</span>We can take the gradient of this value function <spanclass="math inline"><em>V</em></span> w.r.t our policy <spanclass="math inline"><em>π</em><sub><em>θ</em></sub></span>, so this iscalled policy gradient. If you’re unfamiliar with RL setup, you justhave to know we can derive the following gradient and we can approximateit by sampling <span class="math inline"><em>M</em></span> trajectories.$$ <span class="math display">$$Pugging in our $q$ and $\phi$,$$</span> $$</p><h3 id="reparametrization-trick">Reparametrization Trick</h3><p>We have our full learning algorithm and it’s ready to go now.However, there is a tiny improvement we can do.</p><p>We defined our <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub></span> to be anormal distribution <spanclass="math inline">𝒩(<em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub>)</span>Observe that all normal distributions can be written as a function ofthe unit normal distribution. Therefore, a sample <spanclass="math inline"><em>z</em></span> is in effect: <spanclass="math display"><em>z</em> ∼ 𝒩(<em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub>) ⇔ <em>z</em> = <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>, <em>ϵ</em> ∼ 𝒩(0, 1)</span>Let’s rewrite our expectation term to now sample an <spanclass="math inline"><em>ϵ</em></span> from the unit normal distributioninstead. By decomposing <span class="math inline"><em>z</em></span> intothese two parts, we separate the stochastic part and changed <spanclass="math inline"><em>z</em></span> from a sample of some stochasticdistribution into a deterministic function <spanclass="math inline"><em>z</em>(<em>ϕ</em>, <em>ϵ</em>)</span>parametrized by <span class="math inline"><em>ϕ</em></span> and randomvariable <span class="math inline"><em>ϵ</em></span> that is independentof <span class="math inline"><em>ϕ</em></span>. <spanclass="math inline"><em>ϵ</em></span> takes the stochastic part alone.Our learnable parameter <span class="math inline"><em>ϕ</em></span> nowonly parametrizes deterministic quantity. <spanclass="math display">∇<sub><em>ϕ</em></sub><em>J</em>(<em>ϕ</em>) = ∇<sub><em>ϕ</em></sub>𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]</span>Aside from these theoretical benefits, mathematically, we do not have totake gradient w.r.t an expectation of parametrized distribution anymore.Instead, the gradient can go straight into the expectation term now likehow we usually interchange gradient and expectation (think aboutdiscrete case, expectation is just a big sum so we can do it). <spanclass="math display">∇<sub><em>ϕ</em></sub><em>J</em>(<em>ϕ</em>) = 𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[∇<sub><em>ϕ</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]</span>Further, to approximate this expectation, we just sample some <spanclass="math inline"><em>ϵ</em></span> from this normal distribution.<span class="math display">$$\nabla_\phi J(\phi)\approx \frac 1 M \sum_j^M \nabla_\phi R(x_i, \mu_\phi + \epsilon_j\sigma_\phi)$$</span></p><p>With reparametrization, we achieve a lower variance than policygradient because we are using the derivative of R. (<em>Unfortunatelythe lecturer didn’t provide a quantitative analysis on this and I don’tknow how to prove it</em>) On the other hand, previously, we only tookderivative w.r.t. the probability distribution. Why didn’t we usederivative of R back in RL with policy gradient? It’s not we don’t wantto but we can’t: we can’t use reparametrization in RL because in RL weusually cannot take derivative w.r.t. reward R.</p><table><colgroup><col style="width: 6%" /><col style="width: 23%" /><col style="width: 23%" /><col style="width: 23%" /><col style="width: 23%" /></colgroup><thead><tr class="header"><th>Method</th><th>Formula</th><th>Approximation</th><th>Benefit</th><th>Deficit</th></tr></thead><tbody><tr class="odd"><td>Policy Gradient</td><td><spanclass="math inline">∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em> ∣ <em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)]</span></td><td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi[\logq_\phi(z_j \mid x_i)] R(x_i,z_j)$</span></td><td>works with both discrete and continuous latent variable <spanclass="math inline"><em>z</em></span></td><td>High variance, requires multiple samples &amp; small learningrates</td></tr><tr class="even"><td>Reparametrization</td><td><spanclass="math inline">𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[∇<sub><em>ϕ</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]</span></td><td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi R(x_i,\mu_\phi + \epsilon_j \sigma_\phi)$</span></td><td>low variance, simple to implement (we’ll see soon)</td><td>only works with continuous variable <spanclass="math inline"><em>z</em></span> and have to model it with aGaussian</td></tr></tbody></table><p>In fact, you can forget about the policy gradient method and simplytake it for granted that you cannot back propagate a sampled value <spanclass="math inline">∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub></span>,so you have to find some way to make our <spanclass="math inline"><em>z</em></span>​ deterministic, which is what we’redoing here with our reparametrization trick.</p><figure><img src="./images/reparametrization-trick.png"alt="reparametrization-trick" /><figcaption aria-hidden="true">reparametrization-trick</figcaption></figure><p>Left is without the “reparameterization trick”, and right is with it.Red shows sampling operations that are non-differentiable. Blue showsloss layers. We forward the network by going up and back propagate it bygoing down. The forward behavior of these networks is identical, butback propagation can be applied only to the right network. Figure copiedfrom <a href="https://arxiv.org/abs/1606.05908">Carl Doersch: Tutorialon Variational Autoencoders</a></p><h3 id="looking-at-mathcal-l-directly">Looking at <spanclass="math inline">ℒ</span> Directly</h3><p><span class="math display">$$\begin{align}\mathcal L_i = \mathcal L \left( p_\theta(x_i | z), q_\phi(z | x_i)\right)&amp;=  \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)+\log p(z) \right] + \mathcal H (q_\phi(z|x_i))\\&amp;=  \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p_\theta(x_{i}|z)\right] +    \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p(z) \right] + \mathcalH (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)\right] - D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;= \mathbb E_{\epsilon \sim \mathcal N(0,1)} \left[\logp_\theta(x_{i}| \mu_\phi + \epsilon \sigma_\phi)\right] -D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;\approx \frac 1 M \sum_j^M \log p_\theta(x_{i}| \mu_\phi +\epsilon_j \sigma_\phi) - D_{KL}(q_\phi(z | x_i)\|p(z)) \\\end{align}$$</span></p><p>For the first term, we can just evaluate it. For the second KL term,since we chose both distributions to be easy (in this case Gaussian),there often is a nice analytical form for it.</p><p>Therefore, we can go ahead to maximize the variational lower bound<span class="math inline">ℒ</span>​. We can also draw out the followingcomputational graph for the log term and conclude we can back propagatethis graph without any problem. On the other hand, if we didn’t do thereparametrization trick, we will get stuck at <spanclass="math inline"><em>z</em></span>: you cannot back propagate <spanclass="math inline"><em>z</em></span> - a sampled value instead of avariable. And we will have to seek help from policy gradient. Withreparametrization, we decompose <spanclass="math inline"><em>z</em></span> into two variables <spanclass="math inline"><em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub></span>we can back propagate through and one stochastic value <spanclass="math inline"><em>ϵ</em></span> we do not care about.</p><figure><img src="./images/computational-graph.png" alt="computational-graph" /><figcaption aria-hidden="true">computational-graph</figcaption></figure><h2 id="variational-autoencoder">Variational Autoencoder</h2><h3 id="setup-and-interpretation">Setup and Interpretation</h3><p>What we have gone though constitutes the full pipeline of avariational autoencoder.</p><p>In a variation autoencoder, we have observed variable <spanclass="math inline"><em>x</em></span> and latent variable <spanclass="math inline"><em>z</em></span></p><ul><li>encoder <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em>) = 𝒩(<em>μ</em><sub><em>ϕ</em></sub>(<em>x</em>), <em>σ</em><sub><em>ϕ</em></sub>(<em>x</em>))</span></li><li>decoder <spanclass="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>) = 𝒩(<em>μ</em><sub><em>θ</em></sub>(<em>z</em>), <em>σ</em><sub><em>θ</em></sub>(<em>z</em>))</span></li></ul><p>In training, given an observed sample <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>, we encode itto latent variable <spanclass="math inline"><em>z</em><sub><em>i</em></sub></span> using <spanclass="math inline"><em>q</em><sub><em>ϕ</em></sub></span>, then triesto decode it back with decoder <spanclass="math inline"><em>p</em><sub><em>θ</em></sub></span>. We maximizethe variational lower bound during the process. For all <spanclass="math inline"><em>N</em></span> samples, the training objectivelooks like: (where the <span class="math inline"><em>ϵ</em></span> is asampled value) <span class="math display">$$\max_{\phi,\theta} \frac 1 N \sum_i^N \log p_\theta\left(x_{i}|\mu_\phi(x_i) + \epsilon \sigma_\phi(x_i)\right) - D_{KL}(q_\phi(z |x_i)\|p(z)) \\$$</span> In inference (generation), we sample a <spanclass="math inline"><em>z</em></span> from our prior <spanclass="math inline"><em>p</em>(<em>z</em>)</span>, then decode it using<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>: <spanclass="math inline"><em>z</em> ∼ <em>p</em>(<em>z</em>), <em>x</em> ∼ <em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>)</span></p><p>Why does the variational autoencoder work? We talked about manybenefits of maximizing this variational lower bound in <ahref="#Effect-of-Pushing-Up-ELBO-(Analytically)">previous chapter</a>.Let’s look at it again in this decoder-encoder setup,. <spanclass="math display">ℒ<sub><em>i</em></sub> = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)] − <em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)∥<em>p</em>(<em>z</em>))</span></p><ul><li>The first <spanclass="math inline">log <em>p</em><sub><em>θ</em></sub></span> termmaximizes the probability of our observed image <spanclass="math inline"><em>x</em></span> given a sample <spanclass="math inline"><em>z</em></span>, so the model makes decoder <spanclass="math inline"><em>p</em><sub><em>θ</em></sub></span> toreconstruct image <span class="math inline"><em>x</em></span>​ asaccurate as possible.</li><li>The second KL term restricts the encoding of an image to be close tothe actual prior, which makes sure at inference / generate time, we candirectly sample from the prior.</li></ul><h3 id="comparison-with-auto-encoder">Comparison with Auto-Encoder</h3><figure><img src="./images/vae-and-ae.png" alt="vae-and-ae" /><figcaption aria-hidden="true">vae-and-ae</figcaption></figure><p>The VAE’s decoder is trained to convert random points in theembedding space (generated by perturbing the input encodings) tosensible outputs. By contrast, the decoder for the deterministicautoencoder only ever gets as inputs the exact encodings of the trainingset, so it does not know what to do with random inputs that are outsidewhat it was trained on. So a standard autoencoder cannot create newsamples.</p><p>The reason the VAE is better at sample is that it embeds images intoGaussians in latent space, whereas the AE embeds images into points,which are like delta functions. The advantage of using a latentdistribution is that it encourages local smoothness, since a given imagemay map to multiple nearby places, depending on the stochastic sampling.By contrast, in an AE, the latent space is typically not smooth, soimages from different classes often end up next to each other. Figurecopied from <ahref="https://probml.github.io/pml-book/book1.html">ProbabilisticMachine Learning: An Introduction - Figure 20.26</a></p><p>We can leverage the smoothness of the latent space to perform imageinterpolation in latent space.</p><h2 id="reference">Reference</h2><p>Most content of this blog post comes from <ahref="https://www.youtube.com/watch?v=UTMpM4orS30">Berkeley CS 285(Sergey Levine): Lecture 18, Variational Inference</a>, which I thinkorganized his lecture based on <ahref="https://arxiv.org/abs/1906.02691">An Introduction to VariationalAutoencoders</a> (2.1-2.7, and 2.9.1), or more in-depth on the author’sPhD thesis <a href="http://dpkingma.com/#phdthesis">VariationalInference and Deep Learning: A New Synthesis</a> I found this wonderfultutorial in <ahref="https://probml.github.io/pml-book/book2.html">ProbabilisticMachine Learning: Advanced Topics</a></p><p>Some graph come from <ahref="https://probml.github.io/pml-book/book1.html">ProbabilisticMachine Learning: An Introduction</a> itself and <ahref="https://arxiv.org/abs/1606.05908">Carl Doersch: Tutorial onVariational Autoencoders</a>, which is referenced in the previousbook.</p><p>Note though the <em>Probabilistic Machine Learning</em> book itselfis a horrible book with extremely confusing explanations.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;probabilistic-latent-variable-models&quot;&gt;Probabilistic Latent
Variable Models&lt;/h2&gt;
&lt;p&gt;The two general forms of probabilistic models are</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hyper-Parameter Tuning with Optuna</title>
    <link href="https://yao-lirong.github.io/2024-08-23-Hyper-Parameter-Tuning-with-Optuna/"/>
    <id>https://yao-lirong.github.io/2024-08-23-Hyper-Parameter-Tuning-with-Optuna/</id>
    <published>2024-08-23T04:00:00.000Z</published>
    <updated>2025-09-02T23:58:14.685Z</updated>
    
    <content type="html"><![CDATA[<p>After self-implementing a grid-search but having a horrible timewriting pyplot visualizing the result, I finally decided to find anexisting tool to do the HP tuning for me.</p><span id="more"></span><p>There are two popular HP tuning framework</p><ul><li><a href="https://docs.ray.io/en/latest/tune/index.html">RayTune</a>:almost industry standard</li><li><a href="https://optuna.org/">Optuna</a>: user friendly, requiresminimal modification to original code</li></ul><p>There’s also <ahref="https://github.com/skorch-dev/skorch">skorch</a> integratingscikit-learn and pytorch, so you can use <ahref="https://skorch.readthedocs.io/en/v1.0.0/user/quickstart.html#grid-search">sklearn<code>GridSearchCV</code></a>. For our simple task, we will go with<code>Optuna</code>.</p><h2 id="getting-started">Getting Started</h2><p>To get Optuna running, you just need to add 4 lines in your traininglogic and a few more lines to start its search. In training logic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">image_datasets, lr, weight_decay, num_epochs, trial : optuna.trial.Trial=<span class="literal">None</span></span>):</span></span><br><span class="line">    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">&quot;train&quot;</span>]:</span><br><span class="line"><span class="comment"># Training Logic</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">&quot;val&quot;</span>]:</span><br><span class="line">            running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># Eval Logic</span></span><br><span class="line">        epoch_loss = running_loss / dataset_sizes[<span class="string">&quot;val&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> epoch_acc &gt; best_acc <span class="keyword">or</span> (epoch_acc == best_acc <span class="keyword">and</span> epoch_loss &lt; best_loss):</span><br><span class="line">            best_acc, best_loss = epoch_acc, epoch_loss</span><br><span class="line">        <span class="string">&quot;&quot;&quot; OPTUNA CODE GOES HERE:</span></span><br><span class="line"><span class="string">            For each epoch, you should report value of a user-defined factor. </span></span><br><span class="line"><span class="string">            Optuna uses this factor alone to determine whether to prune out </span></span><br><span class="line"><span class="string">            this trial at current epoch step. Your objective value returned </span></span><br><span class="line"><span class="string">            has nothing to do with pruning.</span></span><br><span class="line"><span class="string">            Read for more at:  https://optuna.readthedocs.io/en/v3.6.1/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> trial <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            trial.report(epoch_loss, epoch)</span><br><span class="line">            <span class="keyword">if</span> trial.should_prune():</span><br><span class="line">                <span class="keyword">raise</span> optuna.exceptions.TrialPruned()</span><br><span class="line">    <span class="keyword">return</span> best_loss</span><br></pre></td></tr></table></figure><p>The following code shows how to set the search space and start thesearch.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optuna_objective</span>(<span class="params">trial : optuna.trial.Trial</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Define a custom objective function we want to optimize on. </span></span><br><span class="line"><span class="string">        This function returns value of the criteria you want to finally evaluate your model on. </span></span><br><span class="line"><span class="string">        i.e. how you compare different models. The best model should have the best value of this objective.</span></span><br><span class="line"><span class="string">        If you say the best model should have highest training accuracy at the last epoch, then return training accuracy at the last epoch here. In our example, we think the best model should have the best `best_loss`, where a model&#x27;s `best_loss` is this model&#x27;s lowest validation loss across all epochs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image_datasets = prepare_data()</span><br><span class="line">    lr = trial.suggest_float(<span class="string">&quot;lr&quot;</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br><span class="line">    weight_decay = trial.suggest_float(<span class="string">&quot;weight_decay&quot;</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br><span class="line">    loss = train_model(image_datasets, lr, weight_decay, <span class="number">15</span>, trial)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Create a study called `plant_144` where we minimize the objective passed in.</span></span><br><span class="line"><span class="string">    Start the search. The search ends when we finish 10 trials or spend 3 hours. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    study = optuna.create_study(</span><br><span class="line">        direction=<span class="string">&quot;minimize&quot;</span>,</span><br><span class="line">        study_name=<span class="string">&quot;plant_144&quot;</span>)</span><br><span class="line">    study.optimize(optuna_objective, n_trials=<span class="number">10</span>, timeout=<span class="number">3</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; Objective Value: &quot;</span>, study.best_trial.value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;  Params: &quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> study.best_trial.params.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;    <span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;value&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>The above example was adapted from <ahref="https://github.com/optuna/optuna-examples/blob/ecc3e4282161f3cece1dc26d95f4186e3905e497/pytorch/pytorch_simple.py">Optuna’sPyTorch starting example</a>. For more reporting printout statements,check the original example.</p><h2 id="saving-study-and-board-visualization">Saving Study and BoardVisualization</h2><p>In addition to printing out all the info to the console and losingthem from memory after this python script finishes, we can save them inthe form of an RDB (Relational Database, or just database as mostdatabases are RDB). To do this, we pass a <em>database URL</em> to the<code>storage</code> argument</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">study = optuna.create_study(</span><br><span class="line">    direction=<span class="string">&quot;minimize&quot;</span>,</span><br><span class="line">    study_name=<span class="string">&quot;plant_144&quot;</span>,</span><br><span class="line">    storage=<span class="string">&quot;sqlite:///db.sqlite3&quot;</span>)</span><br></pre></td></tr></table></figure><p>You can now Ctrl+C stop this search at anytime and resume it byrunning the same code again.</p><p>Database exposes itself as a server in machines. Therefore, to accessit (even in local machine), we use Database URL. Just like to access awebpage online, we use an HTTPS url. In our example here, the historywill be stored in a file called <code>db.sqlite3</code> under currentdirectory.</p><p>This file is a general database and can store study other than theone called <code>plant_144</code>. You can store another study insideit.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">study = optuna.create_study(</span><br><span class="line">    direction=<span class="string">&quot;maximize&quot;</span>,</span><br><span class="line">    study_name=<span class="string">&quot;plant_8&quot;</span>,</span><br><span class="line">    storage=<span class="string">&quot;sqlite:///db.sqlite3&quot;</span>)</span><br></pre></td></tr></table></figure><p>For me this code just worked without having to install SQLite DB.This is probably because it comes with my Ubuntu but I have no idea.Check official tutorial <ahref="https://optuna.readthedocs.io/en/v3.6.1/tutorial/20_recipes/001_rdb.html">Saving/ResumingStudy</a> for more on saving and loading.</p><p>You can now visualize the search history, each parameter’simportance, etc. with <ahref="https://github.com/optuna/optuna-dashboard">optuna-dashboard</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optuna-dashboard sqlite:///db.sqlite3</span><br></pre></td></tr></table></figure><figure><img src="./images/optuna-dashboard.png" alt="optuna-dashboard" /><figcaption aria-hidden="true">optuna-dashboard</figcaption></figure><h2 id="multi-gpu-parallelism-support">Multi-GPU ParallelismSupport</h2><p><a href="https://stackoverflow.com/a/62564488">roman’s Stack Overflowanswer</a> provides a very simple way to do multi-GPU tuning byutilizing Optuna’s resume feature. To do so, create a study by followingthe previous code. Then modify your code now to resume instead ofstarting a new study.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    study = optuna.load_study(study_name=<span class="string">&#x27;plant_144&#x27;</span>, storage=<span class="string">&#x27;sqlite:///db.sqlite3&#x27;</span>)</span><br><span class="line">    study.optimize(objective, n_trials=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>and simply start “resume” this study on different available GPUs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=3 nohup python optuna.py &gt; log3.txt 2&gt;&amp;1 &amp;</span><br><span class="line">CUDA_VISIBLE_DEVICES=5 nohup python optuna.py &gt; log5.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>The history from both processes will be stored under the study called<code>plant_144</code> in file <code>db.sqlite3</code>.</p><p>For more information on parallelizing on multiple gpu, check officialguide: <ahref="https://optuna.readthedocs.io/en/v3.6.1/tutorial/10_key_features/004_distributed.html">EasyParallelization</a></p><h2 id="some-complaints">Some Complaints</h2><p>In its visualization, Optuna doesn’t provide an option to filter outthe “bad” trial runs, making the scale of all graph ridiculous andusually of no information.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;After self-implementing a grid-search but having a horrible time
writing pyplot visualizing the result, I finally decided to find an
existing tool to do the HP tuning for me.&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>KV Cache</title>
    <link href="https://yao-lirong.github.io/2024-07-02-KV-Cache/"/>
    <id>https://yao-lirong.github.io/2024-07-02-KV-Cache/</id>
    <published>2024-07-02T04:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.123Z</updated>
    
    <content type="html"><![CDATA[<p>Before this, see <ahref="#2024/06/17-Conducting-Multi-Round-Conversation-with-Transformers">2024/06/17Conducting Multi-Round Conversation with Transformers</a> for why weneed cache. But we have query, key, value three matrices. Why do youonly cache past keys and values? How about past queries?</p><span id="more"></span><h2 id="attention-mechanism-in-detail">Attention Mechanism inDetail</h2>Recall the attention process in transformer can be written in thefollowing matrix form: <span class="math display">$$Z = \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$</span> If we look a particular output at position <spanclass="math inline"><em>i</em></span>, it can be written as: $$ z_i =({}<p>)</p><pre><code>\begin&#123;bmatrix&#125;v_1 \\v_2 \\\vdots  \\v_n\end&#123;bmatrix&#125;</code></pre><p>$$ A simple example can be found in the famous <ahref="https://jalammar.github.io/illustrated-transformer/">IllustratedTransformer</a></p><figure><img src="https://jalammar.github.io/images/t/self-attention-output.png"alt="self attention output" /><figcaption aria-hidden="true">self attention output</figcaption></figure><p>From the formula and the example, we can see that key and values arealways a pair in calculation. In fact, this is aligned with the veryconcept of soft dictionary behind attention: we get a query fromsomewhere and look at all the keys in the dictionaries to find, for eachkey, how much it relates to this query and output the weighted averageof each key’s value based on the relatedness.</p><h2 id="generative-transformer-decoder-based">Generative Transformer(Decoder Based)</h2><figure><imgsrc="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png"alt="Autoregressive Decoder" /><figcaption aria-hidden="true">Autoregressive Decoder</figcaption></figure><p>Let’s consider a causal language model, aka a transformer’sautoregressive generative decoder. At inference time, <strong>we onlycare about the output at the last position</strong> because the model isautoregressive and the outputs at all the previous positions are exactlythe same as our input. (See the above graph from blogpost <ahref="https://huggingface.co/blog/encoder-decoder">Transformers-basedEncoder-Decoder Models</a>) Therefore, if the current sequence haslength <span class="math inline"><em>s</em></span>, we only care about<span class="math inline"><em>z</em><sub><em>s</em></sub></span>. Allthe other outputs <spanclass="math inline"><em>z</em><sub>1…<em>s</em> − 1</sub></span> areuseless.</p><p><ahref="https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L188-L191">Inferencecode in Karpathy’s nanoGPT</a> corroborated this in its inference timeimplementation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span><br><span class="line">    logits = self.lm_head(x[:, [-<span class="number">1</span>], :]) <span class="comment"># note: using list [-1] to preserve the time dim</span></span><br><span class="line">    loss = <span class="literal">None</span></span><br></pre></td></tr></table></figure>Now revisit the formula to calculate the output <spanclass="math inline"><em>z</em><sub><em>s</em></sub></span>: $$ z_s =( {}<p>)</p><pre><code>\begin&#123;bmatrix&#125;v_1 \\v_2 \\\vdots  \\v_s\end&#123;bmatrix&#125;</code></pre><p>$$ It should be clear that to save computation, we only need to cachethe <code>kv</code> values in the previous positions and it’s useless tocache the <code>q</code> value.</p><p>To give a more detailed example, let’s consider the whole process togenerate a sequence of tokens with length <spanclass="math inline"><em>n</em></span>: <spanclass="math inline"><em>t</em><sub>1</sub>, …, <em>t</em><sub><em>n</em></sub></span>.We can see the previous queries are never used in the computation.<br />$$ $$</p><h2 id="time-complexity-boost">Time Complexity Boost</h2><p>People complain about the slow inference time of generativetransformer model, where it has a quadratic sequence length term <spanclass="math inline"><em>O</em>(<em>s</em><sup>2</sup>)</span>. Thisquadratic term is caused by <spanclass="math inline"><em>Q</em><em>K</em><sup><em>T</em></sup></span>matrix multiplication in attention where both matrices have shape <spanclass="math inline"><em>s</em> × <em>d</em></span>. Recall running timeof matmul <span class="math inline"><em>A</em><em>B</em></span> where<span class="math inline">$A \in \R^{m \times p}, B \in \R^{p \timesn}$</span> is <spanclass="math inline"><em>O</em>(<em>m</em><em>p</em><em>n</em>)</span>,so this matmul of query and key matrix has time complexity <spanclass="math inline"><em>O</em>(<em>s</em><sup>2</sup><em>d</em>)</span>.</p><p>However, by observing that we only need the output at the very lastposition in generative model and utilizing KV-cache, we reduce ourmatrix <span class="math inline">$Q \in \R^{s \times d}$</span> to asingle vector of <span class="math inline">$q \in \R^{1 \timesd}$</span> and effectively reduce the time complexity of this operationto <span class="math inline"><em>O</em>(<em>s</em><em>d</em>)</span>.Therefore, we can eliminate the quadratic term from our inference timeand only need linear time <span class="math inline"><em>s</em></span>instead.</p><h2 id="what-about-encoder-based-transformer-model">What about EncoderBased Transformer Model?</h2><p>Encoder Based transformer models do not have the issue of repeatedlycomputing the same past tokens’ attention scores so do not need aKV-cache.</p><h2 id="code-implementation">Code Implementation</h2><p>Facebook’s <ahref="https://github.com/facebookresearch/XLM">cross-lingual languagemodel (XLM)</a> gives a fantastic example of how to implement KV-Cache(or transformers in general, it provides abundant comments of tensorshape at each step).</p><ol type="1"><li><p>At inference time, do not recompute elements (where<code>slen</code> or a more descriptive naming can be<code>cached_sequence_length</code> is how many previous positions havebeen cached): <ahref="https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L373-L380">link</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    _slen = slen - cache[<span class="string">&#x27;slen&#x27;</span>]</span><br><span class="line">    x = x[:, -_slen:]</span><br><span class="line">    positions = positions[:, -_slen:]</span><br><span class="line">    <span class="keyword">if</span> langs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        langs = langs[:, -_slen:]</span><br><span class="line">    mask = mask[:, -_slen:]</span><br><span class="line">    attn_mask = attn_mask[:, -_slen:]</span><br></pre></td></tr></table></figure></li><li><p>Retrieve, use and update cache: <ahref="https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L199-L207">link1</a><ahref="https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L423">link2</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.layer_id <span class="keyword">in</span> cache:</span><br><span class="line">    k_, v_ = cache[self.layer_id]</span><br><span class="line">    k = torch.cat([k_, k], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br><span class="line">    v = torch.cat([v_, v], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br><span class="line">cache[self.layer_id] = (k, v)</span><br><span class="line">...</span><br><span class="line">cache[<span class="string">&#x27;slen&#x27;</span>] += tensor.size(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p>XLM can serve multiple purposes including as a generative causallanguage model, masked language model, or a translation language model.We use KV-Cache only with causal language model in<code>generate()</code> function, see <ahref="https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L482-L498">code</a>.</p><p>XLM has a <code>Memory</code> module that implements <ahref="https://github.com/facebookresearch/XLM#v-product-key-memory-layers-pkm">Product-KeyMemory Layers</a> whose mechanism rings very familiar to me but I can’trecall where I’ve encountered something similar before. Anyway, you canignore those <code>Memory</code> implementations and focus on theattention part if use it as a source to learn cache or the basics ofattention.</p><h2 id="more-code-examples">More Code Examples</h2><ul><li>This Medium post <ahref="https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8">KVcaching explained</a> leads way to where to find Hugging Face’simplementation in general, which can be too modular and abstractnowadays. It’s hidden in the <code>forward</code> function in<code>XXXForCausalLM</code>. Take <ahref="https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/llama2#transformers.LlamaForCausalLM"><code>LlamaForCausalLM</code></a>as an example, in its <code>forward</code> function, we still need to godown the abstraction to <code>LlamaModel</code> -&gt;<code>LlamaDecoderLayer</code> -&gt; <ahref="https://github.com/huggingface/transformers/blob/6c1d0b069de22d7ed8aa83f733c25045eea0585d/src/transformers/models/llama/modeling_llama.py#L337-L340"><code>LlamaAttention</code></a>and we can see the <code>past_key_value</code> there implementing the<code>Cache</code> class. I didn’t read into how Hugging Face didit.</li><li><a href="https://zhuanlan.zhihu.com/p/630832593">This Zhihu postexplaining KV-Cache</a> leads way to <ahref="https://github.com/huggingface/transformers/blob/d1a1bcf56aeb8593b9cc613b21422e6311875599/src/transformers/models/gpt2/modeling_gpt2.py#L318-L321">HuggingFace’s GPT-2</a>. The <ahref="https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L105-L108">originalGPT-2 code</a> is in fact more straightforward, but you’d better justread XLM. It simply has more comments and the naming is moreself-explanatory.</li></ul><h2 id="ps">PS</h2><p>I initially didn’t find where Hugging Face implemented KV-Cache incurrent version (<code>transformer 4.40</code>) but only this <ahref="https://github.com/huggingface/transformers/blob/aec1ca3a588bc6c65f7886e3d3eaa74901a6356f/src/transformers/cache_utils.py#L293"><code>Cache</code>class</a> and failed to find where it’s used. So I followed therecommendation under <ahref="https://zhuanlan.zhihu.com/p/601044938">this Zhihu post</a> to goto transformer 2.5.0 instead. A quick search like “kv” or “cache” led meto <ahref="https://github.com/huggingface/transformers/blob/v2.5.0/src/transformers/modeling_xlm.py"><code>modeling_xlm.py</code></a>.I was surprised to find early Hugging Face model code was more of arename of original implementation instead of a refactor they do now.</p><p>I then read this <ahref="https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8">KVcaching explained</a> post. Its graph isn’t super straightforward but itintroduces how KV-cache reduces time complexity and where to findHugging Face’s implementation.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Before this, see &lt;a
href=&quot;#2024/06/17-Conducting-Multi-Round-Conversation-with-Transformers&quot;&gt;2024/06/17
Conducting Multi-Round Conversation with Transformers&lt;/a&gt; for why we
need cache. But we have query, key, value three matrices. Why do you
only cache past keys and values? How about past queries?&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Conducting Multi-Round Conversation with Transformers</title>
    <link href="https://yao-lirong.github.io/2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/"/>
    <id>https://yao-lirong.github.io/2024-06-17-Conducting-Multi-Round-Conversation-with-Transformers/</id>
    <published>2024-06-17T04:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.124Z</updated>
    
    <content type="html"><![CDATA[<p>I was using LLaVA to query in an image how many characters there are.For higher accuracy, I decided to employ Chain of Thought, but struggledto implement it. CoT is conducted through a multiple round conversation.It is easily done in a graphical chat interface but how is it doneinternally with code?</p><span id="more"></span><h2 id="token-level">Token Level</h2><p>Before diving into instruct / chat model, let’s go to the lowestlevel and think how transformers do generation. Transformer is anautoregressive model: it uses its own output as input for the nextround. Looking at <ahref="https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L328">nanoGPT’s<code>generate</code> function</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br><span class="line"><span class="string">    the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= self.config.block_size <span class="keyword">else</span> idx[:, -self.config.block_size:]</span><br><span class="line">        logits, _ = self(idx_cond)</span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br><span class="line">        probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure><p>If we ignore the details, this for loop is effectively doing:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">token0 = tokenizer(text)</span><br><span class="line">output1 = model(token0)</span><br><span class="line"></span><br><span class="line">token1 = get_resposne(output1) </span><br><span class="line">output2 = model(token0 + token1)</span><br><span class="line"></span><br><span class="line">token2 = get_resposne(output2)</span><br><span class="line">output3 = model(token0 + token1 + token2)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>By writing it out like this, it’s clear that each turn of generation,we feed the previous step input into the model as something new, thoughexactly the same. Therefore, when we call<code>model(token0 + token1)</code>, <strong>we forgot about all theattention we calculated in <code>model(token0)</code></strong> eventhough the attention for <code>token0</code> part is actually completelythe same. This is why people complain transformer inference is slow andthis is where the inference speed-up techniques like KV-cache comesin.</p><p>This also reveals that the very popular graph demonstrating thetheory behind transformer’s inference lied (at least to me). Whencalculate <spanclass="math inline"><em>y</em><sub><em>i</em> + 1</sub></span>, we donot re-use <spanclass="math inline"><em>y</em><sub>0</sub>…<em>y</em><sub><em>i</em></sub></span>or the attention or the activations in the middle. We just re-feed themback into the model as something completely new.</p><figure><imgsrc="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png"alt="Autoregressive Decoder" /><figcaption aria-hidden="true">Autoregressive Decoder</figcaption></figure><h2 id="conversation-level">Conversation Level</h2><p>Chat model is also just a text continuation model except it follows achat template distinguishing which texts are inputted by the user andwhich are generated by the assistant. In the lowest abstraction level -the token level, for each turn, the model outputs one token and usesthat as part of the input in next turn’s generation. One abstractionlevel higher to this conversation level, to do multiple-roundconversation, a chat model similarly outputs one response to one user’sinput and uses that response as a part of the input for next turn’sgeneration. Therefore, to conduct conversation with a chat model, wejust append the model’s response at each turn to its correspondinginput.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input1 = tokenizer(text1)</span><br><span class="line">output1 = model(input1)</span><br><span class="line"><span class="comment"># output1 contains input1 and model&#x27;s response 1</span></span><br><span class="line">response1 = get_resposne(output1) </span><br><span class="line">input2 = tokenizer(text2)</span><br><span class="line">output2 = model(input1 + response1 + input2)</span><br></pre></td></tr></table></figure><p>And yes, this means to get <code>output2</code>, we feed<code>input1 + response1</code> both as new to the model, but thisshouldn’t be a concern anymore since we feed each token as newanyway.</p><h2 id="get_response"><code>get_response</code></h2><p>The question now comes to how we should implement<code>get_response</code> to extract the assistant’s response from thetext-continuation model’s output.</p><ul><li><p>Find the indicator (prefix) of the start of assistant’s message:Note when the model doesn’t follow the instruction and failed togenerate such a prefix, this method fails.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prefix = <span class="string">&quot;\[/INST\]&quot;</span> <span class="comment"># escape special characters for regex</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">answer_idx = [m.end() <span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(prefix, detoked_output)][-<span class="number">1</span>]</span><br><span class="line">answer = detoked_output[answer_idx:]</span><br></pre></td></tr></table></figure></li><li><p><strong>recommended</strong> - Get the substring that is afterthe input (prompt): Hugging Face uses this approach in their <ahref="https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387">TextGenerationPipeline</a>.There’s a <code>clean_up_tokenization_spaces</code> variable in<code>decode</code> function which defaults to <code>False</code>. (Forwhat it does, see <ahref="https://discuss.huggingface.co/t/what-does-the-parameter-clean-up-tokenization-spaces-do-in-the-tokenizer-decode-function/17399">thisdiscussion</a>) Hugging Face set it to <code>True</code> in both call,but I tried set both to <code>False</code> or one to <code>True</code>the other to <code>False</code>, either can give correct results. Thatsaid, it’s still best to follow what Hugging Face wrote. After all theyknow their codes best.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>, </span><br><span class="line">                                  clean_up_tokenization_spaces=<span class="literal">True</span>)</span><br><span class="line">cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br><span class="line">                    inputs[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>],</span><br><span class="line">                    skip_special_tokens=<span class="literal">True</span>,</span><br><span class="line">                    clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br><span class="line">                ))</span><br><span class="line">answer = detoked_output[cutoff:]</span><br></pre></td></tr></table></figure></li></ul><h2 id="detours-when-taking-the-recommended-approach">Detours whenTaking the Recommended Approach</h2><p>I had some trouble with this recommended approach at first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">chat = [</span><br><span class="line">   &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;&lt;image&gt;\nHow many animated characters are there in this image?&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">inputs = processor(prompt, image, return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line">...</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">cutoff = <span class="built_in">len</span>(prompt)</span><br></pre></td></tr></table></figure><p>And <code>cutoff</code> is actually many indexes after the realstarting point of assistant’s response. That is because when we<code>apply_chat_template</code>, we added some special tokens<code>&lt;s&gt; &lt;\s&gt;</code> to indicate the start and end of oneturn of conversation with the assistant, but when we detokenize theoutput, we <code>skip_special_tokens</code> to get the response only andcaused this discrepancy.</p><p>I thought at first that this discrepancy comes from LLaVA replaced<code>&lt;image&gt;</code> token with the image embeddings (or<code>pixel_values</code> as Hugging Face calls it) because<code>&lt;image&gt;</code> also disappeared in the<code>detoked_output</code>. However, after reading LLaVA’s paper: <ahref="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a>Figure 1: LLaVA network architecture, I realized LLaVA actually puts theimage in front of the text input instead of inserting it in themiddle.</p><figure><img src="https://arxiv.org/html/2304.08485v2/x1.png"alt="LLaVA architecture" /><figcaption aria-hidden="true">LLaVA architecture</figcaption></figure><p>And <code>&lt;image&gt;</code> disappeared because it’s also aspecial token. However it was not inside the<code>tokenizer.all_special_tokens</code>. Reading the source code oftokenizer, I’m actually not sure how it was added as a special token sowas not able to debug why it’s not in <code>all_special_tokens</code>.For this specific behavior, I submitted <ahref="https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192">anissue on Hugging Face forum</a>.</p><p>You can find chat template definition in<code>tokenizer_config.json -&gt; "chat_template"</code>. Also in thisfile, <code>"added_tokens_decoder"</code> attribute defines<code>&lt;image&gt;</code> as a special token.</p><h2 id="the-complete-code">The Complete Code</h2><p>I referenced Hugging Face conversation pipeline for <ahref="https://huggingface.co/docs/transformers/main/conversations#what-happens-inside-the-pipeline">thegeneral structure</a> and <ahref="https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387">theresponse extractor</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">queries = [</span><br><span class="line">   <span class="string">&quot;&lt;image&gt;\nHow many animated characters are there in this image?&quot;</span>,</span><br><span class="line">   <span class="string">&quot;Answer with a single number in decimal format. Give no explanations.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_response</span>(<span class="params">image</span>):</span></span><br><span class="line">    chat = []</span><br><span class="line">    <span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">        chat.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: query&#125;)</span><br><span class="line">        prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">        inputs = processor(prompt, image, return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line">        output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        input_ids = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br><span class="line">                            input_ids[<span class="number">0</span>],</span><br><span class="line">                            skip_special_tokens=<span class="literal">True</span>,</span><br><span class="line">                            clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br><span class="line">                        ))</span><br><span class="line">        answer = output[cutoff:]</span><br><span class="line">        chat.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: answer&#125;)</span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><h2 id="ps">PS</h2><p>As written at the start of this blogpost, it all began from me tryingto do multi-round conversation with a transformer. A web search took meto these discussions (<ahref="https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/19">link1</a>, <a href="https://github.com/salesforce/LAVIS/issues/357">link2</a>). It’s obvious <a href="#Conversation-Level">this acceptedapproach</a> of appending output to previous message causes great wasteof computing resources, which made me realize how transform works <ahref="#Token-Level">internally at the lowest level</a> is itself a wasteof resources.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I was using LLaVA to query in an image how many characters there are.
For higher accuracy, I decided to employ Chain of Thought, but struggled
to implement it. CoT is conducted through a multiple round conversation.
It is easily done in a graphical chat interface but how is it done
internally with code?&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>GPT-4o Release</title>
    <link href="https://yao-lirong.github.io/2024-05-14-GPT-4o-Release/"/>
    <id>https://yao-lirong.github.io/2024-05-14-GPT-4o-Release/</id>
    <published>2024-05-14T04:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.133Z</updated>
    
    <content type="html"><![CDATA[<p>One day before Google I/O, OpenAI made a <ahref="https://openai.com/index/spring-update/">Spring UpdateRelease</a>, introducing multi-modal end-to-end model <ahref="https://openai.com/index/hello-gpt-4o/">GPT4-o</a></p><span id="more"></span><h2 id="capabilities-and-features">Capabilities and Features</h2><p>In their <ahref="https://www.youtube.com/watch?v=DQacCB9tDaw">release live</a>, wesee</p><ul><li>Real-time responsiveness in audio mode, ability to beinterruptted</li><li>Detect tone and mood in your speech, including how hard youbreath</li><li>Real-time responsiveness in vision mode: no need to take a picture,just hold your phone’s camera there and it can screenshot(?) foryou</li></ul><p>Right after the live, OpenAI <ahref="https://openai.com/index/hello-gpt-4o/">updated their blog</a>,showing more demos:</p><ul><li><a href="https://vimeo.com/945587746">Two GPT-4os harmonizing</a>:on the same device same session. They can sing and harmonize. They canfollow user’s instruction to sing faster, sing slower, or sing in ahigher voice.</li><li><a href="https://vimeo.com/945587944">Lullaby</a>: user can giveinstruction by speech to tell GPT-4o to go lighter, louder, …</li><li><a href="https://vimeo.com/945587927">Taking faster</a>: user cangive instruction by speech to tell GPT-4o to speak faster, slower</li></ul><p><a href="https://vimeo.com/945591584">Failure cases</a>: itsometimes</p><ul><li>go wild and speak in another language</li><li>fail in translation tasks</li><li>fail in teaching intonation in Chinese</li></ul><h2 id="technicality">Technicality</h2><ul><li>GPT-4o is a single new model <strong>end-to-end across text, vision,and audio</strong>, meaning that all inputs and outputs are processed bythe same neural network. Previously, ChatGPT Voice Mode is a pipeline ofthree separate models: audio-text transcription, GPT-3.5/GPT-4 textmodel, text-to-audio conversion model</li><li>Designed a new tokenizer with greater compression: Hindi has 2.9xfewer tokens, Russian 1.7x, Korean 1.7x, Chinese 1.4x, Japanese 1.4x,and European languages, including English, has 1.1x fewer tokens. Newtokenizer means fully new pre-trained model (brought up in <ahref="https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/comment/l3ww1y7/">thisreddit thread</a>)</li><li>It is super fast, responding to audio inputs with an average of 320milliseconds, while original ChatGPT Voice Mode has latencies of 2.8seconds (GPT-3.5) and 5.4 seconds (GPT-4) on average. At the same time,GPT-4o “achieves GPT-4 Turbo-level performance on text, reasoning, andcoding intelligence.” What did they do to speed up inference? Is itQuantization, MoE or something else? (brought up in <ahref="https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/comment/l3wqmit/">thisreddit thread</a>) What’s the model size? Nothing is reported.</li></ul><h2 id="inspecting-the-new-tokenizer">Inspecting the New Tokenizer</h2><p>When I used reddit on the day GPT-4o released, <ahref="https://www.reddit.com/r/real_China_irl/comments/1crvv4m/openai%E7%AE%80%E5%8D%95%E8%BE%B1%E4%B8%AA%E5%8D%8E/">thispost</a> came to me suggesting Chinese tokens in OpenAI’s new tokenizerare greatly contaminated.</p><p>The new tokenizer <code>o200k_base</code> is actually twice as largeas the last <code>cl100k_base</code> and has already been loaded toGitHub in <ahref="https://github.com/openai/tiktoken/commit/9d01e5670ff50eb74cdb96406c7f3d9add0ae2f8">thiscommit</a>.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;One day before Google I/O, OpenAI made a &lt;a
href=&quot;https://openai.com/index/spring-update/&quot;&gt;Spring Update
Release&lt;/a&gt;, introducing multi-modal end-to-end model &lt;a
href=&quot;https://openai.com/index/hello-gpt-4o/&quot;&gt;GPT4-o&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>CLIP</title>
    <link href="https://yao-lirong.github.io/2024-04-22-CLIP/"/>
    <id>https://yao-lirong.github.io/2024-04-22-CLIP/</id>
    <published>2024-04-22T04:00:00.000Z</published>
    <updated>2025-09-02T23:58:14.681Z</updated>
    
    <content type="html"><![CDATA[<p>CLIP investigates whether it is possible to transfer the success oftask-agnostic web-scale pre-training in NLP to another domain (CV).</p><span id="more"></span><blockquote><p>This line of work represents the current pragmatic middle groundbetween learning from a limited amount of supervised “gold-labels” andlearning from practically unlimited amounts of raw text.</p></blockquote><h2 id="approach">2 Approach</h2><h3 id="advantage-of-natural-language-supervision">2.1 Advantage ofNatural Language Supervision</h3><ul><li>easy to scale: natural language data amount is huge, much easier toobtain than crowd-sourced labeling</li><li>flexible zero-shot transfer: connects image representation tolanguage; different from unsupervised or self-supervised model that islimited to image domain.</li></ul><h3 id="constructing-dataset">2.2 Constructing Dataset</h3><p>To explore effects of web-scale pre-training, we first build aweb-scale dataset.</p><ol type="1"><li>Construct a query list of size 500,000 that contains words occurred&gt;= 100 times in Wikipedia</li><li>Search for images of these queries, construct a dataset of 400M(image, text) pair</li><li><strong>Class balance</strong> (yeah that’s the word describing“make each class have the same number of samples so it’s fair”) byincluding 20,000 pairs per query</li></ol><h3 id="what-to-predict-what-is-the-loss">2.3 What to Predict? What isthe Loss?</h3><p>Previous methods with natural language supervision attempt is aboutpredicting a bag of words (BoW) / phrase n-gram representation oflabels. The authors explore different approaches. This work is all aboutlarge scale pre-training and <strong>scaling</strong>. <strong>Trainingefficiency</strong> is the key to scaling natural language supervision.Authors selected final pre-training method based on efficiency. Theycompared three approaches:</p><ol type="1"><li><strong>Transformer language model (captioning model)</strong>:train a transformer to predict the caption of an image. So this is agenerative task and uses transformer’s loss function. It learns 3 timesslower than the baseline - approach 2.</li><li><strong>A model predicts BoW encoding of the caption</strong>: thiswas used as a simple baseline and authors found approach 1 couldn’t evenbeat this baseline. This approach still tries to <strong>predict theexact words</strong> of the text label, but the order of how wordsappear no longer matters. This is not much easier due to the widevariety of descriptions, comments, and related text that co-occur withimages.</li><li><strong>A contrastive model predicts which text <em>as a whole</em>is paired with which image</strong>: In this way, we decrease the outputspace to only the number of classes we have. We learn 4 times fasterthan the baseline - approach 2.</li></ol><figure><img src="./images/CLIP_2.png" alt="Accuracy vs #(images processed)" /><figcaption aria-hidden="true">Accuracy vs #(imagesprocessed)</figcaption></figure><p>See Figure 2 for a detailed comparison on <em>accuracy vs. #(imagesfed)</em> of these three models. This illustrates how fast / slow atraining method learns.</p><table><colgroup><col style="width: 18%" /><col style="width: 42%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th>Approach</th><th>Output Space</th><th>Answer Space: In ideal scenario, what do we choose from?</th></tr></thead><tbody><tr class="odd"><td>Transformer Language Model</td><td>All English sentences (permutation of all English words)</td><td>500K queries</td></tr><tr class="even"><td>BoW prediction model</td><td>Word count bucket of all English sentences (combination of allEnglish words)</td><td>500K queries</td></tr><tr class="odd"><td>Contrastive pairing model</td><td>Sentences describing class and labels</td><td><code>batch_size</code> pre-selected queries (32768 in CLIP)</td></tr></tbody></table><p>It’s worth noting that CLIP uses a very large minibatch size of <spanclass="math inline">2<sup>15</sup> = 32768</span></p><h3 id="model-architecture-and-scaling">2.4 Model Architecture andScaling</h3><figure><img src="./images/CLIP_1.png" alt="Summary of CLIP" /><figcaption aria-hidden="true">Summary of CLIP</figcaption></figure><p>Image encoder has two architectures: ResNet-50 and ViT</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure><p>Note:</p><ol type="1"><li><code>d_e</code> represents multi-modal embedding space.</li><li>the temperature parameter <spanclass="math inline"><em>τ</em></span> is directly optimized as alog-parameterized multiplicative scalar to avoid turning as ahyper-parameter. <ahref="https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/model.py#L367-L368">implementationin original release</a></li></ol><p>The authors train CLIP from scratch without initializing the imageencoder with ImageNet weights or the text encoder with pre-trainedweights.</p><p>This section also describes how to scale the text encoder and how toscale both kinds of image encoder.</p><h2 id="experiments">3 Experiments</h2><p>Authors conducted experiments on 36 different datasets.</p><h3 id="zero-shot-transfer">3.1 Zero-Shot Transfer</h3><p>Authors wanted to experiment on zero-shot transfer ability because ofthe ability demonstrated in language models. The following is the mostexciting sentence to me in this paper. I think it explains a lot oflarge-scale design choices by OpenAI team. Did this paper inspire Ilyato go all the way down the path of scaling?</p><blockquote><p>Our focus on studying zero-shot transfer as an evaluation of tasklearning is inspired by work demonstrating task learning in the field ofNLP. To our knowledge Liu et al. (2018) first identified task learningas an “unexpected side-effect” when a language model trained to generateWikipedia articles learned to reliably transliterate names betweenlanguages.</p></blockquote><p>Authors explain in detail how we do zero-shot classification and givean interpretation to the pipeline. I wrote the previous “output space”and “answer space” thing based on this interpretation.</p><blockquote><p>The cosine similarity of these embeddings is then calculated, scaledby a temperature parameter <span class="math inline"><em>τ</em></span> ,and normalized into a probability distribution via a softmax. Note thatthis prediction layer is a multinomial logistic regression classifierwith L2-normalized inputs, L2-normalized weights, no bias, andtemperature scaling. When interpreted this way, the image encoder is thecomputer vision backbone which computes a feature representation for theimage and the text encoder is a hypernetwork which generates the weightsof a linear classifier based on the text specifying the visual conceptsthat the classes represent. Continuing with this interpretation, everystep of CLIP pre-training can be viewed as optimizing the performance ofa randomly created proxy to a computer vision dataset which contains 1example per class and has 32,768 total classes defined via naturallanguage descriptions.</p></blockquote><p><strong>prompt engineering and ensembling </strong></p><p>Text in our training data is usually a sentence, but text in testdata is just a one word label. To bridge this gap, we use some prompttemplate.</p><ul><li>default: <code>A photo of a &#123;label&#125;</code></li><li>on several fine-grained image classification datasets, it’s helpfulto specify the category:<code>A photo of a &#123;label&#125;, a type of pet</code> or<code>a satellite photo of a &#123;label&#125;</code></li><li>ensembling several different prompts improve performance: usedifferent context prompts such as <code>A photo of a big &#123;label&#125;</code>and <code>A photo of a small &#123;label&#125;</code>. Authors construct theensemble over the embedding space instead of probability space. In thisway, they cache a single set of averaged text embedding so compute costdoesn’t increase in amortized time.</li></ul><p><strong>scaling law</strong></p><figure><img src="./images/CLIP_9.png"alt="Zero-shot CLIP scales wrt model compute" /><figcaption aria-hidden="true">Zero-shot CLIP scales wrt modelcompute</figcaption></figure><p>Scaling law is the law that empirically shows that performance ispredictable as a function of important quantities such as trainingcompute and dataset size.</p><p>On 36 different datasets, ResNet CLIP’s average zero-shot error iswell modeled by a log-log linear scaling trend. However, performance onindividual evaluations is much more varied despite the smooth overalltrend. Authors did not report ViT CLIP scaling results.</p><h3 id="representation-learning">3.2 Representation Learning</h3><p>To use CLIP as a representation of the image, there are two commonapproaches:</p><ul><li>Fitting a linear classifier on a representation extracted from themodel</li><li>End-to-end fine-tuning of the model.</li></ul><p>Fine-tuning increases flexibility, and prior work has convincinglydemonstrated that fine-tuning outperforms linear classification on mostimage classification datasets. However, OpenAI chooses to use linearclassifier to measure CLIP performance for the following reasons:</p><ul><li><p>the more official reason: we chose it because it’s weak andtherefore better shows how dataset-agnostic CLIP is</p><blockquote><p>Our work is focused on developing a high-performing task anddataset-agnostic pre-training approach. Fine-tuning, because it adaptsrepresentations to each dataset during the fine-tuning phase, cancompensate for and potentially mask failures to learn general and robustrepresentations during the pre-training phase. Linear classifiers,because of their limited flexibility, instead highlight these failuresand provide clear feedback during development</p></blockquote></li><li><p>the more practical reason:</p><blockquote><p>Fine-tuning opens up a much larger design and hyper-parameter space,which makes it difficult to fairly evaluate and computationallyexpensive. By comparison, linear classifiers require minimalhyper-parameter tuning and have standardized implementations andevaluation procedures.</p></blockquote></li><li><p>bonus reason:</p><blockquote><p>Linear classifier has the added benefit of being very similar to theapproach used for its zero-shot classifiers which enables extensivecomparisons and analysis</p></blockquote></li></ul><p><strong>approach</strong>: Appendix A.3 provides a full guideline oftraining such a linear classifier, including details on hyper-parametersearch, solver method, and train-valid-test split. Notably, the input tothe Logistic Regression is the image embedding (output of the imageencoder <code>I_f</code>), not the multi-modal embedding (imageembedding that went through the multi-modal linear projection)</p><p><strong>results</strong>: when comparing to other models of similarcompute requirement, small CLIP have wins and loses. However, CLIPscales very well and the largest model achieves both SOTA score andcompute efficiency.</p><p><strong>ViT vs ResNet</strong>: The authors found CLIP ViT is about3x more compute efficient than CLIP ResNet. This is aligned with ViTpaper’s finding</p><p><strong>Out-of-Domain Performance and Natural DistributionShift</strong>: Researchers often find models exceeding human onImageNet test set can still make simple mistakes on other test data andscore much lower than human. A common explanation is these models areadept at finding patterns within dataset, so improve in-distributionperformance. However many of these patterns are spurious and do not holdfor other distributions and result in large drops in performance onother datasets.</p><p>Most of the studies that reach the above explanation limited theirevaluation model to those trained on ImageNet. Therefore, the authorswant to know to what degree are these failures attributable to deeplearning, ImageNet, or some combination of the two? They explore this byevaluating ImageNet models on natural distribution shifted dataset.</p><p>Natural distribution shift means testing trained models on data thatis different in e.g. image style, image blurriness, geographic location,and camera operation (<em>Hendrycks et al. The many faces ofrobustness</em>). “Natural” is used to make a distinction from syntheticdistribution shift made through style-transferred or adversariallygenerated.</p><ol type="1"><li>Authors found CLIP perform much better on these natural distributionshifted dataset.</li><li>However, this doesn’t necessarily mean supervised learning onImageNet causes a robustness gap. Other details of CLIP, such as itslarge and diverse pre-training dataset or use of natural languagesupervision could also produce robust models.</li><li>Therefore, OpenAI measured how the performance of CLIP models changeafter adapting to the ImageNet distribution via an L2 regularizedlogistic regression classifier fit to CLIP features on the ImageNettraining set. This improved accuracy on ImageNet by 9.2% to 85.4%, butaverage accuracy under distribution shift slightly decreases.</li></ol><p>To me this doesn’t say much. If you fine-tune (or fit a linearclassifier) to a specific dataset, of course you’d expect its behaviorto be bad on some other dataset. But on the contrary, thesenatural-distribution-shifted dataset is not that different fromImageNet. Yes, there are some animations / sketches, but most are justsome more pictures of that class. And CLIP with an ImageNet linear headcannot get them right. I guess what the authors want to say is thatImageNet is not just A arbitrary dataset, but has almost become amachine learning benchmark dataset. It is supposed to be general becauseall models train on it and these models will be deployed to all sorts ofscenario.</p><p>The authors didn’t go far to attack the generality of ImageNet oreven draw any conclusion on why fitting an ImageNet classification headhurts natural distribution shift performance. The authors just prompt tocaution that though prior work has also pre-trained models ondistributions other than ImageNet, it is common to study and releasemodels only after they have been fine-tuned to ImageNet. And it would bewise to also study the models pre-trained on distributions other thanImageNet.</p><p><strong>Results:</strong> Taken together, these results suggest thatthe recent shift towards large-scale task and dataset agnosticpre-training combined with a reorientation towards zero-shot andfew-shot benchmarking on broad evaluation suites promotes thedevelopment of more robust systems and provides a more accurateassessment of performance.</p><h2 id="data-overlap-analysis">5 Data Overlap Analysis</h2><p>A concern with pre-training on a very large internet dataset isunintentional overlap with downstream evals. One option to prevent thisis to identify and remove all duplicates before training a model. Whilethis guarantees reporting true hold-out performance, it requires knowingall possible data which a model might be evaluated on ahead of time.This has the downside of limiting the scope of benchmarking andanalysis.</p><p>Therefore, OpenAI instead built a duplicate detector, document howmuch overlap occurs, and run experiments on dataset with and withoutthese overlaps to measure how performance changes due to these overlaps.So instead of simply removing them, they record performance of beforeand after removing them.</p><p>They found that there is a median overlap of 2.2% and an averageoverlap of 3.2%. Due to this small amount of overlap, overall accuracyis rarely shifted by more than 0.1% with only 7 datasets above thisthreshold.</p><p>It would be useful if OpenAI also releases their duplicate detectormodel. Appendix C discusses it in more details but it doesn’t seem likeOpenAI ever released it.</p><h2 id="limitations">6 Limitations</h2><p><strong>Performance</strong>:</p><ol type="1"><li>CLIP cannot beat dataset-specific trained &amp; designed models:CLIP zero-shot performs better than a pre-trained ResNet-50 feature + alinear classifier, but on most datasets, CLIP is well below the SOTA forthat specific dataset.</li><li>zero-shot CLIP still generalizes poorly to data that is trulyout-of-distribution for it: CLIP simply has a super large domain, notreally a general model. For example, MNIST digits are not at all in itsweb-scraped huge dataset, so CLIP does surprisingly bad on this supersimple dataset.</li><li>CLIP is limited to “choosing”: CLIP cannot just take in a pictureand spit out its class. You need to give CLIP a range to choose from.CLIP is based on “choosing”, not “generating” (image captioningmodel)</li></ol><p><strong>Training Methodology</strong>:</p><ol type="1"><li>In training time, CLIP repeatedly queried performance on fullvalidation sets to guide optimization. These validation sets often havethousands of examples, which is unrealistic for true zero-shotscenarios. On the contrary, LLM in training time doesn’t do this(?)</li><li>Training dataset comes from Internet. Its image-text pairs areunfiltered and uncurated and result in CLIP models learning many socialbiases.</li></ol><p><strong>Supervision with Natural Language</strong>:</p><ol type="1"><li>Many complex tasks and visual concepts can be difficult to specifyjust through text.</li><li>Actual training examples are undeniably useful but CLIP does notoptimize for few-shot performance directly. In our work, we fall back tofitting linear classifiers on top of CLIP’s features. This results in acounter-intuitive drop in performance when transitioning from azero-shot to a few-shot setting.</li></ol><h2 id="broader-impacts">7 Broader Impacts</h2><p>In this section, the authors mainly introduces the bias exists inCLIP and what kind of surveillance it can be used for.</p><p>Nothing too interesting, but they discussed how tweaking the categorysystem can improve model’s performance. This reminds me of what I did inXiaomi’s oversea app store tagging project, where I added new categoryand modified existing category’s definition to improve thecos-similarity based zero-shot classification model performance.</p><blockquote><p>Given that we observed that people under 20 were the most likely tobe classified in both the crime-related and non-human animal categories,we carried out classification for the images with the same classes butwith an additional category ‘child’ added to the categories. We foundthat this drastically reduced the number of images of people under 20classified in either crime-related categories or non-human animalcategories (Table 7). This points to how class design has the potentialto be a key factor determining both the model performance and theunwanted biases or behavior the model may exhibit</p></blockquote><p>The authors then go on to conclude that</p><blockquote><p>Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest</p></blockquote><h2 id="takeaways">Takeaways</h2><ul><li><p>Data is still the king in ML. It is possible to transfer thesuccess of task-agnostic web-scale pre-training in NLP to CV.</p></li><li><p>The key to scaling &amp; training efficiency is how compact youroutput space is (word permutation - &gt; word combination -&gt;<code>batch_size</code>)</p></li><li><p>We can use prompt ensembling to improve CLIP’sperformance.</p></li><li><p>To use CLIP as the feature extractor and put a linear classifieron top of it, we use the image embedding (image encoder’s output), nothe multi-modal embedding (image embedding went through the multi-modallinear projection);</p><p>On the other hand, for zero-shot classification, you use multi-modalembedding, the same as the training process except now you only have oneimage and calculate the cos similarity with all class names.</p></li><li><p>Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;CLIP investigates whether it is possible to transfer the success of
task-agnostic web-scale pre-training in NLP to another domain (CV).&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Gradient Scaling</title>
    <link href="https://yao-lirong.github.io/2024-04-08-Gradient-Scaling/"/>
    <id>https://yao-lirong.github.io/2024-04-08-Gradient-Scaling/</id>
    <published>2024-04-08T04:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.133Z</updated>
    
    <content type="html"><![CDATA[<p>Loss Scaling / Gradient Scaling was mentioned in <ahref="#2024/03/01-Mixed-Precision-Training">Mixed-Precision Training</a>as one of the 3 techniques, but there are many points to be careful withwhen in practice.</p><span id="more"></span><h2 id="overview-typical-use-case">Overview: Typical Use Case</h2><p>Here’s an overview of how to use <code>amp.GradScaler</code> adaptedfrom <ahref="https://pytorch.org/docs/stable/amp.html#gradient-scaling">PyTorchofficial doc</a>.</p><h3 id="background">Background</h3><p>If the forward pass for a particular op has <code>float16</code>inputs, under <ahref="https://pytorch.org/docs/stable/amp.html">Automatic MixedPrecision package - torch.amp</a>, the backward pass for that op willproduce gradients of the same data type - <code>float16</code> .Gradient values with small magnitudes may not be representable in<code>float16</code>. These values will flush to zero (“underflow”), sothe update for the corresponding parameters will be lost.</p><h3 id="code">Code</h3><ol type="1"><li><code>scaler.scale(loss).backward()</code>: To prevent underflow,“gradient scaling’ multiplies the network’s loss(es) by a scale factorand invokes a backward pass on the scaled loss(es). In this way, thegradients on all parameters are scaled by this same factor and we don’thave to worry about them flush to zero. <code>scaler.scale(loss)</code>multiplies a given loss by <code>scaler</code>’s current scale factor.We then call backward on this scaled loss.</li><li><code>scaler.step(optimizer)</code>: After back-propagation, alllearnable parameters get their gradients, which are scaled to preventunderflow. Before applying whatever learning algorithm (Adam, SGD, …) onthem, we have to unscale them so the amount to be updated is correct.<code>scaler.step(optimizer)</code> 1. unscales gradients, 2. calls<code>optimizer.step()</code>, and does the previous two points safely:<ol type="1"><li>Internally invokes <code>unscale_(optimizer)</code> (unless<code>unscale_()</code> was explicitly called for <code>optimizer</code>earlier in the iteration). As part of the <code>unscale_()</code>,gradients are checked for infs/NaNs to prevent overflow/underflow (Forwhy overflow can happen, check point 3 <code>scaler.update</code>)</li><li>If no inf/NaN gradients are found, invokes<code>optimizer.step()</code> using the unscaled gradients. Otherwise,<code>optimizer.step()</code> is skipped to avoid corrupting theparams.</li></ol></li><li><code>scaler.update()</code>: It would be great if we could justmultiply all gradients by a super big number so absolutely no underflowhappens, but doing so can cause overflow. The scaler estimates a goodscaling factor for each iteration, so neither underflow nor overflowhappens. <code>scaler.update()</code> updates <code>scaler</code>’sscale factor for next iteration.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scaler = torch.cuda.amp.GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure><h2 id="working-with-unscaled-gradients---gradient-clipping"><ahref="https://pytorch.org/docs/stable/notes/amp_examples.html#id4">Workingwith Unscaled Gradients - Gradient clipping</a></h2><p>gradient clipping manipulates a set of gradients such that theirglobal norm <ahref="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_"><code>torch.nn.utils.clip_grad_norm_()</code></a>or maximum magnitude <ahref="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_"><code>torch.nn.utils.clip_grad_value_()</code></a>is &lt;= some user-imposed threshold.</p><p>The “gradients” here of course refer to the original, unscaledgradients. Therefore, you need to call<code>scaler.unscale_(optimizer)</code> before clipping.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Unscales the gradients of optimizer&#x27;s assigned params in-place</span></span><br><span class="line">        scaler.unscale_(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since the gradients of optimizer&#x27;s assigned params are unscaled, clips as usual:</span></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer&#x27;s gradients are already unscaled, so scaler.step does not unscale them,</span></span><br><span class="line">        <span class="comment"># although it still skips optimizer.step() if the gradients contain infs or NaNs.</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure><h2 id="working-with-scaled-gradients---gradient-accumulation"><ahref="https://pytorch.org/docs/stable/notes/amp_examples.html#id6">Workingwith Scaled Gradients - Gradient accumulation</a></h2><p>Gradient accumulation adds gradients over an effective batch of size<code>batch_per_step * gradient_accumulation_steps</code>(<code>* num_procs</code> if distributed). Operations related to scaledgradients should occur at effective batch granularity. The followinghappens at the end of each effective batch:</p><ul><li>inf/NaN checking</li><li>step skipping if inf/NaN grads are found</li><li>parameter update</li><li>scale update</li></ul><p>Within an effective batch, all grads you accumulate should all bescaled and the scale factor should remain unchanged.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> micro_step <span class="keyword">in</span> <span class="built_in">range</span>(gradient_accumulation_steps):</span><br><span class="line">        <span class="built_in">input</span>, target = get_data(epoch, micro_step)</span><br><span class="line">        <span class="keyword">with</span> autocast(device_type=<span class="string">&#x27;cuda&#x27;</span>, dtype=torch.float16):</span><br><span class="line">            output = model(<span class="built_in">input</span>)</span><br><span class="line">            loss = loss_fn(output, target)</span><br><span class="line">            loss = loss / gradient_accumulation_steps</span><br><span class="line">        <span class="comment"># Accumulates scaled gradients.</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># If you need to work with unscaled gradients, </span></span><br><span class="line">    <span class="comment"># after all (scaled) grads for the upcoming step have been accumulated</span></span><br><span class="line">    <span class="comment"># may unscale_ here if desired (e.g., to allow clipping unscaled gradients)</span></span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><p><strong>These examples may seem too vanilla, check out <ahref="https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L290-L314">nanoGPT’smixed precision training loop</a> for a lively combination of gradientaccumulation and gradient clipping.</strong></p><h2 id="working-with-scaled-gradients---gradient-penalty"><ahref="https://pytorch.org/docs/stable/notes/amp_examples.html#id7">Workingwith Scaled Gradients - Gradient penalty</a></h2><p>What? Why?</p><p>https://discuss.pytorch.org/t/whats-the-use-of-scaled-grad-params-in-this-example-of-gradient-penalty-with-scaled-gradients/199741/3</p><h2 id="epilogue">Epilogue</h2><p>This <a href="https://deepgram.com/ai-glossary/gradient-scaling">wikipage from Deepgram</a> provides a detailed view of what gradient scalingis about, but I don’t know why it just reads like AI-generated content.Maybe because it gives too many unnecessary details.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Loss Scaling / Gradient Scaling was mentioned in &lt;a
href=&quot;#2024/03/01-Mixed-Precision-Training&quot;&gt;Mixed-Precision Training&lt;/a&gt;
as one of the 3 techniques, but there are many points to be careful with
when in practice.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Decoupled Weight Decay Regularization (SGDW &amp; AdamW)</title>
    <link href="https://yao-lirong.github.io/2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&amp;-AdamW)/"/>
    <id>https://yao-lirong.github.io/2024-03-13-Decoupled-Weight-Decay-Regularization-(SGDW-&amp;-AdamW)/</id>
    <published>2024-03-13T04:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.124Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1711.05101">The paper Decoupled WeightDecay Regularization</a> mainly introduces AdamW, which is the SOTAoptimizer since then. It investigates why Adam with L2 regularizationsometimes performs worse than SGD with L2 regularization. Itdemonstrates weight decay and L2 regularization, two things peopleusually draw an equal sign, are not the same. And it shows weight decayis the ultimate go-to choice.</p><span id="more"></span><p>Weight decay and L2 regularization are equivalent in SGD when set L2regularizer <span class="math inline">$\lambda' = \frac \lambda\alpha$</span>, which is our common practice. The situation is morecomplicated with adaptive gradient algorithms like Adam. Adam performsmuch better with weight decay and the authors propose the new SOTAoptimizer AdamW (Adam with decoupled weight decay). All the conclusionsand main finding can be found in the first 2 pages of the paper andmostly in the Introduction section. I did not read the math.</p><p><ahref="https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html">Thisblogpost from Fast.ai</a> demonstrates how the two methods are differentin code, a bit easier to understand than the paper which doesn’t providea comparision.</p><h2 id="weight-decay-in-transformers">Weight Decay in Transformers</h2><p>AdamW is the go-to optimizer for LLM these days. Researchers chose itbecause LLMs are hard to train and rarely overfit, and Adam is the bestchoice when convergence speed is considered (<ahref="https://www.zhihu.com/question/519307910/answer/2384626354">reference</a>).People have also found AdamW usually performs best with big weight decaycoefficient like 0.05 or 0.1 (<ahref="https://www.zhihu.com/question/536185388">zhihu question</a>, <ahref="https://arxiv.org/abs/2010.11929">ViT paper: Training &amp;Fine-tuning section</a>)</p><p>When we apply weight decay in transformers, we apply it to all layersexcept LayerNorm and bias layers.</p><p>In <ahref="https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/model.py#L268-L271">nanoGPT</a>,Karpathy filtered them out using: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span><br><span class="line"><span class="comment"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don&#x27;t.</span></span><br><span class="line">decay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &gt;= <span class="number">2</span>]</span><br><span class="line">nodecay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &lt; <span class="number">2</span>]</span><br><span class="line">optim_groups = [</span><br><span class="line">   &#123;<span class="string">&#x27;params&#x27;</span>: decay_params, <span class="string">&#x27;weight_decay&#x27;</span>: weight_decay&#125;,</span><br><span class="line">   &#123;<span class="string">&#x27;params&#x27;</span>: nodecay_params, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>One caveat is that, <ahref="https://github.com/karpathy/nanoGPT/commit/7fe4a099ad2a4654f96a51c0736ecf347149c34c#diff-fada037ad086638e65c7ae77e3d223963e9afaa26326aab0ea718f4013176e43L282">inearlier versions</a>, Karpathy did NOT weight decay embeddings:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">elif</span> pn.endswith(<span class="string">&#x27;weight&#x27;</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(m, blacklist_weight_modules):</span><br><span class="line">   <span class="comment"># weights of blacklist modules will NOT be weight decayed</span></span><br><span class="line">   no_decay.add(fpn)</span><br></pre></td></tr></table></figure></p><p>I couldn’t find any instruction on whether you should decayembeddings or not when training a transformer, but <ahref="https://github.com/huggingface/transformers/blob/66ce9593fdb8e340df546ddd0774eb444f17a12c/src/transformers/trainer.py#L979-L988">HuggingFace’s transformer implementation</a> also decays embeddings, in linewith Karpathy’s latest implementation. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get_parameter_names(model, name) excludes layers with `name`</span></span><br><span class="line">decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)</span><br><span class="line">decay_parameters = [name <span class="keyword">for</span> name <span class="keyword">in</span> decay_parameters <span class="keyword">if</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;The paper Decoupled Weight
Decay Regularization&lt;/a&gt; mainly introduces AdamW, which is the SOTA
optimizer since then. It investigates why Adam with L2 regularization
sometimes performs worse than SGD with L2 regularization. It
demonstrates weight decay and L2 regularization, two things people
usually draw an equal sign, are not the same. And it shows weight decay
is the ultimate go-to choice.&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Mixed-Precision Training</title>
    <link href="https://yao-lirong.github.io/2024-03-01-Mixed-Precision-Training/"/>
    <id>https://yao-lirong.github.io/2024-03-01-Mixed-Precision-Training/</id>
    <published>2024-03-01T05:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.127Z</updated>
    
    <content type="html"><![CDATA[<p>Mixed-precision training was introduced in <ahref="https://arxiv.org/abs/1710.03740">Nvidia and Baidu’s research</a>.<ahref="https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/">Theblogpost from Nvidia</a> gave a nice summary of how it’s done and why itworks. Nvidia also gave a more in-depth coverage of the same points intheir tutorial on <ahref="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">trainingwith mixed precision</a>.</p><span id="more"></span><p>I decided to learn this as I was reading <ahref="https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/sample.py#L28-L29">nanoGPT’scode</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on matmul</span></span><br><span class="line">torch.backends.cudnn.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on cudnn</span></span><br></pre></td></tr></table></figure><h2 id="benefits">Benefits</h2><ul><li><strong>Decrease the required amount of memory</strong>: FP32 -&gt;FP16</li><li><strong>Shorten the training or inference time</strong>:<ul><li><strong>memory bandwith</strong>: half-precision halves the numberof bytes need to be accessed, thus reducing time-spent in memory-limitedoperations</li><li><strong>arithmetic bandwidth</strong>: half-precision arithmatic isinherintely faster than single-precision</li></ul></li></ul><h2 id="techniques-in-original-paper">3 Techniques in OriginalPaper</h2><ul><li><p><strong>Accumulation into FP32</strong>: then convert to FP16 forstorage</p></li><li><p><strong>Loss Scaling (Gradient Scaling)</strong>: There are fourtypes of tensors encountered when training DNNs: activations, activationgradients, weights, and weight gradients. In experience activations,weights, and weight gradients can be represented with half precision.However, for some networks activation gradients are too small to berepresented in half-precision range (underflow)</p><p>Therefore, we need to scale up the activation gradients. This can bedone by simply multiply the training loss with the scale factor. Thisadds just a single multiplication and by the chain rule it ensures thatall the gradients are scaled up at no additional cost.</p></li><li><p><strong>FP32 Master Copy of Weights</strong>: Weight gradientmagnitudes are smaller than corresponding weights, especially aftermultiplication with the learning rate. So sometimes no update takesplace.</p><p>The remedy is to store the weights in single precision, but docomputation in half precision. Update this master copy of weights aftereach computation.</p></li></ul><h2 id="more-recent-update">More Recent Update</h2><p>In NVIDIA Ampere GPU architecture, Nvidia introduced <ahref="https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/">TensorFloat32(TF32)</a> with FP32 range (8bit) and FP16 precision (10bit). With theadditional sign bit, it is a novel 19 bit representation of floats.</p><p>On an A100, it brings 8x speed up compared to FP32, while FP16/BF16brings 16x speedup. Therefore, mixed-precision training with a native16-bit format (FP16/BF16) is still the fastest option.</p><p>TF32 is only exposed as a Tensor Core operation mode, not a type.Internally, all storage in memory and other operations remain completelyin FP32, only convolutions and matrix-multiplications convert theirinputs to TF32 right before multiplication. Therefore, it does notprovide the memory benifits or the native arithmetic speed up brought by16-bit format. Its benefit is that it brings Tensor Core acceleration tosingle-precision DL workloads, without needing any changes to modelscripts.</p><p>It needs to be noted that TF32 gives less accurate computationresults. Therefore, PyTorch decided to set<code>toggle torch.backends.matmul.allow_tf32 = False</code> by defaultstarting from <ahref="https://github.com/huggingface/transformers/issues/16588">1.12</a>.Read more about <ahref="https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere">PyTorch’sofficial comparison</a> of speed and numerical stability.</p><h2 id="best-practices">Best Practices</h2><p>PyTorch gave some <ahref="https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/#best-practices">officialbest practices tips to developers</a>. Please do check them outhere.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mixed-precision training was introduced in &lt;a
href=&quot;https://arxiv.org/abs/1710.03740&quot;&gt;Nvidia and Baidu’s research&lt;/a&gt;.
&lt;a
href=&quot;https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/&quot;&gt;The
blogpost from Nvidia&lt;/a&gt; gave a nice summary of how it’s done and why it
works. Nvidia also gave a more in-depth coverage of the same points in
their tutorial on &lt;a
href=&quot;https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html&quot;&gt;training
with mixed precision&lt;/a&gt;.&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Parameter and FLOP Count in Transformer Model</title>
    <link href="https://yao-lirong.github.io/2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/"/>
    <id>https://yao-lirong.github.io/2024-02-22-Parameter-and-FLOP-Count-in-Transformer-Model/</id>
    <published>2024-02-22T05:00:00.000Z</published>
    <updated>2025-09-02T23:34:26.325Z</updated>
    
    <content type="html"><![CDATA[<p>We borrow the results of decoder-only transformer models from <ahref="https://arxiv.org/abs/2001.08361">OpenAI’s paper Scaling Laws forNeural Language Models</a> Section 2.1</p><span id="more"></span><p>We use the following notations:</p><ul><li><p><span class="math inline"><em>L</em></span> = number of layers oftransformer blocks (<span class="math inline"><em>N</em></span> in <ahref="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>)</p></li><li><p><spanclass="math inline"><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub></span>= dimension of the input &amp; output of a transformer block, also theoutput of the text encoder and input of the decoder</p></li><li><p><spanclass="math inline"><em>d</em><sub><em>f</em><em>f</em></sub></span> =dimension of the feed-forward network’s bottleneck. We defined thefeed-forward network as<code>fc1 = fc(d_model, d_ff), fc2 = fc(d_ff, d_model)</code></p></li><li><p><spanclass="math inline"><em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub></span>= dimension of the multi-head attention output (In <ahref="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>,we have <span class="math inline"><em>h</em></span> number of heads.Queries and keys have dimension <spanclass="math inline"><em>d</em><sub><em>k</em></sub></span>. Values havedimension <spanclass="math inline"><em>d</em><sub><em>v</em></sub></span>. In practice,we usually have <spanclass="math inline"><em>d</em><sub><em>k</em></sub> = <em>d</em><sub><em>v</em></sub></span>.<spanclass="math inline"><em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub></span>we have here is defined as <spanclass="math inline"><em>d</em><sub><em>k</em></sub> × <em>h</em></span>)</p></li></ul><table><colgroup><col style="width: 10%" /><col style="width: 17%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>Part</th><th>Parameters</th><th>Explanation</th></tr></thead><tbody><tr class="odd"><td>Embed</td><td><span class="math inline">$n_{vocab}\times d_{model} \\ +n_{ctx}\times d_{model}$</span></td><td>One word embedding matrix (mapping each token to correspondingembedding ) and one positional embedding matrix</td></tr><tr class="even"><td>Attention: Q K V Matrix</td><td><spanclass="math inline"><em>L</em>3<em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub><em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub></span></td><td><span class="math inline"><em>W</em><sub><em>Q</em></sub></span> hasshape <spanclass="math inline">(<em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>, <em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub>)</span>.There’re also <spanclass="math inline"><em>W</em><sub><em>K</em></sub></span> and <spanclass="math inline"><em>W</em><sub><em>V</em></sub></span></td></tr><tr class="odd"><td>Attention: Multi-head Projection</td><td>$L d_{attn} d_{model} $</td><td>After we concat the output from all heads, there’s one projectionfrom all-head output to the final output. This is that matrix. It wasdefined as <spanclass="math inline"><em>W</em><sup><em>O</em></sup></span> in <ahref="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need 3.2.2</a>.</td></tr><tr class="even"><td>Feedforward Network</td><td><spanclass="math inline"><em>L</em>2<em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub><em>d</em><sub><em>f</em><em>f</em></sub></span></td><td>Explained in the definition of <spanclass="math inline"><em>d</em><sub><em>f</em><em>f</em></sub></span> above.</td></tr><tr class="odd"><td>Total (Non-Embedding)</td><td><spanclass="math inline">2<em>L</em><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(2<em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub> + <em>d</em><sub><em>f</em><em>f</em></sub>)</span></td><td></td></tr></tbody></table><p>If we have the standard <spanclass="math inline"><em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub> = <em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub> = <em>d</em><sub><em>f</em><em>f</em></sub>/4</span>,we can get <spanclass="math inline"><em>N</em> = 12<em>L</em><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub><sup>2</sup></span></p><p>Put this into practice, let’s calculate a rough estimate of number ofparameters the vanilla transformer has. The vanilla transformer base,per the paper <ahref="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need Table3</a>, <span class="math inline"><em>L</em> = 6</span>, <spanclass="math inline"><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub> = 512</span>,<spanclass="math inline"><em>d</em><sub><em>f</em><em>f</em></sub> = 2048</span>,<spanclass="math inline"><em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub> = <em>h</em> × <em>d</em><sub><em>k</em></sub> = 8 × 64 = 512</span>,<spanclass="math inline"><em>n</em><sub><em>v</em><em>o</em><em>c</em><em>a</em><em>b</em></sub> = 37000</span>.I didn’t find info about <spanclass="math inline"><em>n</em><sub><em>c</em><em>t</em><em>x</em></sub></span>,but is probably 512.</p><p>Note that different from OpenAI’s favorite decoder-only transformer,the vanilla transformer has an encoder-decoder architecture and thedecoder block has an additional attention block. Therefore, the encoderhas a total <spanclass="math inline">2<em>L</em><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(2<em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub> + <em>d</em><sub><em>f</em><em>f</em></sub>)</span>parameters, the decoder has a total <spanclass="math inline">2<em>L</em><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>(4<em>d</em><sub><em>a</em><em>t</em><em>t</em><em>n</em></sub> + <em>d</em><sub><em>f</em><em>f</em></sub>)</span>params, and the embedding part has a total <spanclass="math inline"><em>n</em><sub><em>v</em><em>o</em><em>c</em><em>a</em><em>b</em></sub> × <em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub> + <em>n</em><sub><em>c</em><em>t</em><em>x</em></sub> × <em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub></span>params. The final result is <spanclass="math inline"> ∼ 63 × 10<sup>6</sup></span>. I tried hard tofigure out where went off from the paper’s <spanclass="math inline">65 × 10<sup>6</sup></span> but had no luck. Addingthe parameters of LayerNorm still didn’t even out the numbers. But it’sclose enough so I’ll call it a day.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;We borrow the results of decoder-only transformer models from &lt;a
href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI’s paper Scaling Laws for
Neural Language Models&lt;/a&gt; Section 2.1&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Memory Pinning and Transfer Data between Host (CPU) and Device (GPU)</title>
    <link href="https://yao-lirong.github.io/2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/"/>
    <id>https://yao-lirong.github.io/2024-02-09-Memory-Pinning-and-Transfer-Data-between-Host-(CPU)-and-Device-(GPU)/</id>
    <published>2024-02-09T05:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.127Z</updated>
    
    <content type="html"><![CDATA[<p><ahref="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers">PyTorchofficial documentation</a> explains this concept very briefly and we gointo more detail here.</p><span id="more"></span><h2 id="what-is-memory-pinning-and-why-we-use-it">What is memory pinningand why we use it</h2><p>First, let’s go back to our OS class and remind what “paged memory”means. Process always wants contiguous memory. The OS uses memory pagingto enable logically contiguous memory that is not physically contiguous.When a process requests memory, OS allocates page frames to the process.These page frames look contiguous to the process, but are actually notso in physical memory. The OS then maps the process’s logical pages tothe physical page frames.</p><p>This <ahref="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned-host-memory">Nvidiablog on data transfer</a> explains what this has to do with GPU: The GPUcannot access data directly from pageable host memory (logicallycontiguous), so when a data transfer from pageable host memory to devicememory is invoked, the CUDA driver must first allocate a temporarypage-locked, or “pinned”, physically contiguous host array, copy thehost data to the pinned array, and then transfer the data from thepinned array to device memory.</p><figure><imgsrc="https://developer-blogs.nvidia.com/wp-content/uploads/2012/12/pinned-1024x541.jpg"alt="pinned memory" /><figcaption aria-hidden="true">pinned memory</figcaption></figure><p>Therefore, we can avoid the cost of the transfer between pageable andpinned host arrays by directly allocating our host arrays in pinnedmemory.</p><p>To my understanding, “directly allocating in pinned memory”corresponds to what’s described in <ahref="https://pytorch.org/docs/stable/data.html#memory-pinning"><code>DataLoader</code>’sdocumentation</a> as:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loader = DataLoader(dataset, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="pin_memory-and-non_blockingtrue"><code>pin_memory()</code> and<code>non_blocking=True</code></h2><p>On the other hand, while reading <ahref="https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L126-L128">nanoGPT’scode</a>, I saw the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> device_type == <span class="string">&#x27;cuda&#x27;</span>:</span><br><span class="line">   <span class="comment"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span></span><br><span class="line">   x, y = x.pin_memory().to(device, non_blocking=<span class="literal">True</span>), y.pin_memory().to(device, non_blocking=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>pin_memory</code> is familiar to us while<code>non_blocking</code> is something new. It tells the program that itcan perform other operations on this data while it being trasferred fromhost to device. (so don’t block till transfer is done to start theoperation) This async copy usually speeds things up. This <ahref="https://stackoverflow.com/a/55564072">Stack Overflow answer</a>gives a detaild example of the async part.</p><p>Here in the code, we are explicitly calling <code>pin_memory()</code>on something already initialized, which really confused me. Sinceaccording to the above quoted Nvidia blog, “when a data transfer frompageable host memory to cuda device memory is invoked, the CUDA drivermust first allocate a pinned host array, copy the host data to thepinned array, and then transfer the data from the pinned array to devicememory.” That is to say: even without such an explicit<code>pin_memory()</code> call, CUDA will do it for us.</p><p>I found <ahref="https://discuss.pytorch.org/t/when-is-pinning-memory-useful-for-tensors-beyond-dataloaders/103710">thisexchange on PyTorch’s forum</a> and <ahref="https://discuss.pytorch.org/t/how-is-explicit-pin-memory-different-from-just-calling-to-and-let-cuda-handle-it/197422">alsoasked this question myself</a>, but didn’t receive a super clear answer.But inferring from what <ahref="https://discuss.pytorch.org/u/ptrblck/summary"><spanclass="citation" data-cites="ptrblck">@ptrblck</span></a> said, I thinkit is correct to say that the following two commands are equal in speed(with the first pinning memory implicitly and the second does itexplicitly)</p><ol type="1"><li><code>t.to("cuda", non_blocking=False)</code></li><li><code>t.pin_memory().to("cuda", non_blocking=False)</code></li></ol><p>and explicit memory pinning call is only useful when used togetherwith <code>to(device, non_blocking=True)</code></p><p>Someone in <a href="https://zhuanlan.zhihu.com/p/477870660">thisZhihu discussion</a> also argues paged memory can be exchanged into diskswap when physical memory is not enough. Explicitly pinning memoryavoids this problem and saves the time of finding these pages in diskfor every query (pinning brings them all out into physical memory). Theposter did not give a reference though.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a
href=&quot;https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers&quot;&gt;PyTorch
official documentation&lt;/a&gt; explains this concept very briefly and we go
into more detail here.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Switching Personal Homepage Theme to al-folio</title>
    <link href="https://yao-lirong.github.io/2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/"/>
    <id>https://yao-lirong.github.io/2024-01-27-Switching-Personal-Webpage-Theme-to-al-folio/</id>
    <published>2024-01-27T05:00:00.000Z</published>
    <updated>2024-01-28T09:22:54.542Z</updated>
    
    <content type="html"><![CDATA[<p>Since long before have I realized I do need a more official and moreacademic homepage in addition to a personal blog site, but I didn’t findtime to do it until I started working. Now after this switch, I have mypersonal homepage of <ahref="https://github.com/alshedivat/al-folio/">al-folio</a> in <ahref="https://jekyllrb.com/">Jekyll</a> and at the same time my blog of<a href="https://github.com/fi3ework/hexo-theme-archer/">archer</a> in<a href="https://hexo.io/">Hexo</a>.</p><span id="more"></span><h2 id="migrating-hexo-blog-to-sub-directory">Migrating Hexo Blog toSub-Directory</h2><p>I want to keep this <ahref="https://github.com/fi3ework/hexo-theme-archer/">archer theme</a>for my blog while creating a new personal homepage. Therefore, I have tomove this repo out of the GitHub page repo and into some other blogrepo. That is, this blog will now be served as a GitHub project page,like my <a href="https://yao-lirong.github.io/ARC-Visualization-40/">ARC40</a>. What needs to be done was surprisingly simple:</p><ol type="1"><li><p>Create a GitHub repo to store your webpages: This repo willcontain the exact same information as my previous <ahref="https://github.com/Yao-Lirong/Yao-Lirong.github.io">Yao-Lirong/Yao-Lirong.github.io</a>.I created <ahref="https://github.com/Yao-Lirong/blog">Yao-Lirong/blog</a>.</p></li><li><p>Change your Hexo <code>_config.yml</code>. You can reference the<a href="https://hexo.io/docs/github-pages.html">official guide ondeploying</a>, but they didn’t mention how to one-command deploy aproject page.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://yao-lirong.github.io/blog</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/blog/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/Yao-Lirong/blog</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br><span class="line">  <span class="attr">message:</span> <span class="string">Updated</span> <span class="string">at</span> &#123;&#123; <span class="string">now(&#x27;YYYY-MM-DD</span> <span class="string">HH:mm&#x27;)</span> &#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p><code>hexo clean &amp;&amp; hexo deploy</code></p></li></ol><h2 id="configuring-al-folio">Configuring al-folio</h2><h3 id="local-install">Local Install</h3><p>Since this migration was a long process, when I was working onal-folio, my archer page was still up and I have to first try outal-folio locally to not break my current GitHub page. To do this, theonly feasible way is to use docker as suggested in their <ahref="https://github.com/alshedivat/al-folio/blob/master/INSTALL.md#local-setup-using-docker-recommended">installinstruction</a>. Don’t bother to try other ways. You won’t have any luckrunning them.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/alshedivat/al-folio.git</span><br><span class="line"><span class="built_in">cd</span> al-folio/</span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up</span><br></pre></td></tr></table></figure><h3 id="personalization">Personalization</h3><ol type="1"><li><p><code>_pages/*.md</code> will be shown on the navigation bar. Ideleted most of them and only kept<code>_pages/about.md, cv.md, projects.md</code></p></li><li><p><code>_news/</code> directory contains what will be shown on thehomepage “news” section. Do change those</p></li><li><p><code>_sass/_themes.scss</code> has a variable<code>--global-theme-color</code> controlling the theme of this site. Ihad</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--global-theme-<span class="attribute">color</span>: #&#123;<span class="variable">$cyan-color</span>&#125;;</span><br><span class="line">--global-hover-<span class="attribute">color</span>: #&#123;<span class="variable">$light-cyan-color</span>&#125;;</span><br></pre></td></tr></table></figure></li><li><p>In <code>_config.yaml</code>, we see that <code>projects</code>is listed under <code>collections</code>. This means we can build thefollowing file structure, where we have <code>_pages/projects.md</code>to be displayed on the navigation bar as an entry point and<code>projects/p1.md</code> for a specific project. Find more about<code>collections</code> in <ahref="https://github.com/alshedivat/al-folio#collections">README</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">_pages/</span><br><span class="line">├─ projects.md - mywebsite.com/projects/</span><br><span class="line">projects/</span><br><span class="line">├─ p1.md - mywebsite.com/projects/p1</span><br><span class="line">├─ p2.md - mywebsite.com/projects/p2</span><br><span class="line">├─ ....</span><br></pre></td></tr></table></figure></li></ol><h2 id="integrating-hexo-blog-into-new-homepage">Integrating Hexo Bloginto New Homepage</h2><ol type="1"><li><p>Delete any blog-related feature that came along with al-folio: <ahref="https://github.com/alshedivat/al-folio/discussions/627"><em>reference</em></a></p><ol type="1"><li><p>Delete all the example posts:<code>rm -rf ./_posts/</code></p></li><li><p>Delete the template generating <code>mywebsite.com/blog</code>page: <code>rm -rf ./blog/</code></p></li><li><p>Comment out the following entries under<code>Jekyll Archives</code> section and <code>Blog</code> section in<code>_config.yml</code> to make sure absolutely nothing is generatedunder <code>mywebsite.com/blog</code> address: (I was about to commentout everything, but it turns out this would break some contentgeneration features, so after some experiments I found you only need tocomment out the followings:)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="comment"># Blog</span></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">/blog/:year/:title/</span></span><br><span class="line"></span><br><span class="line"><span class="attr">related_blog_posts:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">max_related:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="comment"># Jekyll Archives</span></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jekyll-archives:</span></span><br><span class="line">  <span class="attr">enabled:</span> [<span class="string">year</span>, <span class="string">tags</span>, <span class="string">categories</span>] <span class="comment"># enables year, tag and category archives (remove if you need to disable one of them).</span></span><br><span class="line">  <span class="attr">layouts:</span></span><br><span class="line">    <span class="attr">year:</span> <span class="string">archive-year</span></span><br><span class="line">    <span class="attr">tag:</span> <span class="string">archive-tag</span></span><br><span class="line">    <span class="attr">category:</span> <span class="string">archive-category</span></span><br><span class="line">  <span class="attr">permalinks:</span></span><br><span class="line">    <span class="attr">year:</span> <span class="string">&quot;/blog/:year/&quot;</span></span><br><span class="line">    <span class="attr">tag:</span> <span class="string">&quot;/blog/tag/:name/&quot;</span></span><br><span class="line">    <span class="attr">category:</span> <span class="string">&quot;/blog/category/:name/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">display_tags:</span> [<span class="string">&quot;formatting&quot;</span>, <span class="string">&quot;images&quot;</span>, <span class="string">&quot;links&quot;</span>, <span class="string">&quot;math&quot;</span>, <span class="string">&quot;code&quot;</span>] <span class="comment"># these tags will be displayed on the front page of your blog</span></span><br><span class="line"><span class="attr">display_categories:</span> [<span class="string">&quot;blockquotes&quot;</span>] <span class="comment"># these categories will be displayed on the front page of your blog</span></span><br></pre></td></tr></table></figure></li></ol></li><li><p>Set the <code>blog</code> entry on the navigation bar link to myactual blog address:</p><ol type="1"><li><p>set <code>_config.yml blog_nav_title:</code> to empty, so theal-folio’s <code>blog</code> entry will not show on the navigation bar,<ahref="https://github.com/alshedivat/al-folio/issues/1328"><em>reference1</em></a>, <ahref="https://github.com/alshedivat/al-folio/issues/67"><em>reference2</em></a></p></li><li><p>Find in <code>_includes/header.html</code> the if statement<code>&#123;% if site.blog_nav_title %&#125;</code> go to the end of this if statement andadd something: <ahref="https://github.com/alshedivat/al-folio/discussions/579"><em>reference</em></a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if site.blog_nav_title %&#125;</span><br><span class="line"><span class="comment">&lt;!-- Blog --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Empty here because blog_nav_title attribute is empty --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;nav-item &#123;% if page.url contains &#x27;blog&#x27; %&#125;active&#123;% endif %&#125;&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;nav-link&quot;</span> <span class="attr">href</span>=<span class="string">&quot;&#123;&#123; &#x27;/blog/&#x27; | relative_url &#125;&#125;&quot;</span>&gt;</span>&#123;&#123; site.blog_nav_title &#125;&#125;</span><br><span class="line">    &#123;%- if page.url contains &#x27;blog&#x27; -%&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;sr-only&quot;</span>&gt;</span>(current)<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    &#123;%- endif -%&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">&#123;%- endif %&#125;</span><br><span class="line"><span class="comment">&lt;!-- Actual Blog Takes Place Here --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;nav-item&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;nav-link&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://yao-lirong.github.io/blog/&quot;</span>&gt;</span>blog<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">li</span>&gt;</span> </span><br></pre></td></tr></table></figure></li></ol></li><li><p>One thing really good about al-folio is that it allows includingexternal blog posts. You can do this by adding the RSS feed in<code>_config.yml</code>: <ahref="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a"><em>reference</em></a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">external_sources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">blog</span></span><br><span class="line">    <span class="attr">rss_url:</span> <span class="string">https://yao-lirong.github.io/blog/atom.xml</span></span><br></pre></td></tr></table></figure><p>These external posts can also be displayed on your homepage if youset <code>latest_posts: true</code> in the front matter of<code>_pages/about.md</code></p></li></ol><h2 id="tbd">TBD</h2><p>how about sitemap?</p><p>GA4 tags check</p><p>do a photowall?</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Since long before have I realized I do need a more official and more
academic homepage in addition to a personal blog site, but I didn’t find
time to do it until I started working. Now after this switch, I have my
personal homepage of &lt;a
href=&quot;https://github.com/alshedivat/al-folio/&quot;&gt;al-folio&lt;/a&gt; in &lt;a
href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and at the same time my blog of
&lt;a href=&quot;https://github.com/fi3ework/hexo-theme-archer/&quot;&gt;archer&lt;/a&gt; in
&lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Visual Information Theory</title>
    <link href="https://yao-lirong.github.io/2024-01-21-Visual-Information-Theory/"/>
    <id>https://yao-lirong.github.io/2024-01-21-Visual-Information-Theory/</id>
    <published>2024-01-21T05:00:00.000Z</published>
    <updated>2025-09-02T23:24:43.202Z</updated>
    
    <content type="html"><![CDATA[<p>This blog post is adapted from ex-OpenAI researcher, Anthropicco-founder <ahref="https://colah.github.io/posts/2015-09-Visual-Information/">ChristopherOlah’s wonderful work</a>. I removed parts that are generallycommonsense to a CS kid and added some of my own notes &amp;explanations.</p><span id="more"></span><h2 id="visualizing-probability-distribution">Visualizing ProbabilityDistribution</h2><p>The author did a really cool job visualizing probability distributionhere, but doesn’t provide any more knowledge than basic probabilitytheory, so removed this part.</p><h2 id="code">Code</h2><p>I want to communicate with my friend Bob, so we establish a code,mapping each possible word we may say to sequences of bits.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-2bit.png"alt="fixed length code" /><figcaption aria-hidden="true">fixed length code</figcaption></figure><p>However, Bob is a dog lover, with very high probability, he talksabout dogs.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/DogWordFreq.png"alt="Bob’s word frequency" /><figcaption aria-hidden="true">Bob’s word frequency</figcaption></figure><p>Incorporating the code we defined above into this graph, we get thefollowing diagram, with the vertical axis to visualize the probabilityof each word, <span class="math inline"><em>p</em>(<em>x</em>)</span>,and the horizontal axis to visualize the length of the correspondingcodeword, <span class="math inline"><em>L</em>(<em>x</em>)</span>.Notice the total area is the expected length of a codeword we send.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/OldCode.png"alt="Expected length of fixed-length code" /><figcaption aria-hidden="true">Expected length of fixed-lengthcode</figcaption></figure><h2 id="variable-length-code">Variable-Length Code</h2><p>Perhaps we could be very clever and make a variable-length code wherecodewords for common words are made especially short. The challenge isthat there’s competition between codewords – making some shorter forcesus to make others longer. To minimize the message length, we especiallywant the commonly used ones to be. So the resulting code has shortercodewords for common words (like “dog”) and longer codewords for lesscommon words (like “bird”).</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code.png"alt="Variable length code" /><figcaption aria-hidden="true">Variable length code</figcaption></figure><p>Looking at this code format with word frequency, on average, thelength of a codeword is now 1.75 bits!</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/NewCode.png"alt="Expected length of variable-length code" /><figcaption aria-hidden="true">Expected length of variable-lengthcode</figcaption></figure><p>It turns out that this code is the best possible code. There is nocode which, for this word frequency distribution, will give us anaverage codeword length of less than 1.75 bits.</p><p>There is simply a fundamental limit. Communicating what word wassaid, what event from this distribution occurred, requires us tocommunicate at least 1.75 bits on average. No matter how clever ourcode, it’s impossible to get the average message length to be less. Wecall this fundamental limit the <strong>entropy</strong> of thedistribution – we’ll discuss it in much more detail shortly.</p><h2 id="the-space-of-codewords">The Space of Codewords</h2><p>To make our codewords uniquely decodable, we want them to follow the<em>prefix property</em>: no codeword should be the prefix of anothercodeword. i.e. If we see a particular codeword, there shouldn’t be somelonger version that is also a codeword. Codes that obey this propertyare also called <em>prefix codes</em>.</p><p>One useful way to think about this is that every codeword requires asacrifice from the space of possible codewords. If we take the codeword01, we lose the ability to use any codewords it’s a prefix of. We can’tuse 010 or 011010110 anymore because of ambiguity – they’re lost to us.The following graph shows we in effect lost <spanclass="math inline">$\frac {1} {2^{L(01)}} = \frac {1} {2^2} = \frac {1}{4}$</span> of our codeword space.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/CodeSpaceUsed.png"alt="Code space sacrificed" /><figcaption aria-hidden="true">Code space sacrificed</figcaption></figure><p>Since a quarter of all codewords start with 01, we’ve sacrificed aquarter of all possible codewords. That’s the price we pay in exchangefor having one codeword that’s only 2 bits long! In turn this sacrificemeans that all the other codewords need to be a bit longer. There’salways this sort of trade off between the lengths of the differentcodewords. A short codeword requires you to sacrifice more of the spaceof possible codewords, preventing other codewords from being short. Whatwe need to figure out is what the right trade off to make is!</p><h2 id="cost-of-codeword">Cost of Codeword</h2><p>You can think of this like having a limited budget to spend ongetting short codewords. In fact, we have a budget = 1 to spend, where 1is the area of the whole codeword space. We pay for one codeword bysacrificing a fraction of possible codewords.</p><p>We define the cost <span class="math inline"><em>c</em></span> ofhaving a code word <span class="math inline"><em>x</em></span> withlength <span class="math inline"><em>L</em></span> as <spanclass="math inline">$\frac 1 {2^{L(x)}}$</span>.</p><ul><li>The cost of buying a codeword of length 0 is 1, all possiblecodewords – if you want to have a codeword of length 0, you can’t haveany other codeword. <span class="math inline">$x = \emptyset, c = \frac1 {2^{L(\emptyset)}} = \frac 1 {2^0}$</span></li><li>The cost of a codeword of length 1, like “0”, is 1/2 because half ofpossible codewords start with “0”. <span class="math inline">$x = 0, c =\frac 1 {2^{L("0")}} = \frac 1 {2^1}$</span></li><li>The cost of a codeword of length 2, like “01”, is 1/4 because aquarter of all possible codewords start with “01” <spanclass="math inline">$x = 0, c = \frac 1 {2^{L("01")}} = \frac 1{2^2}$</span></li><li>…</li></ul><p>(ignore the gray area in the following graph, the function we plot is<span class="math inline">$\frac 1 {2^L}$</span> and cost is theheight)</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-costonly.png"alt="Code cost" /><figcaption aria-hidden="true">Code cost</figcaption></figure><p>To make the calculation simpler, instead of base 2, we imagine wehave the natural number e. It now is no longer a binary code, but a“natural” code with the cost becomes <span class="math inline">$\frac 1{e^L}$</span>. It is both the height and the gray area: <spanclass="math inline">$\frac 1 {e^L} = \int^\infty_L \frac 1 {e^L} \;dL$</span></p><h2 id="optimal-encoding">Optimal Encoding</h2><p>Recall the expected length of the codeword we pictured above, if welook at each codeword’s contribution to this expected length, eachcodeword makes the average message length longer by its probabilitytimes the length of the codeword. For example, if we need to send acodeword that is 4 bits long 50% of the time, our average message lengthis 2 bits longer than it would be if we weren’t sending that codeword.We can picture this as a rectangle.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-lengthcontrib.png"alt="Contribution to expected length" /><figcaption aria-hidden="true">Contribution to expectedlength</figcaption></figure><p>Now, we have two values related to the length of a codeword and wepicture them together in the following graph:</p><ol type="1"><li>The amount we pay decides the length of the codeword: gray partdecides the width of the purple rectangle</li><li>The length of the codeword controls how much it adds to the averagemessage length: width of the rec decides the area of the rec (height isfixed for a given word because height represents the wordfrequency)</li></ol><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-cost.png"alt="Contribution and cost" /><figcaption aria-hidden="true">Contribution and cost</figcaption></figure><p>Short codewords reduce the average message length but are expensive,while long codewords increase the average message length but arecheap.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-cost-longshort.png"alt="short vs long" /><figcaption aria-hidden="true">short vs long</figcaption></figure><p>What’s the best way to use our limited budget? Just like one wants toinvest more in tools that one uses regularly, we want to spend more onfrequently used codewords. There’s one particularly natural way to dothis: <strong>distribute our budget in proportion to how common an eventis - <spanclass="math inline"><em>c</em>(<em>x</em>) = <em>p</em>(<em>x</em>)</span></strong>.The following graph shows such encoding system.</p><p>Additionally, that’s exactly what we did in our codes with Bob: “dog”appears <span class="math inline">$\frac 1 2$</span> of the time, so wegive it codeword “0” with the cost <span class="math inline">$c = \frac1 {2^{L("0")}} = \frac 1 {2^1}$</span>; “bird” appears <spanclass="math inline">$\frac 1 8$</span> of the time, so we give itcodeword “111” with the cost <span class="math inline">$c = \frac 1{2^{L("111")}} = \frac 1 {2^3}$</span>; If an event only happens 1% ofthe time, we only spend 1% of our budget.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/code-auction-balanced-noderivs.png"alt="Contribution and cost line up" /><figcaption aria-hidden="true">Contribution and cost lineup</figcaption></figure><p>The author then proved it is the optimal thing to do. Honestly Ididn’t understand the proof, but this code system we’re talking abouthere is no more than a general version of Huffman Encoding. So I guessit’s fine as long as you understand how to prove the greedy HuffmanEncoding is optimal.</p><h2 id="calculating-entropy">Calculating Entropy</h2><p>In the <a href="#cost-of-codeword">Cost of Codeword</a> section, wedefined the cost <span class="math inline"><em>c</em></span> of having acode word <span class="math inline"><em>x</em></span> with length <spanclass="math inline"><em>L</em></span> as <spanclass="math inline">$\frac 1 {2^{L(x)}}$</span>. By inverting thisdefinition, given <span class="math inline"><em>c</em></span>, we caninfer the length <span class="math inline">$L(x) = \log_2 \frac 1c$</span></p><p>Furthermore, in the <a href="#optimial-encoding">Optimal Encoding</a>section, we proved the optimal cost to spend on each word is <spanclass="math inline"><em>c</em>(<em>x</em>) = <em>p</em>(<em>x</em>)</span>,so the length of the optimal encoding system is <spanclass="math inline">$L(x) = \log_2 \frac 1 {p(x)}$</span></p><p>Earlier, we said: given a probability distribution of what we want tocommunicate, there is a fundamental limit of expected code length weneed to send no matter how smart our code is. This limit, the expectedcode length using the best possible code, is called the<strong>entropy</strong> of <span class="math inline"><em>p</em></span>:<span class="math inline"><em>H</em>(<em>p</em>)</span>.</p><p><span class="math display">$$H(p) = \mathbb{E}_{p(x)} [L(x)] = \sum_x p(x) \log_2 \frac 1 {p(x)}$$</span></p><p>People usually write entropy in the following way, which makes itmuch less intuitive.</p><p><spanclass="math display"><em>H</em>(<em>p</em>) = −∑<sub><em>x</em></sub><em>p</em>(<em>x</em>)log<sub>2</sub><em>p</em>(<em>x</em>)</span></p><p>The entropy, which by definition is the shortest possible code, hasclear implications for compression. In addition, it describes howuncertain I am and gives a way to quantify information:</p><ul><li>If I knew for sure what was going to happen, I wouldn’t have to senda message at all! <spanclass="math inline"><em>p</em>(<em>x</em>) = 1 ⟹ <em>H</em>(<em>x</em>) = 0</span></li><li>If there’s two things that could happen with 50% probability, I onlyneed to send 1 bit. <spanclass="math inline">∀<em>x</em><sub><em>i</em></sub>, <em>p</em>(<em>x</em><sub><em>i</em></sub>) = 0.5 ⟹ <em>H</em>(<em>x</em>) = 1</span></li><li>If there’s 64 different things that could happen with equalprobability, I’d have to send 6 bits. <span class="math inline">$\forallx_i, p(x_i) = \frac 1 {64} \implies H(x) = 6$</span></li></ul><p>The more concentrated the probability, the more I can craft a clevercode with short average messages. The more diffuse the probability, thelonger my messages have to be.</p><h2 id="cross-entropy">Cross Entropy</h2><p>Imagine now a cat-lover Alice talks about animals with the followingword frequency:</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/DogCatWordFreq.png"alt="Alice loves cats" /><figcaption aria-hidden="true">Alice loves cats</figcaption></figure><p>When Alice sends a message to Bob using Bob’s codes, her messageswere longer than they needed to be. Bob’s code was optimized to hisprobability distribution. Alice has a different probabilitydistribution, and the code is suboptimal for it: the expected codewordlength when Bob uses his own code is 1.75 bits, while when Alice useshis code it’s 2.25. It would be worse if the two weren’t so similar!</p><p>The expected length of communicating an event from distribution <spanclass="math inline"><em>q</em>(<em>x</em>)</span> with the optimal codefor another distribution <spanclass="math inline"><em>p</em>(<em>x</em>)</span> is called the<strong>cross-entropy</strong>. Formally, we define cross-entropy as:(people usually write <spanclass="math inline"><em>H</em>(<em>q</em>, <em>p</em>)</span>instead)</p><p><span class="math display">$$H_p(q) = \mathbb E_{q} \log_2\left(\frac{1}{p(x)}\right)= \sum_xq(x)\log_2\left(\frac{1}{p(x)}\right)$$</span></p><p>To lower the cost of Alice sending message, I asked both of them tonow use Alice’s coding, but this made it suboptimal for Bob andsurprisingly, it’s worse for Bob to use Alice’s code than for Alice touse his! <strong>Cross-entropy isn’t symmetric.</strong></p><ul><li>Bob using his own code (<spanclass="math inline"><em>H</em>(<em>p</em>) = 1.75</span> bits)</li><li>Alice using Bob’s code (<spanclass="math inline"><em>H</em><sub><em>p</em></sub>(<em>q</em>) = 2.25</span> bits)</li><li>Alice using her own code (<spanclass="math inline"><em>H</em>(<em>q</em>) = 1.75</span> bits)</li><li>Bob using Alice’s code (<spanclass="math inline"><em>H</em><sub><em>q</em></sub>(<em>p</em>) = 2.375</span> bits)</li></ul><p>In the following diagram, if the messages are coming from the samedistribution the plots are beside each other, and if they use the samecodes they are on top of each other. This allows you to kind of visuallyslide the distributions and codes together.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyCompare.png"alt="4 cases of cross entropy" /><figcaption aria-hidden="true">4 cases of cross entropy</figcaption></figure><p>So why one is bigger than the other? This is because When Bob usesAlice’s code <spanclass="math inline"><em>H</em><sub><em>q</em></sub>(<em>p</em>)</span>,Bob says “dog” with high probability but “dog” is a word Alice happensto say the least so assigned longest length to. On the other hand for<spanclass="math inline"><em>H</em><sub><em>p</em></sub>(<em>q</em>)</span>,Bob didn’t dislike “cats” that much, so didn’t assign it with too big aword length.</p><h2 id="kl-divergence">KL Divergence</h2><p>Cross-entropy gives us a way to express how different two probabilitydistributions are. When using <spanclass="math inline"><em>q</em></span>’s codewords for <spanclass="math inline"><em>p</em></span>’s distribution, the more differentthe distributions <span class="math inline"><em>p</em></span> and <spanclass="math inline"><em>q</em></span> are, the bigger the difference isbetween cross-entropy of <spanclass="math inline"><em>p</em></span> with respect to <spanclass="math inline"><em>q</em></span> and the entropy of <spanclass="math inline"><em>p</em></span>. In math, <spanclass="math inline"><em>D</em><sub><em>q</em></sub>(<em>p</em>) = <em>H</em><sub><em>q</em></sub>(<em>p</em>) − <em>H</em>(<em>p</em>)</span>is bigger.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyQP.png"alt="H_q(p)" /><figcaption aria-hidden="true">H_q(p)</figcaption></figure><p>Similarly when using <span class="math inline"><em>p</em></span>’scodewords for <span class="math inline"><em>q</em></span>’sdistribution.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyPQ.png"alt="H_p(q)" /><figcaption aria-hidden="true">H_p(q)</figcaption></figure><p>The really interesting thing is the difference <spanclass="math inline"><em>D</em><sub><em>q</em></sub>(<em>p</em>)</span>.That difference is how much longer our messages are because we used acode optimized for a different distribution <spanclass="math inline"><em>q</em></span>. If the distributions are thesame, this difference will be zero. As the difference grows, it will getbigger.</p><p>This difference is the <strong>Kullback–Leibler divergence</strong>,or just the <strong>KL divergence</strong>. The really neat thing aboutKL divergence is that it’s like a distance between two distributions. Itmeasures how different they are! (If you take that idea seriously, youend up with information geometry)</p><p>People usually write it as <spanclass="math inline"><em>D</em><em>L</em><sub><em>K</em></sub>(<em>p</em>||<em>q</em>)</span>,but our more intuitive way writes:</p><p><spanclass="math display"><em>D</em><sub><em>q</em></sub>(<em>p</em>) = <em>H</em><sub><em>q</em></sub>(<em>p</em>) − <em>H</em>(<em>p</em>)</span></p><p>If you expand the definition of KL divergence, you get:</p><p><span class="math display">$$\begin{align}D_q(p) &amp;= \sum_x p(x)\log_2\left(\frac{p(x)}{q(x)} \right) \\&amp;= \sum_x p(x) \left[ \log_2\left(\frac{1}{q(x)} \right) -\log_2\left(\frac{1}{p(x)} \right)\right]\\&amp;= \sum_x p(x) \left[ L_q(x) - L_p(x)\right]\end{align}$$</span></p><p>And we see the <spanclass="math inline">$\log_2\left(\frac{p(x)}{q(x)} \right)$</span> issimply the difference between how many bits a code optimized for <spanclass="math inline"><em>q</em></span> and a code optimized for <spanclass="math inline"><em>p</em></span> would use to represent <spanclass="math inline"><em>x</em></span>. The expression as a whole is theexpected difference in how many bits the two codes would use</p><h2 id="multiple-variables-and-joint-entropy">Multiple Variables andJoint Entropy</h2><p>Let’s return to our weather and clothing example from earlier. Mymother, like many parents, worries that I don’t dress appropriately forthe weather. So, she often wants to know both the weather and whatclothing I’m wearing. How many bits do I have to send her to communicatethis?</p><p>To send both pieces of information, we can flatten the probabilitydistribution (calculate the <strong>joint probability</strong>)</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored1-flat.png"alt="flattened probability" /><figcaption aria-hidden="true">flattened probability</figcaption></figure><p>Now we can figure out the optimal codewords for events of theseprobabilities and compute the average message length:</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-flat.png"alt="joint word length" /><figcaption aria-hidden="true">joint word length</figcaption></figure><p>Everything is the exact same as our normal definition, except withtwo variables instead of one. We call this the <strong>jointentropy</strong> of <spanclass="math inline"><em>X</em></span> and <spanclass="math inline"><em>Y</em></span>, defined as</p><p><span class="math display">$$H(X,Y) = \mathbb E_{p(x,y)}  \log_2\left(\frac{1}{p(x,y)}\right) =\sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x,y)}\right)$$</span></p><p>It’s in fact more intuitive not to flatten it, but Instead to keepthe 2 dimensional square and add word length as a 3rd dimension height.Now the entropy is the volume.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-3D.png"alt="3D code length" /><figcaption aria-hidden="true">3D code length</figcaption></figure><h2 id="conditional-entropy">Conditional Entropy</h2><p>Suppose my mom already knows the weather. She can check it on thenews. Now how much information do I need to provide? I actually need tosend less, because the weather strongly implies what clothing I’llwear!</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/HxCy-sep.png"alt="Conditional code length separated" /><figcaption aria-hidden="true">Conditional code lengthseparated</figcaption></figure><p>When it’s sunny, I can use a special sunny-optimized code, and whenit’s raining I can use a raining optimized code. In both cases, I sendless information than if I used a generic code for both. For example incase of raining, I send code length of 4 when I wear t-shirt and codelength of 4/3 when I wear coat.</p><p><span class="math display">$$H(X \mid \text{Y = raining})= \frac 1 4 \log_2 \frac 1 4 + \frac 3 4 \log_2 \frac 3 4= 0.81$$</span></p><p>(Don’t worry for now why the entropy, representing length of anoptimal codeword, can be fractional, we will <ahref="#fractional-bits">explain it later</a>)</p><p>With a similar calculation, we show <spanclass="math inline"><em>H</em>(<em>X</em> ∣ Y = sunny) = 0.81</span>. Toget the average amount of information I need to send my mother, I justput these two cases together:</p><p><span class="math display">$$\begin{align}H(X \mid Y) &amp;= P(\text{Y = rainy})\,H(X \mid \text{Y = rainy})+P(\text{Y = sunny})\,H(X \mid \text{Y = sunny}) \\&amp;= \frac 1 4 \times 0.81 + \frac 3 4 \times 0.81 = 0.81\end{align}$$</span></p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/HxCy.png"alt="Conditional code length together" /><figcaption aria-hidden="true">Conditional code lengthtogether</figcaption></figure><p>We call this the <strong>conditional entropy</strong>:</p><p><span class="math display">$$\begin{align}H(X|Y) &amp;= \sum_y p(y) H(X \mid Y=y) \\&amp;= \sum_y p(y) \sum_x p(x|y) \log_2\left(\frac{1}{p(x|y)}\right) \\&amp;= \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x|y)}\right)\end{align}$$</span></p><h2 id="mutual-information">Mutual Information</h2><p>In the previous section, we observed that knowing one variable canmean that communicating another variable requires less information.</p><p>One nice way to think about this is to imagine amounts of informationas bars. These bars overlap if there’s shared information between them:some of the information in <spanclass="math inline"><em>X</em></span> and <spanclass="math inline"><em>Y</em></span> is shared between them, so <spanclass="math inline"><em>H</em>(<em>X</em>)</span> and <spanclass="math inline"><em>H</em>(<em>Y</em>)</span> are overlapping bars.And since <spanclass="math inline"><em>H</em>(<em>X</em>, <em>Y</em>)</span> is theinformation in both, it’s the union of the bars <spanclass="math inline"><em>H</em>(<em>X</em>)</span> and <spanclass="math inline"><em>H</em>(<em>Y</em>)</span>.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-1.png"alt="Multi-variate bar 1" /><figcaption aria-hidden="true">Multi-variate bar 1</figcaption></figure><p>We rank the information needed to communicate these 3 kinds of thingsin descending order:</p><ul><li>Both <span class="math inline"><em>X</em></span> and <spanclass="math inline"><em>Y</em></span>: joint entropy <spanclass="math inline"><em>H</em>(<em>X</em>, <em>Y</em>)</span></li><li><span class="math inline"><em>X</em></span> alone: marginal entropy<span class="math inline"><em>H</em>(<em>X</em>)</span></li><li><span class="math inline"><em>X</em></span> when <spanclass="math inline"><em>Y</em></span> is known: conditional entropy<span class="math inline"><em>H</em>(<em>X</em>|<em>Y</em>)</span></li></ul><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview.png"alt="Ranking information contained" /><figcaption aria-hidden="true">Ranking informationcontained</figcaption></figure><p>In the bar perspective, <spanclass="math inline"><em>H</em>(<em>X</em>|<em>Y</em>)</span> is theinformation we need to send to communicate <spanclass="math inline"><em>X</em></span> to someone who already knows <spanclass="math inline"><em>Y</em></span> - it is the information in <spanclass="math inline"><em>X</em></span> which isn’t also in <spanclass="math inline"><em>Y</em></span>. Visually, that means <spanclass="math inline"><em>H</em>(<em>X</em>|<em>Y</em>)</span> is the partof <span class="math inline"><em>H</em>(<em>X</em>)</span> bar whichdoesn’t overlap with <spanclass="math inline"><em>H</em>(<em>Y</em>)</span>.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-4.png"alt="Multi-variate bar 2" /><figcaption aria-hidden="true">Multi-variate bar 2</figcaption></figure><p>From this, we get another identity: the information in <spanclass="math inline"><em>X</em></span> and <spanclass="math inline"><em>Y</em></span> is the information in <spanclass="math inline"><em>Y</em></span> plus the information in <spanclass="math inline"><em>X</em></span> which is not in <spanclass="math inline"><em>Y</em></span>. (sounds like set, doesn’tit?)</p><p><spanclass="math display"><em>H</em>(<em>X</em>, <em>Y</em>) = <em>H</em>(<em>Y</em>) + <em>H</em>(<em>X</em>|<em>Y</em>)</span></p><p>To wrap up, we have</p><ul><li>information in each variable: <spanclass="math inline"><em>H</em>(<em>X</em>)</span> and <spanclass="math inline"><em>H</em>(<em>Y</em>)</span></li><li>union of the information in both: <spanclass="math inline"><em>H</em>(<em>X</em>, <em>Y</em>)</span></li><li>information in one but not the other: <spanclass="math inline"><em>H</em>(<em>X</em>|<em>Y</em>)</span> and <spanclass="math inline"><em>H</em>(<em>Y</em>|<em>X</em>)</span></li></ul><p>We can further define <strong>mutual information</strong>:information both in <span class="math inline"><em>X</em></span> and<span class="math inline"><em>Y</em></span>, or in set terms, theintersection of information:</p><p><spanclass="math display"><em>I</em>(<em>X</em>, <em>Y</em>) = <em>H</em>(<em>X</em>) + <em>H</em>(<em>Y</em>) − <em>H</em>(<em>X</em>, <em>Y</em>)</span></p><blockquote><p>If you expand the definition of mutual information out, you get:</p><p><span class="math display">$$I(X,Y) = \sum_{x,y} p(x,y) \log_2\left(\frac{p(x,y)}{p(x)p(y)} \right)$$</span></p><p>That looks suspiciously like KL divergence!</p><p>Well, it is KL divergence. It’s the KL divergence of <spanclass="math inline"><em>P</em>(<em>X</em>, <em>Y</em>)</span> and itsnaive approximation <spanclass="math inline"><em>P</em>(<em>X</em>)<em>P</em>(<em>Y</em>)</span>.That is, it’s the number of bits you save representing <spanclass="math inline"><em>X</em></span> and <spanclass="math inline"><em>Y</em></span> if you understand the relationshipbetween them instead of assuming they’re independent.</p></blockquote><p>and the <strong>variation of information</strong>. The variation ofinformation is the information which isn’t shared between the variables.It gives a metric of distance between different variables. The variationof information between two variables is zero if knowing the value of onetells you the value of the other and increases as they become moreindependent.</p><p><spanclass="math display"><em>V</em>(<em>X</em>, <em>Y</em>) = <em>H</em>(<em>X</em>, <em>Y</em>) − <em>I</em>(<em>X</em>, <em>Y</em>)</span></p><p>How does this relate to KL divergence, which also gave us a notion ofdistance? Well, KL divergence gives us a distance between twodistributions <strong>over the same variable or set ofvariables</strong>. In contrast, variation of information gives usdistance between <strong>two jointly distributed variables</strong>. KLdivergence is <strong>between distributions</strong>, variation ofinformation <strong>within a distribution</strong>.</p><p>In summary, we put them all in a single diagram:</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info.png"alt="Multi-variate bar 3" /><figcaption aria-hidden="true">Multi-variate bar 3</figcaption></figure><h2 id="fractional-bits">Fractional Bits</h2><p>A careful reader may have noticed that in <ahref="#conditional-entropy">previous calculations</a>, we had fractionallength of message. Isn’t that weird? How should we interpret such amessage? How is it done in real life?</p><p>The answer is: you can think of them as expected length of a message.If half the time one sends a single bit, and half the time one sends twobits, on average one sends one and a half bits. There’s nothing strangeabout averages being fractional.</p><p>That’s a quick but vague answer. Let’s look at an example: consider aprobability distribution where one event <spanclass="math inline"><em>a</em></span> happens 71% of the time andanother event <span class="math inline"><em>b</em></span> occurs 29% ofthe time.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/halfbit-ab.png"alt="Fractional bit single" /><figcaption aria-hidden="true">Fractional bit single</figcaption></figure><p>The optimal code would use 0.5 bits to represent <spanclass="math inline"><em>a</em></span>, and 1.7 bits to represent <spanclass="math inline"><em>b</em></span>. Well, if we want to send a singleone of these codewords, it simply isn’t possible. We’re forced to roundto a whole number of bits, and send on average 1 bit.</p><p>… But if we’re sending multiple messages at once, it turns out thatwe can do better. Let’s consider communicating two events from thisdistribution. If we send them independently, using the code weestablished for a single event, we’d need to send two bits. Can we dobetter?</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/halfbit-ab2.png"alt="Fractional bit double" /><figcaption aria-hidden="true">Fractional bit double</figcaption></figure><p>Half the time, we need to communicate <spanclass="math inline"><em>a</em><em>a</em></span>, 21% of the time we needto send <span class="math inline"><em>a</em><em>b</em></span> or <spanclass="math inline"><em>b</em><em>a</em></span>, and 8% of the time weneed to communicate <spanclass="math inline"><em>b</em><em>b</em></span>. Again, the ideal codeinvolves fractional numbers of bits.</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/halfbit-ab-idealcode.png"alt="Fractional bit double code length" /><figcaption aria-hidden="true">Fractional bit double codelength</figcaption></figure><p>If we round the codeword lengths, we’ll get something like this:</p><figure><imgsrc="https://colah.github.io/posts/2015-09-Visual-Information/img/halfbit-ab-code.png"alt="Fractional bit double code length rounded" /><figcaption aria-hidden="true">Fractional bit double code lengthrounded</figcaption></figure><p>This codes give us an average message length of <spanclass="math inline">1.8</span> bits. That’s less than the <spanclass="math inline">2</span> bits when we send themindependently. Another way of thinking of this is that we’re sending<span class="math inline">1.8/2 = 0.9</span> bits on average for eachevent. If we were to send more events at once, it would become smallerstill. As <span class="math inline"><em>n</em></span> tends to infinity,the overhead due to rounding our code would vanish, and the number ofbits per codeword would approach the entropy.</p><p>Further, notice that the ideal codeword length for a was 0.5 bits,and the ideal codeword length for aa was 1 bit. Ideal codeword lengthsadd, even when they’re fractional! So, if we communicate a lot of eventsat once, the lengths will add.</p><h2 id="conclusion">Conclusion</h2><ul><li>Entropy is optimal length ..</li><li>KL divergence ..</li><li>Entropy over multiple variables can be interpreted as sets ofinformation</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This blog post is adapted from ex-OpenAI researcher, Anthropic
co-founder &lt;a
href=&quot;https://colah.github.io/posts/2015-09-Visual-Information/&quot;&gt;Christopher
Olah’s wonderful work&lt;/a&gt;. I removed parts that are generally
commonsense to a CS kid and added some of my own notes &amp;amp;
explanations.&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Quantization</title>
    <link href="https://yao-lirong.github.io/2023-12-01-Quantization/"/>
    <id>https://yao-lirong.github.io/2023-12-01-Quantization/</id>
    <published>2023-12-01T05:00:00.000Z</published>
    <updated>2025-09-02T23:51:12.130Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k-bit-inference-scaling-laws"><ahref="https://arxiv.org/abs/2212.09720">K-bit Inference ScalingLaws</a></h2><p>This paper and its Appendix serves as a good summary of SOTAquantization techniques and their results.</p><p><strong>Why should we quantize?</strong> The overall computationlatency – the time it takes from start to finish of a computation – ismainly determined by two factors: (1) how long does it take to load thedata from main memory into caches and registers, (2) how long does ittake to perform the computation. Therefore, reducing the time spentloading data from main memory is often the best way to accelerateoverall computation latency. Such reductions can be achieved mainlythrough caching and lower precision numbers.</p><p>Note though, <strong>to do computation</strong>, we dequantize theweight in the cache and perform a 16-bit floating point multiplicationwith the 16-bit input. That’s because no CPU / GPU supports these weirddata type computation.</p><p>In this work, the author used blocking, a zero-shot quantizationmethod, for 3, 4, 5, 6, 7, and 8 bits. By plotting out perplexity vstotal bits of the model, they found that lowering the bit precisiongenerally improves scaling. However, this trend stops across all modelsat 3-bit precision, where performance degrades. Therefore, <strong>4-bitprecision is optimal</strong> for almost all models at all scales, withfew exceptions.</p><p>BLOOM and BLOOMZ show almost the same quantization behavior,indicating that <strong>fine-tuning</strong> an existing model does notchange its quantization properties.</p><p>Data types: The quantile quantization and float data types providebetter scaling than integer and dynamic exponent quantization. Theauthor concluded that <strong>quantile quantization is thebest</strong>.</p><ul><li><strong>zero-shot quantization</strong>: directly quantize a modelwithout any additional information. Can be used immediately, which makesthem easy to use, but zero-shot quantization methods often fail at lowerprecisions.</li><li><strong>one-shot quantizationL</strong> need a mini-batch of datafor quantization. more accurate, such as GPTQ, which optimizes therounding during quantization via a mini-batch of data. But they are alsomore complex and may require hours of optimization before a model can beused.</li></ul><h2 id="llm.int8"><ahref="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8()</a></h2><p>In this quantization paper, the author discovered an emergent outlierfeature in tranformers that totally wreck quantization. They alsoplotted a scaling law for this emergent outlier feature. By doing so, heproposed the <code>LLM.int8()</code> no-performance-degradationquantization method. I didn’t read the paper, but looked at the author’s<ahref="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">blogpost</a> instead.</p><h2 id="how-quantization-works">How Quantization works</h2><p>First, how do we quantize a number? Imagine the following example:you have a data type I5 with values [0, 1, 2, 3, 4, 5] and a data typeI3 with values [0, 2, 4]. We want to quantize I5 vector [3, 1, 2, 3] toI3:</p><ol type="1"><li><p>map from original domain to unit domain [-1, 1]</p><p>find absolute maximum <code>3 = max(abs([3, 1, 2, 3]))</code>, dividethe vector by 3 to get <code>[1.0, 0.33, 0.66, 1.0]</code></p></li><li><p>map from unit domain [-1, 1] to quantized domain</p><p>Multiply by the range of the target data type I3, which is 4:<code>[1.0, 0.33, 0.66, 1.0] -&gt; [4.0, 1.33, 2.66, 4.0]</code></p></li><li><p>round to the nearest representable number in this quantizeddomain</p><p><code>[4.0, 1.33, 2.66, 4.0] -&gt; [4, 0, 2, 4]</code></p></li></ol><p>To dequantize, we reverse this process:</p><ol type="1"><li><p>map from quantized domain to unit domain [-1, 1]</p><p>Divide the vector by range 4 to get<code>[1.0, 0.0, 0.5, 1.0]</code></p></li><li><p>map from unit domain [-1, 1] to original domain</p><p>Multiply by the stored absolute maximum 3:<code>[1.0, 0.0, 0.5, 1.0] -&gt; [3.0, 0.0, 1.5, 3.0]</code></p></li><li><p>round to the nearest representable number in the originaldomain</p><p><code>[3.0, 0.0, 1.5, 3.0] -&gt; [3, 0, 2, 3]</code></p></li></ol><p>We see that our dequantization and quantization led to one error atthe second element.</p><h2 id="emergent-outlier-features">Emergent Outlier Features</h2><p>Since we are using the absolute, it is obvious that if we have anoutlier, there will be more errors. So the authors go on to discover thedistribution of outliers. They call such outliers <strong>emergentoutlier features</strong>.</p><p>The authors explain such outlier features as to select only a singlefeature. At the same time, the other small value part brings theunimportant values down.</p><p>The authors also found this very interesting emergent phenomenom,which I directly quote below:</p><blockquote><p>However, this full “coordination” through a single dimension onlyhappens after the phase shift. Before the phase shift, in transformerswith less than 6.7B parameters some layers disagree which dimension touse for these large features (no prominent outliers).</p><p>…</p><p>The phase shift happens around 6.7B, where 100% of layers use thesame dimension for outliers. At this point, a couple of things happenrapidly:</p><ol start="4" type="1"><li>Transformers become more stable. If you treat the outlier featuresseparately, I believe you can probably run and even train transformersin less than 8-bit precision without degradation in performance.</li></ol></blockquote><p>Note the <strong>model perplexity rather than mere model sizedetermines the phase shift</strong>.</p><h2 id="ggml"><ahref="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml">GGML</a></h2><p><ahref="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml">MLabonne</a>did a great explanation to how GGML did quantization.</p><p>GGML quantizes weights in a rather naive way. Basically, it groupsblocks of values and rounds them to a lower precision. Some techniques,like Q4_K_M and Q5_K_M, implement a <strong>higher precision forcritical layers</strong>. In this case, every weight is stored in 4-bitprecision, with the exception of half of the attention.wv andfeed_forward.w2 tensors. Experimentally, this mixed precision proves tobe a good tradeoff between accuracy and resource usage.</p><p>If we look into the <ahref="https://github.com/ggerganov/ggml/blob/63d8fce8b57c5e97dd1d42b0d7b8c734df1f263c/src/ggml-quants.h"><code>ggml-quants.h</code>file</a>, we can see how the blocks are defined. For example,the <code>block_q4_0</code> structure is defined as:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> QK4_0 32</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="keyword">ggml_fp16_t</span> d;          <span class="comment">// delta</span></span><br><span class="line">    <span class="keyword">uint8_t</span> qs[QK4_0 / <span class="number">2</span>];  <span class="comment">// nibbles / quants</span></span><br><span class="line">&#125; block_q4_0;</span><br></pre></td></tr></table></figure><p>In GGML, weights are processed in blocks, each consisting of 32values. For each block, a scale factor (delta) is derived from thelargest weight value. All weights in the block are then scaled,quantized, and packed efficiently for storage (nibbles).</p><p>Oobabooga, the author of <ahref="https://github.com/oobabooga/text-generation-webui">textgeneration webui</a>, did an <ahref="https://oobabooga.github.io/blog/posts/perplexities/">in-depthsurvey</a> of inference time, model size, vRAM usage on different typesof quantization formats (GPTQ, GGUF, EXL2, …)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;k-bit-inference-scaling-laws&quot;&gt;&lt;a
href=&quot;https://arxiv.org/abs/2212.09720&quot;&gt;K-bit Inference Scaling
Laws&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This paper and its</summary>
      
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Fine-Tuning LLMs: Prompt Tuning, Adapter, LoRA</title>
    <link href="https://yao-lirong.github.io/2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/"/>
    <id>https://yao-lirong.github.io/2023-11-20-Fine-Tuning-LLMs-Prompt-Tuning,-Adapter,-LoRA/</id>
    <published>2023-11-20T05:00:00.000Z</published>
    <updated>2025-09-03T00:06:29.312Z</updated>
    
    <content type="html"><![CDATA[<p>This article <ahref="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">AGuid to Parameter-efficient Fine-tuning (PEFT)</a> made a very goodsummary with nice drawings. There are some differences between itsexplanation with the original paper but the basic architecture is allgood.</p><span id="more"></span><h2 id="prompt-tuning">Prompt Tuning</h2><p>Prefix tuning, prompt tuning, and p-tuning all prepend some vectorsas prefixes / soft prompts to the vector inputs to transformers. Theirgoal is to find a context that steers the language model towardgenerating text that solves a particular task.</p><h2 id="adapter"><ahref="https://arxiv.org/pdf/1902.00751.pdf">Adapter</a></h2><blockquote><p>Before Adapter, when performing vanilla fine-tuning, a modificationis made to the top layer of the network because the label spaces andlosses for the upstream and downstream tasks differ. Now, Adaptermodules perform more general architectural modifications to re-purpose apretrained network for a downstream task: it injects new layers into theoriginal network.</p><p>In standard fine-tuning, the new top-layer and the original weightsare co-trained. In contrast, in Adapter tuning, the parameters of theoriginal network are frozen and therefore may be shared by manytasks.</p></blockquote><figure><img src="./images/Adapter.png" alt="Adapter Architecture" /><figcaption aria-hidden="true">Adapter Architecture</figcaption></figure><blockquote><p>Left: We add the adapter module twice to each Transformer layer:after the projection following multiheaded attention and after the twofeed-forward layers.</p><p>Right: The adapter consists of a bottleneck which contains fewparameters relative to the attention and feedforward layers in theoriginal model. The adapter also contains a skip-connection. Duringadapter tuning, the green layers are trained on the downstream data,this includes the adapter, the layer normalization parameters, and thefinal classification layer</p></blockquote><p>Therefore, we can denote the Adapter layer as:</p><p><spanclass="math display"><em>y</em> = <em>B</em>(<em>σ</em>(<em>A</em><em>x</em>)) + <em>x</em></span></p><p>Define bottleneck dimension <spanclass="math inline"><em>r</em></span> (to be consistent with LoRA, inthe original Adapter paper, this was <spanclass="math inline"><em>m</em></span>), so <span class="math inline">$A\in \R^{r \times d}, B \in \R^{d \times r}$</span>. Including biases, weadd a total <spanclass="math inline">2<em>d</em><em>r</em> + <em>d</em> + <em>r</em></span>parameters with <spanclass="math inline"><em>r</em> ≪ <em>d</em></span>.</p><p>In initialization, we initialize the adapters to a near-identityfunction, so original network is unaffected when training starts.</p><p>Adapter achieves similar results with only 1% needed parameters ascompared to full fine-tuning.</p><h2 id="lora-low-rank-adaptation-of-llm"><ahref="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation ofLLM</a></h2><p>Before LoRA, SOTA techniques have some drawbacks:</p><ul><li>Adapter Layers Introduce Inference Latency: Adapter layers have fewparameters, but “large neural networks rely on <strong>hardwareparallelism</strong> to keep the latency low, and adapter layers<strong>have to be processed sequentially</strong>. This makes adifference in the online inference setting where the batch size istypically as small as one.” I actually don’t understand how toparallelize an LLM inference even if without Adapter</li><li>Directly Optimizing the Prompt is Hard: Prompt tuning and prefixtuning both require adding a prefix (to either the input vector or tothe hidden vector in middle). In this way, it “<strong>reduces thesequence length available</strong> to process a downstream task, whichwe suspect makes tuning the prompt less performant compared to othermethods.” Its <strong>performance changes non-monotonically in trainableparameters</strong>, too. So it’s hard to optimize.</li></ul><p>LoRA’s architecture is simply a matrix multiplication - Adapterwithout non-linearity or skip connection. So instead of <spanclass="math inline"><em>y</em> = <em>B</em>(<em>σ</em>(<em>A</em><em>x</em>)) + <em>x</em></span>,we do <spanclass="math inline"><em>Δ</em><em>y</em> = <em>B</em><em>A</em><em>x</em></span>.There is one fundamental difference though: Adapter is an extra layer<strong>added into the original network</strong>, but LoRA is a layer<strong>added along side with the original network</strong>. That’s whyI have a delta in LoRA’s formula.</p><p>In fact, LoRA was specifically designed for low-rank matrixmultiplication decomposition, note for any pretrained weight <spanclass="math inline">$W_0 \in \R^{d \times k}$</span> with <spanclass="math inline"><em>y</em> = <em>W</em><sub>0</sub><em>x</em></span>and we want to update this weight matrix in fine-tuning: <spanclass="math inline"><em>W</em> = <em>W</em><sub>0</sub> + <em>Δ</em><em>W</em></span>,we define <spanclass="math inline"><em>Δ</em><em>W</em> = <em>B</em><em>A</em></span>,so <spanclass="math inline"><em>W</em> = <em>W</em><sub>0</sub> + <em>B</em><em>A</em></span>.This simple design yields a unimaginable good result: note matrix <spanclass="math inline"><em>B</em><em>A</em></span> is also of dimension<span class="math inline">$\R^{d \times k}$</span>. Therefore, we candirectly add this result matrix to the original matrix and inferencingwith LoRA, when we add this matrix in, gives us <strong>no additionalinference latency</strong>.</p><p>Similar to Adapter, LoRA uses a random Gaussian initialization for Aand zero for B, so <spanclass="math inline"><em>Δ</em><em>W</em><em>x</em> = <em>B</em><em>A</em><em>x</em></span>is zero at the beginning of training and <spanclass="math inline"><em>W</em> = <em>W</em><sub>0</sub> + <em>Δ</em><em>W</em></span>yields the same result (identity) as before.</p><p>Thanks to the matrix decomposition nature, we can apply LoRA to anymatrix multiplication in principle. However, we found that intransformers, imagine if we have 8 ranks to distribute, when <spanclass="math inline"><em>r</em>(<em>W</em><sub><em>q</em></sub>) = <em>r</em>(<em>W</em><sub><em>v</em></sub>) = 4</span>or <spanclass="math inline"><em>r</em>(<em>W</em><sub><em>q</em></sub>) = <em>r</em>(<em>W</em><sub><em>k</em></sub>) = <em>r</em>(<em>W</em><sub><em>v</em></sub>) = <em>r</em>(<em>W</em><sub><em>o</em></sub>) = 2</span>gives best result. The author also found that the fine-tune matrixactually has a very low rank, so in practice even if we set <spanclass="math inline"><em>r</em> = 1</span> can give good enough results.Go to Section 7 for more interesting experiments they conducted.</p><p>In diffusion models, we use LoRA on the stable diffusion’s crossattention layer.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This article &lt;a
href=&quot;https://www.leewayhertz.com/parameter-efficient-fine-tuning/&quot;&gt;A
Guid to Parameter-efficient Fine-tuning (PEFT)&lt;/a&gt; made a very good
summary with nice drawings. There are some differences between its
explanation with the original paper but the basic architecture is all
good.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Graph Networks &amp; GraphCast</title>
    <link href="https://yao-lirong.github.io/2023-11-16-Graph-Networks-&amp;-GraphCast/"/>
    <id>https://yao-lirong.github.io/2023-11-16-Graph-Networks-&amp;-GraphCast/</id>
    <published>2023-11-16T05:00:00.000Z</published>
    <updated>2025-09-03T00:11:40.257Z</updated>
    
    <content type="html"><![CDATA[<h2 id="graph-networks"><a href="https://arxiv.org/abs/1806.01261">GraphNetworks</a></h2><p>This is a very detailed and clear intro to Graph Networks byDeepmind.</p><h3 id="graph-definition-box-3-3.2.1">Graph Definition (Box 3 &amp;3.2.1)</h3><p>We define a graph to have node, edge, and global attributes. Globalattribute, <span class="math inline"><em>u</em></span>, for example canbe the gravitational field in a three body problem setting.</p><p>Global attribute is there to give a chance to any local node / edgeto know what’s happening in a global perspective (mostly places far awyfrom it). If we exclude the global <spanclass="math inline"><em>u</em></span> (which aggregates information fromacross the nodes and edges), the information that a node has access toafter m steps of propagation is determined by the set of nodes and edgesthat are at most m hops away (Figure 7). This can be interpreted asbreaking down a complex computation into smaller elementary steps.</p><h3 id="graph-update-3.2.2-3.2.3-4.2">Graph Update (3.2.2 &amp; 3.2.3&amp; 4.2)</h3><figure><img src="./images/GN_algorithm.png" alt="Graph Network Algorithm" /><figcaption aria-hidden="true">Graph Network Algorithm</figcaption></figure><p>The whole GN consists of the following 6 functions:</p>$$\begin{array}{l}{\mathbf{e}_{k}^{\prime}=\phi^{e}\left(\mathbf{e}_{k},\mathbf{v}_{r_{k}},\mathbf{v}_{s_{k}},\mathbf{u}\right)}\\ {\mathbf{v}_{i}^{\prime}=\phi^{v}\left({{\bar{\mathbf{e}}}}_{i}^{\prime},\mathbf{v}_{i},\mathbf{u}\right)}\\ {\mathbf{u}^{\prime}=\phi^{u}\left({\bar{\mathbf{e}}}^{\prime},{\bar{\mathbf{v}}}^{\prime},\mathbf{u}\right)}\end{array}\begin{array}{l}{\bar{{\mathbf{e}}}_{i}^{\prime}=\rho^{e\to v}\left(E_{i}^{\prime}\right)}\\ {\bar{{\mathbf{e}}}^{\prime}=\rho^{e\to u}\left(E^{\prime}\right)}\\ {\bar{{\mathbf{v}}}^{\prime}=\rho^{v\to u}\left(V^{\prime}\right)}\end{array}$$<p><span class="math inline"><em>ϕ</em><sup><em>e</em></sup></span> isan edge specific function and is applied to every edge, same for nodeand global. <span class="math inline"><em>ϕ</em></span> can be anyfunction, people usually use Neural Networks.</p><p>On the other hand, <span class="math inline"><em>ρ</em></span> takesin a set as input, so needs to be an aggregate function invariant topermutations, like sum, mean, max, min, etc. These functions are notaffected by the order they are sent in.</p><h3 id="composing-multi-graph-block-architecture-4.3">Composing MultiGraph Block Architecture (4.3)</h3><p>A GN is simply three aggregate functions and three update functionsfor node, edge, global respectively. It always takes in a graphcomprised of edge, node, and global attributes as input, and returning agraph comprised of edge, node, and global attribute. The GN operates onany graph without caring about size of the graph, so it is extremelyeasy to stack multiple GN block together.</p><p>Figure 6 shows 3 commons ways of stacking GN blocks.</p><h3 id="graph-networks-advantage-3.2.4">Graph Networks Advantage(3.2.4)</h3><ol type="1"><li>First, graphs can express arbitrary relationships among entities, sothe way input interacts is not limited to a fixed modelarchitecture.</li><li>Graphs represent entities and their relations as sets, which areinvariant to permutations. However, we can still impose ordering byencoding the indices in the node or edge attribute.</li><li>A GN’s per-edge and per-node functions are reused across all edgesand nodes, respectively. Therefore, GN can easily operate on graphs ofany size.</li></ol><h3 id="limitations-5.2">Limitations (5.2)</h3><p>Notions like recursion, control flow, and conditional iteration arenot straightforward to represent with graphs.</p><h2 id="graphcast"><ahref="https://www.science.org/doi/10.1126/science.adi2336">GraphCast</a></h2><p>GraphCast by Deepmind is a perfect example to understandencode-process-decode GN structure and it is clearly written. Note themodel architecture is inside the <ahref="https://www.science.org/doi/suppl/10.1126/science.adi2336/suppl_file/science.adi2336_sm.pdf">supplementarymaterial</a>. The following notes mostly come from Section3 GraphCastmodel.</p><p>They defined two space for this task. One on the grid space based onearth’s latitude and logitude, where the atmospheric information comesfrom. The other on a “mesh space” based on a regular sphere, where theactual processing happens. The design was made on the observation that“Using the latitude-longitude grid is not an advisable representationdue to its spatial inhomogeneity, and high resolution at the poles whichdemands disproportionate compute resources.”</p><figure><imgsrc="https://www.science.org/cms/10.1126/science.adi2336/asset/98816fa7-3765-4307-bbe7-29ebbd5e26a3/assets/images/large/science.adi2336-f1.jpg"alt="Mesh Space" /><figcaption aria-hidden="true">Mesh Space</figcaption></figure><p>As for the mesh space, it is not only 1 mesh space, but 7 mesh spacelayered together. The first mesh space (M0) is a regular icosahefron (12nodes and 20 faces). The second (M1) is the first layer divided 6 times.The third (M2) is the second divided 6 times and so forth. The M0 layeris the base and the coarsest one. Among all layers, It has the fewestpoints and the greatest distance between each point. On the other hand,M6 layer has the finest resolution, most points, and each point is onlya few units away from each other.</p><p>When we update a particular point on the net, we look at each layer:M0, M1, …, M6. For each layer Mi, we look at this point’s neighbor onthis mesh and use these edges to update this point’s value. On M0, theneighbors are far away, so you get information from away. On M6, theneighbors are very close, so you get information from close points. Thisis exactly what depicted on paper’s Fig1 e) - every point has input fromdifferent distance.</p><p>What’s most important is that since the finer resolution layer isobtained by dividing the coarser layer, every point and edge on thecoarser layer can be directly found on the finer layer. Therefore, inmemory we only have to store a single layer M6.</p><p>You might ask what if a point on M6 is not originally on M0? Will itstill have neighbors on M0 properly defined? Yes (I think), because youcan think of the mesh construction process in reverse: if you start withan arbitrary point on the finest mesh and make the resolution coarser,you will always be able to get back to some regular icosahefron with thesame strcuture as the M0 we started with (though not the exact same onebecause this point you started with might indeed not be on the M0 webuilt the multi-mesh upon).</p><p>The encoder is a graph mapping grid nodes to mesh nodes. Theprocesser happens on mesh nodes only. The decoder maps mesh nodes backto the grid nodes.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;graph-networks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;&gt;Graph
Networks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a very detailed and clear intro to Gra</summary>
      
    
    
    
    
    <category term="ML" scheme="https://yao-lirong.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>First Time Debugging with ChatGPT</title>
    <link href="https://yao-lirong.github.io/2023-04-04-First-Time-Debugging-with-ChatGPT/"/>
    <id>https://yao-lirong.github.io/2023-04-04-First-Time-Debugging-with-ChatGPT/</id>
    <published>2023-04-04T04:00:00.000Z</published>
    <updated>2023-04-04T21:43:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>I was trying out the sampling in <ahref="https://github.com/AI-Guru/MMM-JSB">MMM music generation model</a>today and encountered the problem described in <ahref="https://github.com/craffel/pretty-midi/issues/220">this issue Iproposed</a>. I have no experience writing C in python with<code>ctypes</code>, so I figured why not ask the magic <del> conchshell </del> ChatGPT?</p><span id="more"></span><p>I’ve asked him several “how to write …” questions since its release,but this was the first time I actually ask him to help me understand asnippet of code so I can proceed to debugging.</p><p>It did pretty good in my first question, as it should do as the SOTALLM model.</p><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/q0.png"alt="q0" /><figcaption aria-hidden="true">q0</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/a0.png"alt="a0" /><figcaption aria-hidden="true">a0</figcaption></figure><p>What strikes me is the context-aware ability it showed in my secondquestion. It is well known it can do so from all the demos, but seeingit actually work with a real example of your own is really a differentstory. <strong>Here, I just asked an absolutely arbitrary question andit knew what I was referring to</strong></p><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/q1.png"alt="q1" /><figcaption aria-hidden="true">q1</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/a1.png"alt="a1" /><figcaption aria-hidden="true">a1</figcaption></figure><p>It became mostly clear to me where the bug was, but to make sure</p><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/q2.png"alt="q2" /><figcaption aria-hidden="true">q2</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/a2.png"alt="a2" /><figcaption aria-hidden="true">a2</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/q3.png"alt="q3" /><figcaption aria-hidden="true">q3</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/a3.png"alt="a3" /><figcaption aria-hidden="true">a3</figcaption></figure><p>At last, I conveniently asked it how to fix this bug</p><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/q4.png"alt="q4" /><figcaption aria-hidden="true">q4</figcaption></figure><figure><img src="/images/2023-04-04-First-Time-Debugging-with-ChatGPT/a4.png"alt="a4" /><figcaption aria-hidden="true">a4</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;I was trying out the sampling in &lt;a
href=&quot;https://github.com/AI-Guru/MMM-JSB&quot;&gt;MMM music generation model&lt;/a&gt;
today and encountered the problem described in &lt;a
href=&quot;https://github.com/craffel/pretty-midi/issues/220&quot;&gt;this issue I
proposed&lt;/a&gt;. I have no experience writing C in python with
&lt;code&gt;ctypes&lt;/code&gt;, so I figured why not ask the magic &lt;del&gt; conch
shell &lt;/del&gt; ChatGPT?&lt;/p&gt;</summary>
    
    
    
    
    <category term="Journal" scheme="https://yao-lirong.github.io/tags/Journal/"/>
    
  </entry>
  
</feed>
